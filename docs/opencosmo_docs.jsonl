{"url": "https://opencosmo.readthedocs.io/en/latest/", "title": "Welcome to the OpenCosmo Toolkit Documentation — OpenCosmo Documentation", "content": "# Welcome to the OpenCosmo Toolkit Documentation\nWelcome to the OpenCosmo Toolkit Documentation\n\nThe OpenCosmo Python Toolkit provides utilities for reading, writing and manipulating data from cosmological simulations produced by the Cosmolgical Physics and Advanced Computing (CPAC) group at Argonne National Laboratory. It can be used to work with smaller quantities data retrieved with the CosmoExplorer, as well as the much larget datasets these queries draw from. The OpenCosmo toolkit integrates with standard tools such as AstroPy, and allows you to manipulate data in a fully-consistent cosmological context.\nGetting Started\nInstallation\nFirst Steps\nGeneral Usage\nMain Transformations API\nUnit Conventions\nAdding Columns\nFiltering\nSelecting Columns\nTaking Rows\nSorting\nSpatial Querying\nIterating Over Rows\nEvaluating Complex Expressions\nWorking with Columns\nQuerying Based on Column Values\nQuerying In Collections\nAdding Custom Columns\nWorking with Collections\nTypes of Collections\nLightcones\nHealpix Maps\nSimulation Collections\nStructure Collections\nTransformations on Structure Collections\nWorking with Units\nConverting Between Unit Conventions\nConverting Columns to an Equivalent Unit\nSingle-Column Conversions\nConversion Precedence\nStructure Collection Conversions\nClearing Conversions\nReading and Writing Data in the OpenCosmo Format\nOptions for Reading Data\nWriting Data\nOther Formats\nAdvanced Usage\nEvaluating Complex Expressions on Datasets and Collections\nWorking with MPI\nPerformance Tips\nAnalysis and Visualization\nAnalyzing Particle Data with yt\nSimulating X-ray Emission with pyXSIM\nVisualizing Halos\nAPI Reference\nReading/Writing\nDataset\nCollections\nColumns\nParameters and the OpenCosmoHeader\nRegions\nAnalysis\nChangelog\nopencosmo 1.0.1 (2026-01-21)\nopencosmo 1.0.0 (2026-01-21)\nopencosmo 0.9.6 (2025-10-20)\nopencosmo 0.9.4 (2025-09-26)\nopencosmo 0.9.3 (2025-09-25)\nopencosmo 0.9.2 (2025-09-25)\nopencosmo 0.9.1 (2025-09-16)\nopencosmo 0.9.0 (2025-09-15)\nopencosmo 0.8.0 (2025-07-22)"}
{"url": "https://opencosmo.readthedocs.io/en/latest/installation.html", "title": "Installation — OpenCosmo Documentation", "content": "# Installation\nInstallation\n\nThe OpenCosmo library is available for Python 3.11 and later on Linux and MacOS (and Windows via\nWSL\n). It can be installed with\npip\n:\npip\ninstall\nopencosmo\nThere’s a good chance the default version of Python on your system is less than 3.11. Whether or not this is the case, we recommend installing\nopencosmo\ninto a virtual environment. If you’re using\nConda\nyou can create a new environment and install\nopencosmo\ninto it automatically:\nconda\ncreate\n-n\nopencosmo_env\npython\n=\n3\n.11\nconda-forge::opencosmo\nor, if you already have a virtual environment you’d like to use:\nconda\ninstall\nconda-forge::opencosmo\nIf you plan to use\nopencosmo\nin a Jupyter notebook, you can install the\nipykernel\npackage to make the environment available as a kernel:\nconda\ninstall\nipykernel\npython\n-m\nipykernel\ninstall\n--user\n--name\n=\nopencosmo\nBe sure you have run the\nactivate\ncommand shown above before running the\nipykernel\ncommand.\nIf you are interested in the tools in the\nanalysis\nmodule, they need to be installed seperately:\nopencosmo\ninstall\nhaloviz\n## Installing with MPI Support\nInstalling with MPI Support\n\nopencosmo\ncan leverage MPI to distribute analysis on a very large dataset across multiple cores or nodes. You simply need to install the\nmpi4py\npackage:\npip\ninstall\nmpi4py\nopencosmo\nCan both read and write data in an MPI context with no additional setup. By default, ranks must write to the file one at a time. This may result in poor performance if the data being written is large. This can be improved by installing the\nh5py\npackage with MPI support. Pre-built wheels with MPI support are not generally available on PyPI, so you will need to build\nh5py\nfrom source against a version of the HDF5 library that was built with MPI support. Many HPC systems have an optimized MPI-enabled version of HDF5 available. For example, on Polaris at the Argonne Leadership Computing Facility (ALCF), run the follwing commands in the Python environment you plan to use with\nopencosmo\n:\npip\nuninstall\nh5py\n# If a non-MPI version is already installed in your environment\nHDF5_MPI\n=\n\"ON\"\nHDF5_DIR\n=\n$HDF5_DIR\nCC\n=\n\"cc\"\npip\ninstall\n--no-cache-dir\n--force-reinstall\n--no-binary\n=\nh5py\nh5py\nParallel HDF5 is also available at\nNERSC\nand\nOLCF\n. See the linked documentation for details of getting your environment set up at one of those facilities."}
{"url": "https://opencosmo.readthedocs.io/en/latest/index.html", "title": "Welcome to the OpenCosmo Toolkit Documentation — OpenCosmo Documentation", "content": "# Welcome to the OpenCosmo Toolkit Documentation\nWelcome to the OpenCosmo Toolkit Documentation\n\nThe OpenCosmo Python Toolkit provides utilities for reading, writing and manipulating data from cosmological simulations produced by the Cosmolgical Physics and Advanced Computing (CPAC) group at Argonne National Laboratory. It can be used to work with smaller quantities data retrieved with the CosmoExplorer, as well as the much larget datasets these queries draw from. The OpenCosmo toolkit integrates with standard tools such as AstroPy, and allows you to manipulate data in a fully-consistent cosmological context.\nGetting Started\nInstallation\nFirst Steps\nGeneral Usage\nMain Transformations API\nUnit Conventions\nAdding Columns\nFiltering\nSelecting Columns\nTaking Rows\nSorting\nSpatial Querying\nIterating Over Rows\nEvaluating Complex Expressions\nWorking with Columns\nQuerying Based on Column Values\nQuerying In Collections\nAdding Custom Columns\nWorking with Collections\nTypes of Collections\nLightcones\nHealpix Maps\nSimulation Collections\nStructure Collections\nTransformations on Structure Collections\nWorking with Units\nConverting Between Unit Conventions\nConverting Columns to an Equivalent Unit\nSingle-Column Conversions\nConversion Precedence\nStructure Collection Conversions\nClearing Conversions\nReading and Writing Data in the OpenCosmo Format\nOptions for Reading Data\nWriting Data\nOther Formats\nAdvanced Usage\nEvaluating Complex Expressions on Datasets and Collections\nWorking with MPI\nPerformance Tips\nAnalysis and Visualization\nAnalyzing Particle Data with yt\nSimulating X-ray Emission with pyXSIM\nVisualizing Halos\nAPI Reference\nReading/Writing\nDataset\nCollections\nColumns\nParameters and the OpenCosmoHeader\nRegions\nAnalysis\nChangelog\nopencosmo 1.0.1 (2026-01-21)\nopencosmo 1.0.0 (2026-01-21)\nopencosmo 0.9.6 (2025-10-20)\nopencosmo 0.9.4 (2025-09-26)\nopencosmo 0.9.3 (2025-09-25)\nopencosmo 0.9.2 (2025-09-25)\nopencosmo 0.9.1 (2025-09-16)\nopencosmo 0.9.0 (2025-09-15)\nopencosmo 0.8.0 (2025-07-22)"}
{"url": "https://opencosmo.readthedocs.io/en/latest/first_steps.html", "title": "First Steps — OpenCosmo Documentation", "content": "# First Steps\nFirst Steps\n\nTo follow this tutorial, download the “haloproperites.hdf5” and “haloparticles.hdf5” files from the\nOpenCosmo Google Drive\nand set the environment variable. This file contains properties of dark-matter halos from a small hydrodynamical simulation run with HACC. You can easily open the data with the\nopen\nfunction:\nimport\nopencosmo\nas\noc\ndataset\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nprint\n(\ndataset\n)\nOpenCosmo Dataset (length=237441)\nCosmology: FlatLambdaCDM(name=None, H0=<Quantity 67.66 km / (Mpc s)>, Om0=0.3096446816186967, Tcmb0=<Quantity 0. K>, Neff=3.04, m_nu=None, Ob0=0.04897468161869667)\nFirst 10 rows:\nblock fof_halo_1D_vel_disp fof_halo_center_x ... sod_halo_sfr unique_tag\n             km / s               Mpc        ... solMass / yr\nint32       float32             float32      ...   float32      int64\n----- -------------------- ----------------- ... ------------ ----------\n    0            32.088795         1.4680439 ...       -101.0      21674\n    0             41.14525        0.19616994 ...       -101.0      44144\n    0             73.82962         1.5071135 ...    3.1447952      48226\n    0             31.17231         0.7526525 ...       -101.0      58472\n    0            23.038841         5.3246417 ...       -101.0      60550\n    0            37.071426         0.5153746 ...       -101.0     537760\n    0            26.203058         2.1734374 ...       -101.0     542858\n    0              78.7636         2.1477687 ...          0.0     548994\n    0             37.12636         6.9660196 ...       -101.0     571540\n    0             58.09235          6.072006 ...    1.5439711     576648\nThe\nopen\nfunction returns a\nDataset\nobject, which holds the raw data as well as information about the simulation. You can easily access the data and cosmology directly as Astropy objects:\ndataset\n.\ndata\ndataset\n.\ncosmology\nThe first will return an astropy table of the data, with all associated units already applied. The second will return the astropy cosmology object that represents the cosmology the simulation was run with.\n## Basic Querying\nBasic Querying\n\nAlthough you can access data directly,\nopencosmo\nprovides tools for querying and transforming the data in a fully cosmology-aware context. For example, suppose we wanted to plot the concentration-mass relationship for the halos in our simulation above a certain mass. One way to perform this would be as follows:\ndataset\n=\ndataset\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\n)\n.\ntake\n(\n1000\n)\n.\nselect\n((\n\"fof_halo_mass\"\n,\n\"sod_halo_cdelta\"\n))\nprint\n(\ndataset\n)\nOpenCosmo Dataset (length=1000)\nCosmology: FlatLambdaCDM(name=None, H0=<Quantity 67.66 km / (Mpc s)>, Om0=0.3096446816186967, Tcmb0=<Quantity 0. K>, Neff=3.04, m_nu=None, Ob0=0.04897468161869667)\nFirst 10 rows:\n fof_halo_mass   sod_halo_cdelta\n    solMass\n    float32          float32\n---------------- ---------------\n11220446000000.0       4.5797048\n17266723000000.0       7.4097505\n51242150000000.0       1.8738283\n70097712000000.0       4.2764015\n51028305000000.0        2.678151\n11960567000000.0       3.9594727\n15276915000000.0        5.793542\n16002001000000.0       2.4318497\n47030307000000.0       3.7146702\n15839942000000.0        3.245569\nWe could then plot the data, or perform further transformations.\n## Data Collections\nData Collections\n\nThis is cool on its own, but the real power of\nopencosmo\ncomes from its ability to work with different data types. Go ahead and download the “haloparticles” file from the\nOpenCosmo Google Drive\nand try the following:\nimport\nopencosmo\nas\noc\ndata\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\nThis will return a data\ncollection\nthat will allow you to query and transform the data as before, but will associate the halos with their particles.\nstructures\n=\ndata\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\n)\n.\ntake\n(\n1000\n,\nat\n=\n\"random\"\n)\nfor\nhalo\nin\nstructures\n.\nhalos\n([\n\"dm_particles\"\n,\n\"star_particles\"\n]):\nhalo_mass\n=\nhalo\n[\n\"halo_properties\"\n][\n\"fof_halo_mass\"\n]\ndm_particles\n=\nhalo\n[\n\"dm_particles\"\n]\nstar_particles\n=\nhalo\n[\n\"star_particles\"\n]\n# do_work\nIn each iteration, the “halo” object will be a dictionary containing a “halo_properties” dictionary as well as two\nopencosmo.Dataset\nobjects, one containing the dark matter particles associated with the halos and the other containing the star particles. Because these are standard datasets, you can perform further transformaions on them as is useful for your analysis."}
{"url": "https://opencosmo.readthedocs.io/en/latest/main_api.html", "title": "Main Transformations API — OpenCosmo Documentation", "content": "# Main Transformations API\nMain Transformations API\n\nopencosmo\nprovides a simple but powerful API for transforming and querying datasets and collections. Both the main\nopencosmo.Dataset\ntype and the various collection types will have these transformations available., although the details of how they behave will differ slightly. Individual collection types may also have additional convinience methods based on their purpose, see\nWorking with Collections\nfor more info. The main transformations are:\nwith_units\n: Change the unit convention of the dataset or collection.\nfilter\n: Filter a dataset based on the value of one more more columns.\nselect\n: Select a subset of columns from a dataset.\ntake\n: Select a subset of rows from a dataset.\nsort_by\n: Sort a dataset by one of its columns\nbound\n: Limit a dataset or collection to a given spatial region.\nwith_new_columns\n: Combine columns in a dataset into a new column with automatic unit handling.\nevaluate\n: Evaluate a computation over all the rows in a dataset or collection.\nEach of these transformations is returns a new dataset or collection with the transformations applied. Because transformations are applied lazily, chaining them together is efficient:\nimport\nopencosmo\nas\noc\n# Load a dataset\nds\n=\noc\n.\nopen\n()\n# Apply a series of transformations\nds\n=\nds\n.\nwith_units\n(\n\"scalefree\"\n)\nds\n=\nds\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\n)\nds\n=\nds\n.\nselect\n([\n\"fof_halo_mass\"\n,\n\"fof_halo_center_x\"\n,\n\"fof_halo_center_y\"\n,\n\"fof_halo_center_z\"\n])\nds\n=\nds\n.\ntake\n(\n100\n,\nat\n=\n\"random\"\n)\nds\n=\nds\n.\nwith_units\n(\n\"physical\"\n)\ndata\n=\nds\n.\ndata\nIn this example, we are we are applying a cut in halo mass using scalefree coordinates, meaning this filter will include all galaxies over 1e13 Msun/h. We then select a subset of the columns and transform them into physical units, removing the factors of h in the final values. See below for more information about unit conventions.\nWhen writing queries like this, it can feel a bit redundant to write\nds\n=\nds.transform(...)\nover and over. In practice it is often more readable to simply apply transformations on top of each other:\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nds\n=\nds\n.\nwith_units\n(\n\"scalefree\"\n)\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\n)\n.\nselect\n([\n\"fof_halo_mass\"\n,\n\"fof_halo_center_x\"\n,\n\"fof_halo_center_y\"\n,\n\"fof_halo_center_z\"\n])\n.\ntake\n(\n100\n,\nat\n=\n\"random\"\n)\n.\nwith_units\n(\n\"physical\"\n)\ndata\n=\nds\n.\ndata\nNote that if you’re working in a Jupyter notebook, you’ll need to use the line continuation character to split the query across multiple lines:\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nds\n=\nds\n\\\n.\nwith_units\n(\n\"scalefree\"\n)\n\\\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\n)\n\\\n.\nselect\n([\n\"fof_halo_mass\"\n,\n\"fof_halo_center_x\"\n,\n\"fof_halo_center_y\"\n,\n\"fof_halo_center_z\"\n])\n\\\n.\ntake\n(\n100\n,\nat\n=\n\"random\"\n)\n\\\n.\nwith_units\n(\n\"physical\"\n)\ndata\n=\nds\n.\ndata\nYou are also free to create multiple derivative datasets from the same original dataset:\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nlow_mass_ds\n=\nds\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\n,\nos\n.\ncol\n(\n\"fof_halo_mass\"\n)\n<\n1e14\n)\n.\nwith_units\n(\n\"phsyical\"\n)\n.\nselect\n([\n\"fof_halo_mass\"\n,\n\"fof_halo_cdelta\"\n])\nhigh_mass_ds\n=\nds\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e14\n)\n.\nwith_units\n(\n\"physical\"\n)\n.\nselect\n([\n\"fof_halo_mass\"\n,\n\"fof_halo_cdelta\"\n])\ndata1\n=\nds1\n.\ndata\ndata2\n=\nds2\n.\ndata\nHowever you may also be interested in including all data that passes\neither\nfilter in a single dataset. You can combine filters with boolean logic using the & and | operators:\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nhigh_mass_cut\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e14\nlow_mass_cut\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n<\n1e12\nlow_concentration_cut\n=\noc\n.\ncol\n(\n\"sod_halo_cdelta\"\n)\n<\n5\nmy_filter\n=\n(\nhigh_mass_cut\n|\nlow_mass_cut\n)\n&\nlow_concentration_cut\nfiltered_ds\n=\nds\n.\nfilter\n(\nmy_filter\n)\nBecause transformations are evaluated lazily, you can have many derivative datasets without incurring a large memory overhead.\n## Unit Conventions\nUnit Conventions\n\nThe\nwith_units\ntransformation is used to change the unit convention of the dataset.\nopencosmo\nsupports the following unit conventions:\nunitless\n: The dataset is read without applying any units\nscalefree\n: The dataset is in “scale-free” units, meaning all lengths are in comoving Mpc/h and all masses are in Msun/h. This is the unit convention that the raw values are stored in.\ncomoving\n: Factors of\nh\nare absorbed into the values, but positions and velocities still use comoving coordinates.\nphysical\n: Factors of\nh\nare absorbed into the values, and positions and velocities are converted to physical coordinates.\nWhen you initially load a dataset, it always uses the “comoving” unit convention. You can change this at any time on any dataset or collection by simply calling\nwith_units\nwith the desired unit convention. For more information, see\nWorking with Units\n## Adding Columns\nAdding Columns\n\nYou can add new columns to a given that are derived from pre-existing columns using the\noc.col()\nto construct new columns and passing them to\nwith_new_columns\n. The new columns will inherit the cosmological dependence of the columns they are created from, and can be used throughout the transformations API as usual.\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nfof_halo_vtotal\n=\n(\noc\n.\ncol\n(\n\"fof_halo_com_vx\"\n)\n**\n2\n+\noc\n.\ncol\n(\n\"fof_halo_com_vy\"\n)\n**\n2\n+\n(\n\"fof_halo_com_vz\"\n)\n**\n2\n)\n**\n(\n0.5\n)\nfof_halo_com_p\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n*\nfof_halo_vtotal\nds\n=\nds\n.\nwith_new_columns\n(\nfof_halo_com_p\n=\nfof_halo_com_p\n)\nThe dataset will now contain a “fof_halo_com_p” column that can be used for filtering and selections as usual. Because the column definition was created outside the dataset itself, it can be used across multiple datasets as needed.\nYou can also simply pass values as a numpy array or astropy quantity:\nimport\nastropy.units\nas\nu\nimport\nnumpy\nas\nnp\nrandom_angle\n=\nnp\n.\nrandom\n.\nuniform\n(\n10\n,\n50\n,\nlen\n(\nds\n))\n*\nu\n.\narcmin\nds\n=\nds\n.\nwith_new_columns\n(\nangle\n=\nrandom_angle\n)\nColumns can be added to collections as well, but there are some subtelties. See\nWorking with Collections\nfor more information.\n## Filtering\nFiltering\n\nFilters operate on columns of a given dataset and return a new dataset that only contains the rows that satisfy the filter. Filters are constructed using the\nopencosmo.col()\nfunction, so they can be constructed independently of any single dataset. Available filters include:\nEquality:\ncol(\"column_name\")\n==\nvalue\nInequality:\ncol(\"column_name\")\n!=\nvalue\nGreater than:\ncol(\"column_name\")\n>\nvalue\nGreater than or equal to:\ncol(\"column_name\")\n>=\nvalue\nLess than:\ncol(\"column_name\")\n<\nvalue\nLess than or equal to:\ncol(\"column_name\")\n<=\nvalue\nMembership:\ncol(\"column_name\").isin([value1,\nvalue2,\n...])\nFilters do not need to include units, however a filter with\nincorrect\nunits will raise an error:\nimport\nastropy.units\nas\nu\nfrom\nastropy.cosmology\nimport\nunits\nas\nu\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\n# This will work fine\nmin_mass\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\nds\n=\nds\n.\nfilter\n(\nmin_mass\n)\n# This will work fine\nmin_mass_unitful\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\n*\nu\n.\nMsun\nds\n=\nds\n.\nfilter\n(\nmin_mass_unitful\n)\n# This will fail, because the masses are not in Msun / h\nmin_mass_unitful\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\n*\nu\n.\nMsun\n/\ncu\n.\nlittleh\nds\n=\nds\n.\nfilter\n(\nmin_mass_unitful\n)\nThe behavior of filters on collections depends on the collection type. See the\nWorking with Collections\npage for more information.\n## Selecting Columns\nSelecting Columns\n\nFor small datasets, it is usually not an issue to request all the columns in a given dataset. However for large datasets, loading everything into memory is slow and consumes singificant quantities of memory. We can use the\nopencosmo.Dataset.select()\ntransformation to select only the subset of columns that are useful for our analysis. Select transformations can be applied sequentially, in which case the second select will only work if it contains columns that were selected in the first select. For example:\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nds\n=\nds\n.\nselect\n([\n\"fof_halo_mass\"\n,\n\"fof_halo_center_x\"\n,\n\"fof_halo_center_y\"\n,\n\"fof_halo_center_z\"\n])\n.\nselect\n([\n\"fof_halo_mass\"\n,\n\"fof_halo_center_x\"\n])\n# This is fine\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nds\n=\nds\n.\nselect\n([\n\"fof_halo_mass\"\n,\n\"fof_halo_center_x\"\n,\n\"fof_halo_center_y\"\n,\n\"fof_halo_center_z\"\n])\n.\nselect\n([\n\"fof_halo_mass\"\n,\n\"sod_halo_cdelta\"\n])\n# This will raise an error, because sod_halo_cdelta was not in the first select\nFilters and selects generally behave as you might expect. If you select\nafter\nfiltering, the resulting dataset will only have the columns that were selected for the rows that passed the filter. If you select\nbefore\nfiltering, the filter can only use columns that were included in the select. For example, this works:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nds\n=\nds\n.\nselect\n([\n\"fof_halo_mass\"\n,\n\"fof_halo_center_x\"\n,\n\"fof_halo_center_y\"\n,\n\"fof_halo_center_z\"\n])\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\n)\nas does this:\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nds\n=\nds\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\n)\n.\nselect\n([\n\"fof_halo_center_x\"\n,\n\"fof_halo_center_y\"\n,\n\"fof_halo_center_z\"\n])\n# This is also fine\nbut this will raise an error:\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nds\n=\nds\n.\nselect\n([\n\"fof_halo_center_x\"\n,\n\"fof_halo_center_y\"\n,\n\"fof_halo_center_z\"\n])\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\n)\n# fof_halo_mass is not in the dataset when \"filter\" is called.\n## Taking Rows\nTaking Rows\n\nThe\nopencosmo.Dataset.take()\ntransformation is used to select a subset of rows from a dataset. The\nat\nargument can be used to specify how the rows are selected. The available options are:\nat=\"random\"\n: Select a random subset of n rows from the dataset (default).\nat=\"start\"\n: Select the first n rows from the dataset.\nat=\"end\"\n: Select the last n rows from the dataset.\nAs with the\nselect\ntransformations,\ntake\ntransformations can be chained together. However you cannot take more rows than are present in the dataset:\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nds\n=\nds\n.\ntake\n(\n100\n,\nat\n=\n\"random\"\n)\n.\ntake\n(\n500\n,\nat\n=\n\"random\"\n)\n# This will raise an error\nYou can also take a range of rows with\nopencosmo.Dataset.take_range()\n. As with all other transformations, this creates a new dataset so the following is valid:\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nds\n=\nds\n.\ntake_range\n(\n500\n,\n1000\n)\n.\ntake\n(\n100\n,\nat\n=\n\"start\"\n)\nThis will take the rows 500-1000 from the original dataset, and then take the first 100 rows from that new dataset. The original dataset is unchanged.\n## Sorting\nSorting\n\nYou can re-order a dataset based on the value of some column with\nopencosmo.Dataset.sort_by()\n. By default, this sorts in ascending order (from lowest to highest), however you can sort in descending order by passing\ninvert\n=\nTrue\n.\nFor example, to get the 100 most massive halos in a given simulation, ordered from most to least massive:\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nds\n=\nds\n.\nsort_by\n(\n\"fof_halo_mass\"\n,\ninvert\n=\nTrue\n)\n.\ntake\n(\n100\n,\nat\n=\n\"start\"\n)\nOr, to get the 100\nleast\nmassive halos, ordered from least to most massive:\nds\n=\nds\n.\nsort_by\n(\n\"fof_halo_mass\"\n)\n.\ntake\n(\n100\n,\nat\n=\n\"start\"\n)\nYou can also use\ntake\nin clever ways to get other results. For example, to get the 100\nmost\nmassive halos but ordered from\nleast to most massive\n:\nds\n=\nds\n.\nsort_by\n(\n\"fof_halo_mass\"\n)\n.\ntake\n(\n100\n,\nat\n=\n\"end\"\n)\n## Spatial Querying\nSpatial Querying\n\nOpenCosmo data contains a spatial index which makes it efficient to perform spatial queries on the data. These queries can be performed by defining a region, and then passing it into\nopencosmo.Dataset.bound()\n:\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nregion\n=\noc\n.\nmake_box\n((\n20\n,\n20\n,\n20\n),\n(\n40\n,\n40\n,\n40\n))\nbound_ds\n=\nds\n.\nbound\n(\nregion\n)\nFor lightcone data, spatial queries are performed using two dimensional regions on the sky. For example:\nimport\nastropy.units\nas\nu\nfrom\nastropy.coordinates\nimport\nSkyCoord\nds\n=\noc\n.\nopen\n(\n\"lc_haloproperties.hdf5\"\n)\ncenter\n=\nSkyCoord\n(\n45\n*\nu\n.\ndeg\n,\n-\n30\n*\nu\n.\ndeg\n)\nradius\n=\n30\n*\nu\n.\narcmin\nregion\n=\nopencosmo\n.\nmake_cone\n(\ncenter\n,\nradius\n)\nbound_ds\n=\nds\n.\nbound\n(\nregion\n)\nSee\nRegions\nfor more information about constructing regions.\nAs with other transformations, spatial queries can be chained together to build complex query pipelines. If a given region contains no data, the spatial query will return a dataset with length zero.\nThere are some complications that arise when working with spatial queries in an MPI context. See\nWorking with MPI\nfor more details.\n## Iterating Over Rows\nIterating Over Rows\n\nIf you want to work row-by-row, you can always iterate over the dataset with\nopencosmo.Dataset.rows()\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nfor\nrow\nin\nds\n.\nrows\n():\n# Do something with the row\nprint\n(\nrow\n[\n\"fof_halo_mass\"\n],\nrow\n[\n\"fof_halo_center_x\"\n])\nAt each iteration, the row will be a dictionary of values for the specified rows with units applied. If you only need a subset of the columns, consider using\nopencosmo.Dataset.select()\nto select only those columns before iteration.\n## Evaluating Complex Expressions\nEvaluating Complex Expressions\n\nGenerally, basic data manipulation is not sufficient for science. We need to fit models and perform complex operations. The\nevaluate\nmethod can handle the low-level data management, leaving you to focus on building your model. See\nEvaluating Complex Expressions on Datasets and Collections\nfor more information."}
{"url": "https://opencosmo.readthedocs.io/en/latest/cols.html", "title": "Working with Columns — OpenCosmo Documentation", "content": "# Working with Columns\nWorking with Columns\n\nYou can use the toolkit to query your data based on the value of a column, or create new columns based on combinations of columns that already exist in the dataset.\n## Querying Based on Column Values\nQuerying Based on Column Values\n\nQuerying your dataset based on the value of a single column is straightforward:\n# select halos with mass greather than 1e13 Msun\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nquery\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e14\nds\n=\ndata\n.\nfilter\n(\nquery\n)\nBecause the query is contructed outside the dataset itself, you are free to use it across several datasets at the same time. Because queries create new datasets, you can query the same dataset multiple times easily:\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nquery_high\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n<\n5e13\nquery_low\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\nds_low\n=\ndata\n.\nfilter\n(\nquery_low\n)\nds_high\n=\ndata\n.\nfilter\n(\nquery_high\n)\nYou can also combine multiple queries to create more specific datasets:\n# select halos with mass between 1e13 and 5e13 Msun\nlower_bound\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\nupper_bound\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n<\n5e13\nds_bounded\n=\nds\n.\nfilter\n(\nlower_bound\n,\nupper_bound\n)\nAs well as multiple queries across multiple columns.\n# select high-concentration halos between mass of 1e13 and 5e13\nlower_bound\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\nupper_bound\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n<\n5e13\nc_bound\n=\noc\n.\ncol\n(\n\"sod_halo_cdelta\"\n)\n>\n3\nds_bounded\n=\nds\n.\nfilter\n(\nlower_bound\n,\nupper_bound\n,\nc_bound\n)\nThe value in the query is always evaluated in the unit conventions of the dataset. For example, the following two queries will find different rows, even if the\nds_scalefree\ndataset is transformed back to comoving units after the fact:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nquery\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e14\nds_scalefree\n=\ndata\n.\nwith_units\n(\n\"scalefree\"\n)\n.\nfilter\n(\nquery\n)\nds_comoving\n=\ndata\n.\nfilter\n(\nquery\n)\nFor more information, see\nUnit Conventions\n.\n## Querying In Collections\nQuerying In Collections\n\nQueries can generally be performed as usual on collections. In a\nopencosmo.StructureCollection\n, the query will be performed on the properties of the structures within the structure collection. For example:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\nds\n=\nds\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e14\n)\nThe resultant\nopencosmo.StructureCollection\nwill contain only halos with a mass greater than 10^14, along with all their associated particles.\nIn a\nopencosmo.SimulationCollection\n, the filter will be applied to all datasets inside the collection.\nFor more details, see\nWorking with Collections\n.\n## Adding Custom Columns\nAdding Custom Columns\n\nYou can use the\nDataset.with_new_columns\nto add new columns to your data. You can either combine existing columns to create new ones, or create wholly-new columns by providing data as a numpy array or astropy quantity array.\n### Combining Columns into New Columns\nCombining Columns into New Columns\n\nYou can use\nopencosmo.col()\nto combine columns to create new columns. Because these new columns are created from pre-existing colums, they will behave as expected under transformations such as a change in unit convention.\nfof_halo_speed_sqrd\n=\noc\n.\ncol\n(\n\"fof_halo_com_vx\"\n)\n**\n2\n+\noc\n.\ncol\n(\n\"fof_halo_com_vy\"\n)\n**\n2\n+\noc\n.\ncol\n(\n\"fof_halo_com_vz\"\n)\n**\n2\nfof_halo_ke\n=\n0.5\n*\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n*\nfof_halo_speed_sqrd\nds\n=\nds\n.\nwith_new_columns\n(\nfof_halo_ke\n=\nfof_halo_ke\n)\nds\n=\nds\n.\nwith_units\n(\n\"physical\"\n)\nYou can also always add multiple derived columns in a single call:\nfof_halo_speed_sqrd\n=\noc\n.\ncol\n(\n\"fof_halo_com_vx\"\n)\n**\n2\n+\noc\n.\ncol\n(\n\"fof_halo_com_vy\"\n)\n**\n2\n+\noc\n.\ncol\n(\n\"fof_halo_com_vz\"\n)\n**\n2\nfof_halo_ke\n=\n0.5\n*\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n*\nfof_halo_speed_sqrd\nfof_halo_p\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n*\nfof_halo_speed_sqrd\n**\n0.5\nds\n=\nds\n.\nwith_new_columns\n(\nfof_halo_ke\n=\nfof_halo_ke\n,\nfof_halo_p\n=\nfof_halo_p\n)\nopencosmo.Dataset.with_new_columns()\nchecks to ensure that the columns you are using already exist in the dataset and that the units of the various columns match. For example\n# Forgot to square the x velocity!\nfof_halo_speed_sqrd\n=\noc\n.\ncol\n(\n\"fof_halo_com_vx\"\n)\n+\noc\n.\ncol\n(\n\"fof_halo_com_vy\"\n)\n**\n2\n+\noc\n.\ncol\n(\n\"fof_halo_com_vz\"\n)\n**\n2\nfof_halo_ke\n=\n0.5\n*\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n*\nfof_halo_speed_sqrd\nds\n=\nds\n.\nwith_new_columns\n(\nfof_halo_ke\n=\nfof_halo_ke\n)\nyou will get an error.\nopencosmo.units.UnitError: To add and subtract columns, units must be the same!\n### Built-In Column Combinations\nBuilt-In Column Combinations\n\nOpenCosmo provides a number of built-in column combinations that may be useful for a wide variety of analysis. For example, to calculate the total velocity of a halo from its velocity components:\nimport\nopencosmo\nas\noc\nfrom\nopencosmo.columns\nimport\nnorm_cols\ndataset\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\ntotal_halo_velocity\n=\nnorm_cols\n(\n\"fof_halo_com_vx\"\n,\n\"fof_halo_com_vy\"\n,\n\"fof_halo_com_vz\"\n)\ndataset\n=\ndataset\n.\nwith_new_columns\n(\nfof_halo_com_velocity\n=\ntotal_halo_velocity\n)\nYou can find a list of available column combinations in the\ncolumn API reference\n### Adding Columns Manually\nAdding Columns Manually\n\nThe\nwith_new_columns\nmethod accepts numpy arrays and astropy quantity arrays. These arrays must be the same length as the dataset they are being added to. Note that if your data has units, these units will not be transformed under calls to\nwith_units\n.\nrandom_data\n=\nnp\n.\nrandom\n.\nrandint\n(\n0\n,\n1000\n,\nsize\n=\nlen\n(\nds\n))\n*\nu\n.\ns\ndataset\n=\ndataset\n.\nwith_new_columns\n(\nrandom_time\n=\nrandom_data\n)\n### Creating New Columns in Collections\nCreating New Columns in Collections\n\nCalls to\nopencosmo.StructureCollection.with_new_columns()\nmust explicitly say which dataset the column is being added to:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\nfof_halo_speed_sqrd\n=\noc\n.\ncol\n(\n\"fof_halo_com_vx\"\n)\n+\noc\n.\ncol\n(\n\"fof_halo_com_vy\"\n)\n**\n2\n+\noc\n.\ncol\n(\n\"fof_halo_com_vz\"\n)\n**\n2\nfof_halo_ke\n=\n0.5\n*\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n*\nfof_halo_speed_sqrd\nfof_halo_p\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n*\nfof_halo_speed_sqrd\n**\n0.5\nds\n=\nds\n.\nwith_new_columns\n(\ndataset\n=\n\"halo_properties\"\n,\nfof_halo_ke\n=\nfof_halo_ke\n,\nfof_halo_p\n=\nfof_halo_pe\n)\nCalls to\nopencosmo.SimulationCollection.with_new_columns()\nwill always apply the new columns to all the datasets in the collection. Because of this, passing a numpy array or astropy quantity object will generally not work, since the length of the datasets within the collection will be different. You can always add a column to a single dataset in the collections by passing the optional\ndataset\nparameter:\nrandom_data\n=\nnp\n.\nrandom\n.\nrandint\n(\n0\n,\n100\n,\nlen\n(\ncollection\n[\n\"simulation_a\"\n]))\ncollection\n=\ncollection\n.\nwith_new_columns\n(\ndataset\n=\n\"simulation_a\"\n,\nrandom_data\n=\nrandom_data\n)"}
{"url": "https://opencosmo.readthedocs.io/en/latest/collections.html", "title": "Working with Collections — OpenCosmo Documentation", "content": "# Working with Collections\nWorking with Collections\n\nMultiple datasets can be grouped together into\ncollections.\nA collection allows you to perform high-level operations across many datasets at a time, and link related datasets together. In general, collections implement the same\nMain Transformations API\nas the\nopencosmo.Dataset\nclass, with some important caveats (see below).\nDatasets behave a lot like dictionaries. You can get the names of the dataset with\ncollection.keys()\n, the datasets themselves with\ncollection.values()\n, or both with\ncollection.items()\n. A given dataset within the collection can always be accessed with\ncollection[dataset_name]\n.\n## Types of Collections\nTypes of Collections\n\nOpenCosmo currently implements three collection types.\nopencosmo.Ligthcone\ncollections stack datasets in angular coordinates from  different redshift slices into a single dataset-like object.\nopencosmo.SimulationCollection\ncollections hold\nopencosmo.Dataset\nor\nopencosmo.StructureCollection\nof the same type from several simulations.\nopencosmo.StructureCollection\ncollections hold multiple data types from a single collection, grouped by object. A\nopencosmo.HealpixMap\nholds one or more on-sky maps. See below for information of how these collections can be used.\nCollections can be opened just like datasets using\nopencosmo.open()\n, and written with\nopencosmo.write()\n.\n## Lightcones\nLightcones\n\nLightcones hold datasets from several different redshift steps, which are stacked to form a single dataset. In general, the\nLightcone\nAPI is identical to the standard\nDataset\nAPI with the addition of some extra convinience functions. For example:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"step_600/haloproperties.hdf5\"\n,\n\"step_601/haloproperties.hdf5\"\n)\nds\n=\nds\n.\nwith_redshift_range\n(\n0.040\n,\n0.0405\n)\nprint\n(\nds\n)\nOpenCosmo Lightcone Dataset (length=31487, 0.04 < z < 0.0405)\nCosmology: FlatLambdaCDM(name=None, H0=<Quantity 67.66 km / (Mpc s)>, Om0=0.3096446816186967, Tcmb0=<Quantity 0. K>, Neff=3.04, m_nu=None, Ob0=0.04897468161869667)\nFirst 10 rows:\nblock    chi     fof_halo_1D_vel_disp fof_halo_center_a fof_halo_center_x ... sod_halo_radius   theta   unique_tag [replication, halo_tag]   redshift\n                        km / s                                 Mpc        ...       Mpc\nint64  float32         float32             float32           float32      ...     float32      float32            (int32, int64)             float32\n----- ---------- -------------------- ----------------- ----------------- ... --------------- --------- ---------------------------------- -----------\n    0  120.25088            29.816034         0.9610807         126.16309 ...       -149.2758 1.5622419             (1049601, 60016004986) 0.040495396\n    2 120.031685            21.276937         0.9611496         126.76229 ...       -149.2758 1.5499203             (1049601, 60174689392)  0.04042077\n    9  119.12508            35.604523         0.9614344         125.27003 ...       -149.2758 1.5308051             (1049601, 58587164999) 0.040112495\n    9 119.718056             45.24862        0.96124804        126.242195 ...       -149.2758 1.5266299             (1049601, 59222154598) 0.040314198\n    9  119.43137            24.338223         0.9613382        126.190384 ...       -149.2758 1.5229564             (1049601, 59380902001) 0.040216565\n   10   119.0838             37.28911         0.9614474         127.76812 ...       -149.2758 1.5297077             (1049601, 61285757392)  0.04009843\n   10  120.16473            42.233627         0.9611078         129.43185 ...       -149.2758 1.5301584             (1049601, 62238266991)  0.04046607\n   10 119.903854             46.74386        0.96118975         128.91646 ...       -149.2758 1.5263848             (1049601, 62555736594)  0.04037726\n   11 119.049706            49.151897        0.96145815         126.78099 ...      0.12496548 1.5141958             (1049601, 60492121202) 0.040086865\n   11  119.60659             43.32933         0.9612831        127.099144 ...       -149.2758  1.519284             (1049601, 60492184203)  0.04027629\n## Healpix Maps\nHealpix Maps\n\nMaps contain pixelized spatial data, integrated over a redshift range, in a single dataset. This may contain one or more different types of data (e.g. x-ray emission or projected density fields) for a given simulation and range. The\nHealpixMap\nAPI is identical to the standard\nDataset\nAPI, with a few notable differences. We do not support unit conversions, as we do not assume fine-grained redshift sampling, and instead keep everything in the observed frame. We also provide the data in either Healpix Map format (a numpy array with nested pixelization), or Healsparse format (a sparse implementation which supports partial sky coverage). Some knowledge of Healsparse formats may be useful to work with this data. A simple pixel, value return is demonstrated below.\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\nhealpix_map_path\n)\nds\n.\nget_data\n()\n{'ksz': HealSparseMap: nside_coverage = 64, nside_sparse = 2048, float32, 50331648 valid pixels,\n 'tsz': HealSparseMap: nside_coverage = 64, nside_sparse = 2048, float32, 50331648 valid pixels}\ntsz_map\n=\nds\n.\ndata\n[\n'tsz'\n]\npix_list\n=\ntsz_map\n.\nvalid_pixels\nvals\n=\ntsz_map\n.\nget_values_pix\n(\npix_list\n)\n## Simulation Collections\nSimulation Collections\n\nSimulationCollections implement an identical API to the\nopencosmo.Dataset\nor\nopencosmo.StructureCollection\nit holds. All operations will automatically be mapped over all datasets held by the collection, which will always be of the same type. See the documentation for those classes for more information\n## Structure Collections\nStructure Collections\n\nA Structure Collection contains datasets of multiple types that are linked together by they structure (halo or galaxy) they are associated with in the simulation. Structure collections always contain at least one\nproperties\ndataset, and one or more particle or profile dataset.\nYou can always access the individual datasets in the collection just as you would values in a dictionary:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\ndm_particles\n=\ndata\n[\n\"dm_particles\"\n]\nHowever the real power of working with a\nStructureCollection\nis the automatic grouping of these datasets by structure. You can iterate through the structures in the dataset easily:\nimport\nopencosmo\nas\noc\ndata\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\nfor\nhalo\nin\ndata\n.\nhalos\n():\nprint\n(\nhalo\n)\nAt each iteration of the loop,\nstructure\nwill contain a dictionary of the properties and datasets associated with the given halo.\nIf you don’t need all the particle species, you can always select one or multiple that you actually care about when you do the iteration:\nfor\nstructure\nin\ndata\n.\nobjects\n([\n\"dm_particles\"\n,\n\"gas_particles\"\n]):\n# do work\nWhere\nstructure\nwill now be a dictionary containing three things:\nstructure[\"halo_properties\"]\nwill be a dictionary of the halo properties for the given halo.\nstructure[\"dm_particles\"]\nwill be an\nopencosmo.Dataset\nwith the dark matter particles associated with the halo\nstructure[\"gas_particles\"]\nwill be an\nopencosmo.Dataset\nwith the gas particles associated with the halo\nIt is also possible for structure collections to contain other structure collections. For example, in a hydro simulation a single halo may contain more than one galaxy.\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n,\n\"galaxyproperties.hdf5\"\n,\n\"galaxyparticles.hdf5\"\n)\nfor\nstructure\nin\nds\n.\nhalos\n():\ngals_ds\n=\nstructure\n[\n\"galaxies\"\n]\nfor\ngalaxy\nin\ngals_ds\n.\ngalaxies\n():\n# do work with galaxies.\nYou can now iterate through galaxies in the galaxies in the halo just as you would iterate through halos in your full dataset.\nBecause the structure collection returns regular\nopencosmo.Dataset\nobjects, you can query or transform them further as needed.\nParticles take a lot of space on disk, and it is common to only store particles for a certain subset of halos in a simulation. By default,\nopencosmo\nwill filter out structures that do not have any associated particles. If you want to disable this behavior, you can set the\nignore_empty\nflag to\nFalse\nwhen you open the collection:\nimport\nopencosmo\nas\noc\ndata\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n,\nignore_empty\n=\nFalse\n)\nfor\nhalo\nin\ndata\n.\nhalos\n():\n# Will now include halos that have no particles\nprint\n(\nhalo\n)\n## Transformations on Structure Collections\nTransformations on Structure Collections\n\nStructure Collections implement the\nMain Transformations API\n, but with some important differences to behavior.\nFilters Apply to the Halo/Galaxy Properties\nStructure Collections always contain a property dataset that contains the high-level information about the structures in the dataset. Filters by default will always be applied on this dataset.\nFor example, calling “filter” on the structure collection will always operate on columns in the propeties dataset. For example, suppose you have a large collection of halos and their associated particles and you want to work only on halos greater than 10^13 m_sun:\nimport\nopencosmo\nas\noc\ndata\n=\noc\n.\nopen\n(\n\"my_collection.hdf5\"\n)\ndata\n=\ndata\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e13\n)\nfor\nhalo\nin\ndata\n.\nobjects\n():\n# do work\nIf your collection contains both a halo properties dataset and a galaxy properties dataset, you can filter based on the galaxy properties by passing an additional argument like so:\nimport\nopencosmo\nas\noc\ndata\n=\noc\n.\nopen\n(\n\"my_collection.hdf5\"\n)\ndata\n=\ndata\n.\nfilter\n(\noc\n.\ncol\n(\n\"gal_mass\"\n)\n>\n1e11\n,\ndataset\n=\n\"galaxy_properties\"\n)\nHowever this comes with an important caveat. Filtering based on properties of a galaxy removes any halo that does not contain any a galaxy that meets the threshold. If a halo hosts multiple galaxies and at least one meets the criteria, all galaxies in the halo will be retained.\nSelect Can Be Made on a Per-Dataset Basis\nYou can always select subests of the columns in any of the individual datasets while keeping them housed in the collection\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"my_collection.hdf5\"\n)\nds\n=\ndata\n.\nselect\n([\n\"x\"\n,\n\"y\"\n,\n\"z\"\n]),\ndataset\n=\n\"dm_particles\"\n)\nIf the “dataset” argument is not provided, the selection will be performed on the property dataset.\nUnit Transformations Apply to All Datasets\nTransforming to a different unit convention is identical to\nopencosmo.Dataset.with_units()\nand always applies to all datasets in the collection:\nimport\nopencosmo\nas\noc\ndata\n=\noc\n.\nopen\n(\n\"my_collection.hdf5\"\n)\ndata\n=\ndata\n.\nwith_units\n(\n\"scalefree\"\n)\nTake Operations Take Structure\nCalling\nopencosmo.StructureCollection.take()\nwill create a new\nStructureDataset\nwith the number of structures specified in the take operation. This means the following operation will behave as you might expect:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"my_collection.hdf5\"\n)\nds\n=\nds\n.\ntake\n(\n10\n)\nfor\nhalo\n,\nparticles\nin\nds\n.\nobjects\n():\n# this loop iterate over 10 halos"}
{"url": "https://opencosmo.readthedocs.io/en/latest/units.html", "title": "Working with Units — OpenCosmo Documentation", "content": "# Working with Units\nWorking with Units\n\nThe raw output of cosmological simulations produced with HACC (and many other cosmology codes) is in\nscale free\nunits: coordinates and velocities use\ncomoving coordinates\nand values such as mass include terms of the reduced Hubble constant\nh\n. Some downstream data products, such as synthetic galaxy catalogs, may be in other conventions.\n## Converting Between Unit Conventions\nConverting Between Unit Conventions\n\nOpenCosmo allows you to convert between these conventions as needed based on what is most convinient for your analysis. The default convention is “comoving,” which leaves coordinates and velocities in comoving units but absorbs any factors of h into their values. The other available conventions are “physical”, “scalefree” and “unitless.”\nThe “comoving”, “physical” and “unitless” conventions will always be available, while the “scalefree” convention will only be available if the raw data is already stored in this convention. The “unitless” convention will always return the raw data in the file, regardless of the convention the data is stored in.\nYou can readily convert between these conventions with the\nopencosmo.Dataset.with_units()\nmethod:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nread\n(\n\"galaxyproperties.hdf5\"\n)\nds\n=\nds\n.\nwith_units\n(\n\"physical\"\n)\nYou can also have work with a datasets in multiple unit conventions at the same time.\nimport\nopencosmo\nas\noc\n# comoving is the default\nds_comoving\n=\noc\n.\nread\n(\n\"galaxyproperties.hdf5\"\n)\nds_physical\n=\nds_comoving\n.\nwith_units\n(\n\"physical\"\n)\n# ds_comoving is not changed\nWhen you filter a dataset with\nopencosmo.Dataset.filter()\n, the filtering will be performed in the same unit convention as the dataset. If you provide a unitless value to a filter, it will be interpreted as a value with the same units as the column in question.\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nread\n(\n\"haloproperties.hdf5\"\n)\n.\nwith_units\n(\n\"scalefree\"\n)\nds_filtered\n=\nds\n.\nfilter\n(\nds\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e10\n)\n# This will filter out halos with mass greater than 1e10 * h^-1 Msun\nds_comoving\n=\nds\n.\nwith_units\n(\n\"comoving\"\n)\nds_filtered_comoving\n=\nds_comoving\n.\nfilter\n(\nds_comoving\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e10\n)\n# This will filter out halos with mass greater than 1e10 Msun (no factor of h!)\nIf you change unit conventions after performing a filter, the filter will still be applied in the original unit convention. For example:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nread\n(\n\"haloproperties.hdf5\"\n)\n.\nwith_units\n(\n\"scalefree\"\n)\nds_filtered\n=\nds\n.\nfilter\n(\nds\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e10\n)\n.\nwith_units\n(\n\"comoving\"\n)\nprint\n(\nds_filtered\n.\ndata\n[\n\"fof_halo_mass\"\n]\n.\nmin\n())\nshould output a value of ~1.5 x 10^10 Msun, which is about 1e10/0.67.\n## Converting Columns to an Equivalent Unit\nConverting Columns to an Equivalent Unit\n\nYou can convert all columns with a given unit into a different unit with the\nconversions\nargument:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nread\n(\n\"haloproperties.hdf5\"\n)\nconversions\n=\n{\nu\n.\nMpc\n:\nu\n.\nlyr\n}\nds\n=\nds\n.\nwith_units\n(\nconversions\n=\nconversions\n)\nIn the new dataset, all columns that originally had units of megaparsecs will be converted to lightyears. Composite units including megaparsec (e.g. km / s / Mpc, Mpc^2) will\nnot\nbe converted. All-column conversions are always peformed after a change of unit conventions. Changing units\nafter\ndoing a conversion always clears the conversions.\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nread\n(\n\"haloproperties.hdf5\"\n)\nconversions\n=\n{\nu\n.\nMpc\n:\nu\n.\nlyr\n}\nds\n=\nds\n.\nwith_units\n(\nconversions\n=\nconversions\n)\n## Single-Column Conversions\nSingle-Column Conversions\n\nYou can also use\nwith_units\nto convert the values in individual columns to their values in an equivalent unit:\nimport\nastropy.units\nas\nu\ndataset\n=\noc\n.\nread\n(\n\"haloproperties.hdf5\"\n)\n.\nwith_units\n(\nfof_halo_center_x\n=\nu\n.\nlyr\n,\nfof_halo_center_y\n=\nu\n.\nlyr\n,\nfof_halo_center_z\n=\nu\n.\nlyr\n,\n)\nUnit conversions like these are always performed\nafter\nany change in unit convention, and changing unit conventions clears any existing unit conversions:\n# this works\ndataset\n=\ndataset\n.\nwith_units\n(\nfof_halo_mass\n=\nu\n.\nkg\n)\n# this clears the previous conversion,\n# the masses are now in Msun / h\ndataset\n=\ndataset\n.\nwith_units\n(\n\"scalefree\"\n)\n# This now fails, because the units of masses\n# are Msun / h, which cannot be converted to kg\ndataset\n=\ndataset\n.\nwith_units\n(\nfof_halo_mass\n=\nu\n.\nkg\n)\n# this will work, the units of halo mass in the \"physical\"\n# convention are Msun (no h), and the change of convention\n# happens before the conversions\ndataset\n=\ndataset\n.\nwith_units\n(\n\"physical\"\n,\nfof_halo_mass\n=\nu\n.\nkg\n,\nfof_halo_center_x\n=\nu\n.\nlyr\n)\n# reset all units\ndataset\n=\ndataset\n.\nwith_units\n(\n\"physical\"\n)\nUnit conversions on\nLightcones\nand\nSimulationCollections\nbehave identically to single datasets. In\nStructureCollections\n, unit conversions must be passed on a per-dataset basis:\nimport\nastropy.units\nas\nu\nstructures\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\nstructures\n=\nstructures\n.\nwith_units\n(\nhalo_properties\n=\n{\n\"fof_halo_mass\"\n:\nu\n.\nkg\n},\ndm_particles\n=\n{\n\"mass\"\n:\nu\n.\nkg\n}\n)\nAs with all-column conversions, composite units that include the target unit will not be converted. If you want to convert a composite unit, the conversion must be stated seperately.\n## Conversion Precedence\nConversion Precedence\n\nIn cases where a blanket conversion is provided alongside a conversion for a specific column, the specific conversion always take precedence:\nimport\nastropy.units\nas\nu\nconversions\n=\n{\nu\n.\nMpc\n:\nu\n.\nlyr\n}\nds\n=\nds\n.\nwith_units\n(\nconversions\n=\nconversions\n,\nfof_halo_center_x\n=\nu\n.\nkm\n)\nAll columns with units of megaparsecs will be converted to lightyears, except for the\nfof_halo_center_x\ncolumn which will be converted to kilometers.\n## Structure Collection Conversions\nStructure Collection Conversions\n\nWhen working with a structure collection, you can provide conversions that apply to the entire collection, as single dataset inside the collection, or individual columns within a given dataset. As you might expect, conversions on an individual dataset takes precedence over those that apply to all datasets.\nimport\nastropy.units\nas\nu\nconversions\n=\n{\nu\n.\nMpc\n:\nu\n.\nlyr\n}\nstructures\n=\nstructures\n.\nwith_units\n(\nconversions\n=\nconversions\nhalo_properties\n=\n{\n\"conversions\"\n:\n{\nu\n.\nMpc\n:\nu\n.\nkm\n},\n\"fof_halo_center_x\"\n:\nu\n.\nm\n}\n)\nIn this example, all values in Mpc will be converted to lightyears, except in the “halo_properties” dataset, where they will be converted to kilometers. The column “fof_halo_center_x” in “halo_properties” will be converted to meters instead.\n## Clearing Conversions\nClearing Conversions\n\nConversions are always cleared when changing unit conventions, or you can also clear them by calling\nwith_units\nwith no arguments.\ndataset\n=\noc\n.\nread\n(\n\"haloproperties.hdf5\"\n)\n.\nwith_units\n(\nconversions\n=\n{\nu\n.\nMpc\n:\nu\n.\nlyr\n},\nfof_halo_center_x\n=\nu\n.\nlyr\n,\n)\ndataset\n=\ndataset\n.\nwith_units\n()\n# all unit conversion reset"}
{"url": "https://opencosmo.readthedocs.io/en/latest/io.html", "title": "Reading and Writing Data in the OpenCosmo Format — OpenCosmo Documentation", "content": "# Reading and Writing Data in the OpenCosmo Format\nReading and Writing Data in the OpenCosmo Format\n\nOpenCosmo defines a data format for storing simulation data in hdf5 files. A dataset in this format can be transformed and written by the OpenCosmo library to produce a new file that can be read by others (or yourself at a later date!) using the library.\n## Options for Reading Data\nOptions for Reading Data\n\nAny single opencosmo file can be open with\nopencosmo.open()\n. This function will parse the structure of the file and return the appropriate object, such as a\nopencosmo.Dataset\nor\nopencosmo.StructureCollection\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nYou can also use open as a context manager to automatically close the file when you’re done with it:\nimport\nopencosmo\nas\noc\nwith\noc\n.\nopen\n(\n\"galaxyproperties.hdf5\"\n)\nas\nds\n:\nprint\n(\nds\n.\ndata\n)\nWhen opening multiple files that are linked to each other, use\nopencosmo.open()\n## Writing Data\nWriting Data\n\nWriting data to a new file is straightforward:\noc\n.\nwrite\n(\n\"my_output.hdf5\"\n,\nds\n)\nTransformations applied to the data will propogate to the file when written, with the exception of\nwith_units\nand\nsort_by\n. Data is always stored in the same unit convention as its source dataset, and ordering according to a spatial index.\n## Other Formats\nOther Formats\n\nOpenCosmo support dumping some data into other formats. These new files will not be readable by the toolkit, but may be more convinient for your specific usecase. You can install all additional io dependencies with\npip\ninstall\n\"opencosmo[io]\"\nor you can install individual output formats (see below).\n### Parquet\nParquet\n\nYou can dump a\nDatast\n,\nLightcone\n, or parts of a\nStructureCollection\nto parquet with\nopencosmo.io.write_parquet()\n. You will need to install pyarrow with parquet support first:\npip\ninstall\n\"pyarrow[parquet]\"\nA dataset will simply be dumped as a collection of columns. Any querying (selection, filtering, etc.) will persist into the output. Metadata such as unit information and the spatial index will not be included:\nimport\nopencosmo\nas\noc\nfrom\nopencosmo.io\nimport\nwrite_parquet\ndataset\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nwrite_parquet\n(\n\"my_dataset.parquet\"\n,\ndataset\n)\nYou can also write the particles of a\nStructureCollection\n.\nstructures\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\nwrite_parquet\n(\n\"my_structure/\"\n,\nstructures\n)\nThis will produce one parquet file for each particle type in the collection."}
{"url": "https://opencosmo.readthedocs.io/en/latest/evaluating.html", "title": "Evaluating Complex Expressions on Datasets and Collections — OpenCosmo Documentation", "content": "# Evaluating Complex Expressions on Datasets and Collections\nEvaluating Complex Expressions on Datasets and Collections\n\nSometimes, it’s possible to compute new quantities with simple algebraic combinations of columns that are already in your dataset. In these cases, you can use\nwith_new_columns\nto efficiently create new columns out of existing ones. However most interesting science requires more sophisticated options. Maybe you have a large number of dark matter halos and you want to find the best-fit NFW profile for each of them. Or maybe you are interested in galaxy clusters, and want to determine the most massive galaxy within each cluster-scale halo.\nFor these kinds of cases, OpenCosmo provides the\nevaluate\nmethod.\nevaluate\ntakes a function you provide it and evaluates over all the rows in your dataset. When you’re working with a\nStructureCollection\nyou can use\nevaluate\nto evalute your function over all\nstructures\nin the collection, potentially performing complex computations that involve multiple particle species.\n## Evaluating on Datasets\nEvaluating on Datasets\n\nTo evaluate a function on all rows in a single dataset, simply write a function that takes in arguments with the same name as some of the columns in your dataset and returns a dictionary of values:\nimport\nopencosmo\nas\noc\ndataset\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nds\n=\noc\n.\nopen\n(\ninput_path\n)\n.\nfilter\n(\noc\n.\ncol\n(\n\"sod_halo_cdelta\"\n)\n>\n0\n)\ndef\nnfw\n(\nsod_halo_radius\n,\nsod_halo_cdelta\n,\nsod_halo_mass\n):\nr_\n=\nnp\n.\nlogspace\n(\n-\n2\n,\n0\n,\n50\n)\nA\n=\nnp\n.\nlog\n(\n1\n+\nsod_halo_cdelta\n)\n-\nsod_halo_cdelta\n/\n(\n1\n+\nsod_halo_cdelta\n)\nhalo_density\n=\nsod_halo_mass\n/\n(\n4\n/\n3\n*\nnp\n.\npi\n*\nsod_halo_radius\n**\n3\n)\nprofile\n=\nhalo_density\n/\n(\n3\n*\nA\n*\nr_\n)\n/\n(\n1\n/\nsod_halo_cdelta\n+\nr_\n)\n**\n2\nreturn\n{\n\"nfw_radius\"\n:\nr_\n*\nsod_halo_radius\n,\n\"nfw_profile\"\n:\nprofile\n}\nds\n=\nds\n.\nevaluate\n(\nnfw\n,\ninsert\n=\nTrue\n)\nYour dataset will now include a column named “nfw_radius” with the radius values, and “nfw_profile” with the associated profile. Note that opencosmo, like astropy, supports multi-dimensional columns.\n### Additional Arguments\nAdditional Arguments\n\nIf your function requires more arguments than just column names, you can pass them directly as keyword arguments to\nevaluate\n. These arguments will be passed along to the underlying without modification. For example, suppose we wanted to perturb the true mass of a halo by some random amount to simulate the uncertainty associated with inferring masses through observation:\nimport\nopencosmo\nas\noc\nfrom\nscipy.stats\nimport\nnorm\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\ndm_pdf\n=\nlambda\n:\nnorm\n.\nrvs\n(\n1\n,\n0.25\n,\n1\n)\ndef\nperturbed_mass\n(\nsod_halo_cdelta\n,\nsod_halo_mass\n,\ndm_simulator\n):\n# suppose uncertainty increases with mass and decreases with concentration\ndm\n=\ndm_simulator\n()\nmass_perturbation\n=\nsod_halo_mass\n*\ndm\n/\nsod_halo_cdelta\nreturn\nmass_perturbation\nresult\n=\nds\n.\nevaluate\n(\nperturbed_mass\n,\ndm_simulator\n=\ndm_pdf\n)\nNote that we also do not set\ninsert\n=\nTrue\nin the call to\nevaluate\n. Instead, the method will simply return the resutlts to us. Additionally, our function returns a single value rather than a dictionary. As a result, the name of the column will be the same as the name of the function.\n### Vectorizing or Batching Computations\nVectorizing or Batching Computations\n\nAlthough the above example will work it involves performing the computation one row at a time, which is very inefficient. We can speed this up in two ways. First, we can generate all the random values ahead of time, rather than making a call to the random number generator at each iteration:\nimport\nopencosmo\nas\noc\nfrom\nscipy.stats\nimport\nnorm\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\ndm_vals\n=\nlambda\n:\nnorm\n.\nrvs\n(\n1\n,\n0.25\n,\nlen\n(\nds\n))\ndef\nperturbed_mass\n(\nsod_halo_cdelta\n,\nsod_halo_mass\n,\ndm\n):\nmass_perturbation\n=\nsod_halo_mass\n*\ndm\n/\nsod_halo_cdelta\nreturn\nmass_perturbation\nresult\n=\nds\n.\nevaluate\n(\nperturbed_mass\n,\ndm\n=\ndm_vals\n)\nThe toolkit will automatically detect that dm_vals is the same length as the dataset, and break it up by row accordingly.\nHowever this is still not very efficient. This entire computation can be vectorized by simply doing the computation with the entire columns. Because Astropy columns are just numpy arrays, standard numpy syntax will work without issue. You can request vectorization by simply setting\nvectorize\n=\nTrue\nin the call to\nevaluate\n:\nimport\nopencosmo\nas\noc\nfrom\nscipy.stats\nimport\nnorm\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\ndm_vals\n=\nlambda\n:\nnorm\n.\nrvs\n(\n1\n,\n0.25\n,\nlen\n(\nds\n))\ndef\nperturbed_mass\n(\nsod_halo_cdelta\n,\nsod_halo_mass\n,\ndm\n):\nmass_perturbation\n=\nsod_halo_mass\n*\ndm\n/\nsod_halo_cdelta\nreturn\nmass_perturbation\nresult\n=\nds\n.\nevaluate\n(\nperturbed_mass\n,\ndm\n=\ndm_vals\n,\nvectorize\n=\nTrue\n)\nIn some cases you may want to operate on may rows at once, but it would not be feasible to operate on the entire dataset at once. In these cases, you can use the\nbatch_size\nargument to specify the size of batches to be passed to your function. Results will still be collected into a single column as usual.\nimport\nopencosmo\nas\noc\nfrom\nscipy.stats\nimport\nnorm\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\ndm_vals\n=\nlambda\n:\nnorm\n.\nrvs\n(\n1\n,\n0.25\n,\nlen\n(\nds\n))\ndef\nperturbed_mass\n(\nsod_halo_cdelta\n,\nsod_halo_mass\n,\ndm\n):\nmass_perturbation\n=\nsod_halo_mass\n*\ndm\n/\nsod_halo_cdelta\nreturn\nmass_perturbation\nresult\n=\nds\n.\nevaluate\n(\nperturbed_mass\n,\ndm\n=\ndm_vals\n,\nbatch_size\n=\n100_000\n)\nSetting\nbatch_size\ncauses\nvectorize\nto be ignored. Data will be passed into your function in batches which are\nat most\nthis size. In certain cases they may also be smaller, but they will never be larger.\n## Evaluating on Structure Collections\nEvaluating on Structure Collections\n\nWhen working with a\nStructureCollection\n, you can use\nevaluate\nto perform complex computations involving multiple datasets.\ncollection\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\n.\ntake\n(\n200\n)\ndef\noffset\n(\nhalo_properties\n,\ndm_particles\n):\ndx_fof\n=\n(\nnp\n.\nmean\n(\ndm_particles\n[\n\"x\"\n])\n-\nhalo_properties\n[\n\"fof_halo_center_x\"\n]\n)\ndy_fof\n=\n(\nnp\n.\nmean\n(\ndm_particles\n[\n\"x\"\n])\n-\nhalo_properties\n[\n\"fof_halo_center_x\"\n]\n)\ndz_fof\n=\n(\nnp\n.\nmean\n(\ndm_particles\n[\n\"x\"\n])\n-\nhalo_properties\n[\n\"fof_halo_center_x\"\n]\n)\ndx_sod\n=\nnp\n.\nmean\n(\ndm_particles\n[\n\"x\"\n])\n-\nhalo_properties\n[\n\"sod_halo_com_x\"\n]\ndy_sod\n=\nnp\n.\nmean\n(\ndm_particles\n[\n\"x\"\n])\n-\nhalo_properties\n[\n\"sod_halo_com_y\"\n]\ndz_sod\n=\nnp\n.\nmean\n(\ndm_particles\n[\n\"x\"\n])\n-\nhalo_properties\n[\n\"sod_halo_com_z\"\n]\ndr_fof\n=\nnp\n.\nlinalg\n.\nnorm\n([\ndx_fof\n,\ndy_fof\n,\ndz_fof\n])\ndr_sod\n=\nnp\n.\nlinalg\n.\nnorm\n([\ndx_sod\n,\ndy_sod\n,\ndz_sod\n])\nreturn\n{\n\"dr_fof\"\n:\ndr_fof\n,\n\"dr_sod\"\n:\ndr_sod\n}\ncollection\n=\ncollection\n.\nevaluate\n(\noffset\n,\ninsert\n=\nTrue\n,\nformat\n=\n\"numpy\"\n,\ndm_particles\n=\n[\n\"x\"\n,\n\"y\"\n,\n\"z\"\n]\nhalo_properties\n=\n[\n\"fof_halo_center_x\"\n,\n\"fof_halo_center_y\"\n,\n\"fof_halo_center_z\"\n,\n\"sod_halo_com_x\"\n,\n\"sod_halo_com_y\"\n,\n\"sod_halo_com_z\"\n]\n)\nThere are two clear differences between this example and the one with a single dataset. First, you must explicitly declare which columns you need from each of the datasets in the collection. The columns are passed as keyword arguments to\nevaluate\n. Secondly, the function that does the computation takes the names of the datasets themselves as input parameters, rather than the names of columns. This ensures you can, for example, work with multiple species of particles in a single function even if they have some of the same column names.\nYou will also notice that we set\nformat\n=\n\"numpy\"\nin the call to\nevaluate\n. With this option set, the data will be provided to our function as a dictionary of scalars (for halo_properties) and a dictionary of numpy arrays (for dm_particles). If we had chosen instead\nformat\n=\n\"astropy\"\n(the default), the data would have been provided as a dictionary of astropy quantities and a dictionary of quantity arrays, respectively.\nIf you have a nested\nStructureCollection\n, it will be passed to your function directly. You can still select specific columns from these datasets though:\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n,\n\"galaxyproperties.hdf5\"\n,\n\"galaxyparticles.hdf5\"\n)\ndef\nmy_cool_function\n(\nhalo_properties\n,\ndm_particles\n,\ngalaxies\n):\n# the \"galaxies\" argument will be a StructureCollection\n# You can use its data directly, iterate through its galaxies\n# or further filter.\n# do fun stuff here.\ncollection\n=\ncollection\n.\nevaluate\n(\noffset\n,\ninsert\n=\nTrue\n,\nformat\n=\n\"numpy\"\n,\ndm_particles\n=\n[\n\"x\"\n,\n\"y\"\n,\n\"z\"\n],\nhalo_properties\n=\n[\n\"fof_halo_center_x\"\n,\n\"fof_halo_center_y\"\n,\n\"fof_halo_center_z\"\n,\n\"sod_halo_com_x\"\n,\n\"sod_halo_com_y\"\n,\n\"sod_halo_com_z\"\n]\ngalaxies\n=\n{\n\"galaxy_properties\"\n:\n[\n\"gal_mass_bar\"\n,\n\"gal_mass_star\"\n],\n\"star_particles\"\n:\n[\n\"x\"\n,\n\"y\"\n,\n\"z\"\n]\n}\n)\nCurrently, there are three use cases that are supported if you want the data to be inserted as a new column:\nYou evaluate using data from multiple datasets in the collection, and insert into the halo_properties or galaxy_properties dataset.\nYou evaluate using the data from a single dataset (chunked by structure), and insert the results into that dataset\nYou evaaluate using all the data in a single dataset, and insert into that dataset (using\nevaluate_on_dataset\n)\n### Evaluating on a Single Dataset in a Structure Collection\nEvaluating on a Single Dataset in a Structure Collection\n\nYou can evaluate on individual datasets in the structure collection either in a structure-by-structure fashion, or in one go. For example, suppose you wanted to run a novel clustering algorithm on the star particles in a halo collection, but you only care about clustering within the individual halos:\ncollection\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\ndef\ncluster_id\n(\nx\n,\ny\n,\nz\n):\nnum_clusters\n=\n10\nnp\n.\nrandom\n.\nrandint\n(\n0\n,\n10\n,\nlen\n(\nx\n))\ncollection\n.\nevaluate\n(\ncluster_id\n,\ndataset\n=\n\"star_particles\"\n,\ninsert\n=\nTrue\n)\nThe coordinates of the star particles will be provided to the function on a structure-by-structure basis. Each star particle in your star_particles dataset will now have a cluster id that is local to its halo. Obviously this is not a good clustering algorithm, but you can replace it with whatever you want (k-means, DBSCA, FoF). Note that the function follows the same rules as if we we’re calling\nevaluate\non the dataset itself. The required columns are determined from the function arguments.\nNote that for now, we only support this type of workflow if the function only depends on data from the dataset it will be insrted into. We expect this to change in the future.\nYou may also want to do an evaluation on an individual dataset that without worrying about chunking by structure. Suppose you have a structure collection, and as part of a longer, more-complex analysis you want compute the perturbed mass we saw in the\nprevious example\n. You can accomplish this by instead using\nevaluate_on_dataset\nimport\nopencosmo\nas\noc\nfrom\nscipy.stats\nimport\nnorm\ncollection\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\ndm_vals\n=\nlambda\n:\nnorm\n.\nrvs\n(\n1\n,\n0.25\n,\nlen\n(\nds\n))\ndef\nperturbed_mass\n(\nsod_halo_cdelta\n,\nsod_halo_mass\n,\ndm\n):\nmass_perturbation\n=\nsod_halo_mass\n*\ndm\n/\nsod_halo_cdelta\nreturn\nmass_perturbation\nresult\n=\ncollection\n.\nevaluate_on_dataset\n(\nperturbed_mass\n,\ndataset\n=\n\"halo_properties\"\n,\ninsert\n=\nTrue\n,\nvectorize\n=\nTrue\n,\ndm\n=\ndm_vals\n)\nNotice that because we are operating on a single dataset, the format of the function looks exactly as if we were evaluating on a single dataset. If you only care about the result, you could also evaluate on the dataset directly:\nresult\n=\ncollection\n[\n\"halo_properties\"\n]\n.\nevaluate_on_dataset\n(\nperturbed_mass\n,\nvectorize\n=\nTrue\n,\ndm\n=\ndm_vals\n)\nHowever if you try to do the above while inserting the values\nnew_halo_properties\n=\ncollection\n[\n\"halo_properties\"\n]\n.\nevaluate_on_dataset\n(\nperturbed_mass\n,\ninsert\n=\nTrue\n,\nvectorize\n=\nTrue\n,\ndm\n=\ndm_vals\n)\nthe return value will be a new dataset with the new\nperturbed_mass\ncolumn, but it will be not be part of the collection. By using\nevaluate\non the structure collection directly, the result will be a new structure collection with the new column.\n## Evaluating on Lightcones and Simulation Collections\nEvaluating on Lightcones and Simulation Collections\n\nUsing\nLightcone.evaluate\nis identical to using\nDataset.evaluate\n. Although OpenCosmo represents lighcones internally as a collection of\nDatasets\n, the details of broadcasting over these datasets are handled for you.\nUsing\nSimulationCollection.evaluate\nshould also feel very familiar. However if you plan to provide arguments on a per-dataset basis (i.e. an extra numpy array that is used in the calculation) these arguments must be provided as a dictionary with the same keys as the names of the dataset in the\nSimulationCollection\n. For example:\ncollection\n=\noc\n.\nopen\n(\n\"haloproperties_multi.hdf5\"\n)\ndef\nfof_px\n(\nfof_halo_mass\n,\nfof_halo_com_vx\n,\nrandom_value\n,\nother_value\n):\nreturn\nfof_halo_mass\n*\nfof_halo_com_vx\n*\nrandom_value\n/\nother_value\nrandom_data\n=\n{\nkey\n:\nnp\n.\nrandom\n.\nrandint\n(\n0\n,\n10\n,\nlen\n(\nds\n))\nfor\nkey\n,\nds\nin\ncollection\n.\nitems\n()\n}\nrandom_val\n=\n{\nkey\n:\nnp\n.\nrandom\n.\nrandint\n(\n1\n,\n100\n,\n1\n)\nfor\nkey\nin\ncollection\n.\nkeys\n()\n}\noutput\n=\ncollection\n.\nevaluate\n(\nfof_px\n,\nvectorize\n=\nTrue\n,\ninsert\n=\nFalse\n,\nformat\n=\n\"numpy\"\n,\nrandom_value\n=\nrandom_data\n,\nother_value\n=\nrandom_val\n,\n)\nA\nSimulationCollection\ncan contain\nDatasets\nor other collections. Besides arguments that are provided on a per-argument basis as discussed above, everything passed into this function will be passed directly into the\nevaluate\nmethod of the underlying object.\n## Evaluating Without Return Values\nEvaluating Without Return Values\n\nIt is also possible to pass a function that returns None. Such a function could be used, for example, to produce a series of plots that are saved to disk:\nimport\nmatplotlib.pyplot\nas\nplt\nfrom\npathlib\nimport\nPath\nhalos\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"sodproperties.hdf5\"\n)\nhalos\n=\nhalos\n.\nfilter\n(\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e14\n)\n.\ntake\n(\n10\n)\noutput_path\n=\nPath\n(\n\"my_plots/profiles/\"\n)\ndef\nplot_profiles\n(\nhalo_properties\n,\nhalo_profiles\n,\noutput_path\n)\nplot_output_path\n=\noutput_path\n/\nf\n\"\n{\nhalo_properties\n[\n\"fof_halo_tag\"\n]\n}\n.png\"\ndm_count\n=\nhalo_profiles\n[\n\"sod_halo_bin_count\"\n]\n*\nhalo_profiles\n[\n\"sod_halo_bin_cdm_fraction\"\n]\nplt\n.\nfigure\n()\nplt\n.\nscatter\n(\nhalo_profiles\n[\n\"sod_halo_bin_radius\"\n],\ndm_count\n)\nplt\n.\nsavefig\n(\nplot_output_path\n)\nhalos\n.\nevaluate\n(\nplot_profiles\n,\nhalo_properties\n=\n[\n\"fof_halo_tag\"\n],\nhalo_profiles\n=\n[\n\"sod_halo_bin_count, sod_halo_bin_cdm_fraction\"\n,\n\"sod_halo_bin_radius\"\n],\noutput_path\n=\noutput_path\n)\n## Stateful Computations\nStateful Computations\n\nSome computations may be\nstateful\n, meaning that the result of a computation on a given row in your dataset may affect the way the computation is performed on a later row. An example could include a machine learning model that is learning a distribution of halo profiles from a large number of examples.\nIn cases like these, the stateful part of the computation should be passed into\nevaluate\nas a keyword argument, and mutated inside the provided function. OpenCosmo simply passes the keyword argument along to the eavluated function, so any mutated state will persist between function calls.\n## Performance Note\nPerformance Note\n\nEvaluations on individual datasets (and lightcones) are lazy, whether processed through\nevaluate\nor a StructureCollection’s\nevaluate_on_dataset\nmethod. Evaluations on StructureCollections that involve multiple datasets are performed eagerly."}
{"url": "https://opencosmo.readthedocs.io/en/latest/mpi.html", "title": "Working with MPI — OpenCosmo Documentation", "content": "# Working with MPI\nWorking with MPI\n\nOpenCosmo can read and write data in an MPI environment. In general the API works exactly the same within an MPI context as it does otherwise, but there are some things to be aware of in the current version of the library (see below). More flexibility in working in an MPI context is planned for future work\n## I/O with Parallel HDF5 and Select Operations\nI/O with Parallel HDF5 and Select Operations\n\nReading or writring data in parallel via MPI requires no additional work on your part. OpenCosmo will automatically coordinate across ranks to distribute data, and coordinate to write data at the end of your pipeline. However when working with large datasets and/or many MPI ranks, we strongly recommend installing a copy of HDF5 with parallel support. Parallel hdf5 allows multiple ranks to write data simultaneously, which will significantly decrease the amount of time required to write the data. See\nInstallation\nfor details on how to install a parallel version of hdf5 on your system.\n## “Take” Operations\n“Take” Operations\n\nWhen a dataset is opened in an MPI context, the data is chunked across all ranks.\nopencosmo.Dataset.take()\noperations will always operate on the data that is local to the given rank. For example, taking 100 rows at random on all ranks will actually take 100*N_ranks rows, distributed evenly across the ranks. Taking 100 rows with\nat\n=\n\"start\"\nwill take the first 100 rows on each rank.\n## Spatial Queries\nSpatial Queries\n\nIn OpenCosmo, raw data is ordered according to its location in the spatial index. When a dataset is loaded with MPI, each rank recieves an equal share of the regions in the spatial index. As a result, most spatial queries are likely to return no data for most ranks. Ranks that fall completely outside the query region will return a zero-length dataset.\nopencosmo.write()\nwill handle the zero-length datasets automatically.\nYou can retrieve the region the local dataset is contained with in by calling\nopencosmo.Dataset.region()\n. One possible workflow is to perform different spatial queries for each rank depending on the region that is local to that rank.\nCurrently OpenCosmo does not support sharing data across ranks, such as when a given spatial query crosses a rank boundary. This will be improved in the future."}
{"url": "https://opencosmo.readthedocs.io/en/latest/performance.html", "title": "Performance Tips — OpenCosmo Documentation", "content": "# Performance Tips\nPerformance Tips\n\nopencosmo\nis designed to handle datasets on any scale, from smaller catalogs you work with on your local machine to much larger catalogs housed on supercomputers. However there are some best-practices to keep in mind if you want the best possible performances, especially with very large datasets.\n## Don’t Request Data Until You Actually Need It\nDon’t Request Data Until You Actually Need It\n\nFor the most part,\nopencosmo\nqueries are\nlazily evaluated\n. This means that\nopencosmo\nonly reads data when it absolutely needs to, such as when a user requests it with\nDataset.data\nor writes a dataset to a new file with\nwrite\n.\nThe main exception to this rule is transformations that filter the dataset based on the value of the columns. If you\nfilter\na dataset based on the value of a column, that column will be loaded into memory and your filter will be evaluated immediately. However this only loads the columns necessary to evaluate the filter, so it is usually quite fast.\nAs a result, it is usually a good idea to whittle down the dataset as much as possible before requesting data. More specific suggestions can be found below.\n## Select the Columns You Actually Need\nSelect the Columns You Actually Need\n\nLoading data from disk into memory will almost always be the most time-consuming part of any query.\nopencosmo\nallows you to\nselect\nthe columns you need at any time. As you may be able to guess by now, a column that is not selected will not be loaded into memory at all. If you have a dataset with many columns and you only needa few, selecting those columns and excluding the others can greatly speed up the time it takes to perform the query.\n## Preview Data withtake\nPreview Data with\ntake\n\nIf you want to test something out, consider taking a small number of rows using the\ntake\nmethod. Doing this avoids loading data into memory that you do not plan to use for your test. It is easy to create a small test dataset without losing access to the full dataset:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\n.\nselect\n([\n\"fof_halo_mass\"\n,\n\"sod_halo_mass\"\n])\ntest_ds\n=\nds\n.\ntake\n(\n10\n,\nat\n=\n\"start\"\n)\nprint\n(\ntest_ds\n.\ndata\n)\n# or do other tests\n# do additional work with original dataset\n## Create Multiple Child Datasets from a Single Ancestor\nCreate Multiple Child Datasets from a Single Ancestor\n\nopencosmo\nallows you to create many children from a single ancestor dataset. If there are a set of common operations that must be applied to all children, it is generally better to perform them on the ancestor datasets before creating the children. This is especially true for filters and spatial queries. For example, suppose you were interested in looking at the concentration-mass relation for halos in a particular region of your dataset. You want to explore the relationship seperately in two different mass bins. One approach could be to do the following:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nregion\n=\noc\n.\nbox\n((\n300\n,\n300\n,\n300\n),\n(\n600\n,\n600\n,\n6\n))\nds_low_mass\n=\nds\n.\nfilter\n(\noc\n.\ncol\n(\n\"sod_halo_mass\"\n)\n>\n1e12\n,\noc\n.\ncol\n(\n\"sod_halo_mass\"\n)\n<\n1e13\n)\n.\nbound\n(\nregion\n)\n.\nselect\n([\n\"sod_halo_mass\"\n,\n\"sod_halo_cdelta\"\n])\nds_high_mass\n=\nds\n.\nfilter\n(\noc\n.\ncol\n(\n\"sod_halo_mass\"\n)\n>\n1e13\n,\noc\n.\ncol\n(\n\"sod_halo_mass\"\n)\n<\n1e14\n)\n.\nbound\n(\nregion\n)\n.\nselect\n([\n\"sod_halo_mass\"\n,\n\"sod_halo_cdelta\"\n])\nhowever this is not optimal, because the spatial query now has to be performed twice. Instead, you can do the following:\nimport\nopencosmo\nas\noc\nregion\n=\noc\n.\nbox\n((\n300\n,\n300\n,\n300\n),\n(\n600\n,\n600\n,\n6\n))\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nds\n=\nds\n.\nbound\n(\nregion\n)\n.\nselect\n([\n\"sod_halo_mass\"\n,\n\"sod_halo_cdelta\"\n])\nds_low_mass\n=\nds\n.\nfilter\n(\noc\n.\ncol\n(\n\"sod_halo_mass\"\n)\n>\n1e12\n,\noc\n.\ncol\n(\n\"sod_halo_mass\"\n)\n<\n1e13\n)\nds_high_mass\n=\nds\n.\nfilter\n(\noc\n.\ncol\n(\n\"sod_halo_mass\"\n)\n>\n1e13\n,\noc\n.\ncol\n(\n\"sod_halo_mass\"\n)\n<\n1e14\n)\nOf course, you may also want to name the parent dataset something different so that you can keep access to the full un-filtered catalog."}
{"url": "https://opencosmo.readthedocs.io/en/latest/analysis.html", "title": "Analyzing Particle Data with yt — OpenCosmo Documentation", "content": "# Analyzing Particle Data with yt\nAnalyzing Particle Data with yt\n\nNote: The dependencies for the yt analysis tools are not installed by default. You can install them with\nopencosmo\ninstall\nhaloviz\n.\nyt\nis an open-source Python package for analyzing and visualizing volumetric simulation data. Although yt was originally designed with AMR (Adaptive Mesh Refinement) codes in mind, support for SPH (Smoothed Particle Hydrodynamics) data is continually improving. As of yt version 4.4, most core functionality works reliably with SPH data, though some features may still require workarounds. In many cases, this involves depositing particle data onto a mesh using a\nYTArbitraryGrid\nobject before passing it to specific yt functions.\nIn OpenCosmo, you can load particle datasets into yt using\nopencosmo.analysis.create_yt_dataset()\n.\nThis is effectly doing the same as\nyt.load\n, however, we have opted to use the OpenCosmo toolkit\nto handle the initial data selection.\nHere is an example for how to use\ncreate_yt_dataset\nto load a selection of data into yt and make a simple projection\nfrom\nopencosmo.analysis\nimport\ncreate_yt_dataset\n,\nParticleProjectionPlot\n# select a random halo\nwith\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\n.\ntake\n(\n1\n,\nat\n=\n\"random\"\n)\nas\ndata\n:\n# get yt data container\nyt_ds\n=\ncreate_yt_dataset\n(\nnext\n(\ndata\n.\nhalos\n()))\n# list all fields\nprint\n(\nyt_ds\n.\nderived_field_list\n)\n# project DM particle mass\nParticleProjectionPlot\n(\nyt_ds\n,\n\"z\"\n,\n(\n\"dm\"\n,\n\"particle_mass\"\n))\n.\nsave\n()\nFor convenience, OpenCosmo includes wrappers for several commonly used yt plotting functions, including:\nopencosmo.analysis.ParticleProjectionPlot()\n(wraps\nyt.ParticleProjectionPlot\n)\nopencosmo.analysis.ProjectionPlot()\n(wraps\nyt.ProjectionPlot\n)\nopencosmo.analysis.SlicePlot()\n(wraps\nyt.SlicePlot\n)\nopencosmo.analysis.ProfilePlot()\n(wraps\nyt.ProfilePlot\n)\nopencosmo.analysis.PhasePlot()\n(wraps\nyt.PhasePlot\n)\nThese wrappers follow the same naming conventions as the original yt functions and have been verified to work out-of-the-box with HACC SPH data.\nFor an overview of yt’s broader functionality, refer to the official\nyt documentation\n.\nFor introductory tutorials, see:\nMaking Simple Plots\nA Few Complex Plots\n# Simulating X-ray Emission with pyXSIM\nSimulating X-ray Emission with pyXSIM\n\nTo include synthetic X-ray emissivity and luminosity fields from gas particles in your yt dataset, you can enable the\ncompute_xray_fields\nflag when calling\nopencosmo.analysis.create_yt_dataset()\n. This integrates with\npyXSIM\n, a toolkit for generating synthetic X-ray observations from simulation data.\nWhen\ncompute_xray_fields=True\n, the function internally creates a\npyxsim.CIESourceModel\nusing the particle data and attaches the following derived fields to the\nyt\ndataset:\nX-ray emissivity per particle\nX-ray luminosity in a user-specified energy band\nAny additional fields required for photon sampling (e.g., emission measure)\nYou can also pass model-specific configurations via the\nsource_model_kwargs\nargument, which is forwarded directly to the\npyxsim.CIESourceModel\nconstructor. Common options include:\nemin\n(float): Minimum photon energy in keV (default: 0.1)\nemax\n(float): Maximum photon energy in keV (default: 10.0)\nnbins\n(int): Number of bins across the energy band (default: 1000)\nmodel\n(str): which emission model to use (default: “apec”)\nFor the full list of options, see\nCIESourceModel\n.\nIf\nreturn_source_model=True\n, the function will return a 2-tuple\n(ds,\nsource_model)\n, where\nsource_model\nis the\nCIESourceModel\ninstance. This allows further customization or photon generation using pyXSIM directly.\nWe will now edit the code-block from before to compute X-ray luminosities:\nfrom\nopencosmo.analysis\nimport\ncreate_yt_dataset\n,\nParticleProjectionPlot\n# set source model parameters\nsource_model_kwargs\n=\n{\n\"emin\"\n:\n0.1\n,\n# keV\n\"emax\"\n:\n10.0\n# keV\n}\n# select a random halo\nwith\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\n.\ntake\n(\n1\n,\nat\n=\n\"random\"\n)\nas\ndata\n:\n# get yt data container\nds_yt\n,\nsource_model\n=\ncreate_yt_dataset\n(\nnext\n(\ndata\n.\nhalos\n()),\ncompute_xray_fields\n=\nTrue\n,\nreturn_source_model\n=\nTrue\n)\n# list all fields\nprint\n(\nds_yt\n.\nderived_field_list\n)\n# project X-ray luminosity in the specified band\nParticleProjectionPlot\n(\nds_yt\n,\n\"z\"\n,\n(\n\"gas\"\n,\n\"xray_luminosity_0.1_10.0_keV\"\n))\n.\nsave\n()\n# Visualizing Halos\nVisualizing Halos\n\nIn addition to individual yt plots, OpenCosmo provides utilities for visualizing multiple halo projections at once.\nThe two primary functions for this purpose are:\nopencosmo.analysis.visualize_halo()\n— a simple 2x2 panel plot for one halo\nopencosmo.analysis.halo_projection_array()\n— a customizable grid of halos and fields\nThese use yt under the hood, and are useful for visually inspecting halos with minimal input required.\n## Quick Projections\nQuick Projections\n\nThe\nvisualize_halo()\nfunction takes in a single halo ID and creates a multi-panel image showing particle projections of dark matter, stars, gas, and/or gas temperature for a the halo. If\n\"dm\"/\"gravity\"\n,\n\"star\"\n, and\n\"gas\"\nparticles are all present, this will output a 2x2-panel figure. Otherwise, this will create a 1xN-panel figure showing whichever particles/fields from the list are present.\nThis function essentially uses\nhalo_projection_array()\nwith pre-filled settings for fields, colormaps, and labels.\nSettings are tuned to look good for halos with\n\\(M_\\mathrm{200c} > 10^{14}\\ M_\\odot\\)\n.\nfrom\nopencosmo.analysis\nimport\nvisualize_halo\nimport\nopencosmo\nas\noc\nimport\nmatplotlib.pyplot\nas\nplt\n# load one halo at random\nwith\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\n.\ntake\n(\n1\n,\nat\n=\n\"random\"\n)\nas\ndata\n:\nhalo\n=\nnext\n(\ndata\n.\nhalos\n())\nhalo_id\n=\nhalo\n[\n\"halo_properties\"\n][\n\"unique_tag\"\n]\nfig\n=\nvisualize_halo\n(\nhalo_id\n,\ndata\n)\n# save the image\nfig\n.\nsavefig\n(\n\"halo_2x2_example.png\"\n)\n## A More Customizable Option\nA More Customizable Option\n\nThe\nhalo_projection_array()\nfunction allows fine-grained control over what gets visualized, including:\nPlotting different halos and/or fields per panel\nWeighting projections by other quantities\nUsing different colormaps and colorbar limits\nCustomizing panel labels and layout\nFor the full list of customization options, see\nopencosmo.analysis.halo_projection_array()\n### Multiple Halos, Single Field\nMultiple Halos, Single Field\n\nAt minimum,\nhalo_projection_array()\ntakes in a 2D array of halo IDs and the\nStructuredCollection\ndataset containing the relevant halos.\nThe outputted figure is an array of images, with the shape matching that of the halo ID array. For example:\nfrom\nopencosmo.analysis\nimport\nhalo_projection_array\nimport\nopencosmo\nas\noc\nimport\nmatplotlib.pyplot\nas\nplt\nimport\nnumpy\nas\nnp\n# load 16 halos at random\nwith\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\n.\ntake\n(\n16\n,\nat\n=\n\"random\"\n)\nas\ndata\n:\nhalo_ids\n=\n[\nhalo\n[\n'halo_properties'\n][\n'unique_tag'\n]\nfor\nhalo\nin\ndata\n.\nhalos\n()]\n# construct 4x4 array of halo ids and make a 4x4 array of dark matter projections\nfig\n=\nhalo_projection_array\n(\nnp\n.\nreshape\n(\nhalo_ids\n,(\n4\n,\n4\n)),\ndata\n,\nfield\n=\n(\n\"dm\"\n,\n\"particle_mass\"\n),\nwidth\n=\n6.0\n)\n# save the image\nfig\n.\nsavefig\n(\n\"halo_4x4_example.png\"\n)\n### Multiple Halos, Multiple Fields\nMultiple Halos, Multiple Fields\n\nOne can also define a dictionary of plotting parameters to plot different fields and/or halos in each panel:\nfrom\nopencosmo.analysis\nimport\nhalo_projection_array\nimport\nopencosmo\nas\noc\nimport\nmatplotlib.pyplot\nas\nplt\nimport\nnumpy\nas\nnp\nwith\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\n.\ntake\n(\n2\n,\nat\n=\n\"random\"\n)\nas\ndata\n:\nhalo_ids\n=\n[\nhalo\n[\n\"halo_properties\"\n][\n\"unique_tag\"\n]\nfor\nhalo\nin\ndata\n.\nhalos\n()]\n# We are going to make a 2x3 panel figure, where each row is a different halo, and\n# each column is a different projected quantity\nhalo_ids\n=\n(\n[\nhalo_ids\n[\n0\n],\nhalo_ids\n[\n0\n],\nhalo_ids\n[\n0\n]],\n[\nhalo_ids\n[\n1\n],\nhalo_ids\n[\n1\n],\nhalo_ids\n[\n1\n]]\n)\n# construct dictionary of plotting parameters.\n# Each item should be a 2x3 array\nparams\n=\n{\n\"fields\"\n:\n(\n[(\n\"dm\"\n,\n\"particle_mass\"\n),\n(\n\"gas\"\n,\n\"particle_mass\"\n),\n(\n\"star\"\n,\n\"particle_mass\"\n)],\n[(\n\"dm\"\n,\n\"particle_mass\"\n),\n(\n\"gas\"\n,\n\"particle_mass\"\n),\n(\n\"star\"\n,\n\"particle_mass\"\n)]\n),\n\"labels\"\n:\n(\n[\n\"Dark Matter\"\n,\n\"Gas\"\n,\n\"Stars\"\n],\n[\nNone\n,\nNone\n,\nNone\n]\n),\n\"cmaps\"\n:\n(\n[\n\"gray\"\n,\n\"cividis\"\n,\n\"bone\"\n],\n[\n\"gray\"\n,\n\"cividis\"\n,\n\"bone\"\n]\n),\n}\n# Make 2x3 array of halo projections with length scales displayed on the leftmost column\nfig\n=\nhalo_projection_array\n(\nhalo_ids\n,\ndata\n,\nparams\n=\nparams\n,\nlength_scale\n=\n\"all left\"\n)\n# save the image\nfig\n.\nsavefig\n(\n\"halo_2x3_example.png\"\n)"}
{"url": "https://opencosmo.readthedocs.io/en/latest/io_ref.html", "title": "Reading/Writing — OpenCosmo Documentation", "content": "# Reading/Writing\nReading/Writing\n\nopencosmo.\nopen\n(\n*\nfiles\n,\n**\nopen_kwargs\n)\n\nOpen a dataset or data collection from one or more opencosmo files.\nIf you open a file with this function, you should generally close it\nwhen you’re done\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"path/to/file.hdf5\"\n)\n# do work\nds\n.\nclose\n()\nAlternatively you can use a context manager, which will close the file\nautomatically when you are done with it.\nimport\nopencosmo\nas\noc\nwith\noc\n.\nopen\n(\n\"path/to/file.hdf5\"\n)\nas\nds\n:\n# do work\nWhen you have multiple files that can be combined into a collection,\nyou can use the following.\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\nParameters\n:\n*files\n(\nstr\nor\npathlib.Path\n) – The path(s) to the file(s) to open.\n**open_kwargs\n(\nbool\n) – True/False flags that can be used to only load certain datasets from\nthe files. Check the documentation for the data type you are working\nwith for available flags. Will be ignored if only one file is passed\nand the file only contains a single dataset.\nReturns\n:\ndataset\n– The dataset or collection opened from the file.\nReturn type\n:\noc.Dataset or oc.Collection\nopencosmo.\nwrite\n(\npath\n,\ndataset\n,\noverwrite\n=\nFalse\n,\n**\nschema_kwargs\n)\n\nWrite a dataset or collection to the file at the sepecified path.\nParameters\n:\nfile\n(\nstr\nor\npathlib.Path\n) – The path to the file to write to.\ndataset\n(\noc.Dataset\n) – The dataset to write.\noverwrite\n(\nbool\n,\ndefault = False\n) – If the file already exists, overwrite it\npath\n(\nPath\n)\nRaises\n:\nFileExistsError\n– If the file at the specified path already exists and overwrite is False\nFileNotFoundError\n– If the parent folder of the ouput file does not exist\nReturn type\n:\nNone\nopencosmo.io.\nwrite_parquet\n(\npath\n,\nto_write\n,\noverwrite\n=\nFalse\n,\n**\nkwargs\n)\n\nWrite a dataset or collection to a parquet file at the given path. If you are writing a\nopencosmo.Dataset\n,\nor\nopencosmo.Lightcone\nthe data will be written as a single file at the given path. If you are writing\na\nopencosmo.StructureCollection\nthe data will be written to several files, one for each type of particle.\nParameters\n:\npath\n(\nPathLike\n) – The path to write the data to. If you are writing a\nDataset\nor\nLightcone\nthis should be a single parquet file. If you\nare writing a\nStructureCollection\n, this should be\na folder.\nto_write\n(\nopencosmo.Dataset\n|\nopencosmo.Lightcone\n|\nopencosmo.StructureCollection\n) – The dataset or collection to write\noverwrite\n(\nbool\n,\ndefault = False\n) – If true, any existing data at the given path will be overwritten."}
{"url": "https://opencosmo.readthedocs.io/en/latest/dataset_ref.html", "title": "Dataset — OpenCosmo Documentation", "content": "# Dataset\nDataset\n\nclass\nopencosmo.\nDataset\n(\nheader\n,\nstate\n,\ntree\n=\nNone\n)\n\nParameters\n:\nheader\n(\nOpenCosmoHeader\n)\nstate\n(\nDatasetState\n)\ntree\n(\nOptional\n[\nTree\n]\n)\nproperty\nheader\n:\nOpenCosmoHeader\n\nThe header associated with this dataset.\nOpenCosmo headers generally contain information about the original data this\ndataset was produced from, as well as any analysis that was done along\nthe way.\nReturns\n:\nheader\nReturn type\n:\nopencosmo.header.OpenCosmoHeader\nproperty\ncolumns\n:\nlist\n[\nstr\n]\n\nThe names of the columns in this dataset.\nReturns\n:\ncolumns\nReturn type\n:\nlist[str]\nproperty\nmeta_columns\n:\nlist\n[\nstr\n]\n\nproperty\ndescriptions\n:\ndict\n[\nstr\n,\nstr\n|\nNone\n]\n\nReturn the descriptions (if any) of the columns in this dataset as a dictonary.\nColumns without a description will be included in the dictionary with a value\nof None\nReturns\n:\ndescriptions\n– The column descriptions\nReturn type\n:\ndict[str, str | None]\nproperty\ncosmology\n:\nCosmology\n\nThe cosmology of the simulation this dataset is drawn from as\nan astropy.cosmology.Cosmology object.\nReturns\n:\ncosmology\nReturn type\n:\nastropy.cosmology.Cosmology\nproperty\ndtype\n:\nstr\n\nThe data type of this dataset.\nReturns\n:\ndtype\nReturn type\n:\nstr\nproperty\nredshift\n:\nfloat\n|\ntuple\n[\nfloat\n,\nfloat\n]\n|\nNone\n\nThe redshift slice or range this dataset was drawn from\nReturns\n:\nredshift\nReturn type\n:\nfloat\nproperty\nregion\n:\nRegion\n\nThe region this dataset is contained in. If no spatial\nqueries have been performed, this will be the entire\nsimulation box for snapshots or the full sky for lightcones\nReturns\n:\nregion\nReturn type\n:\nopencosmo.spatial.Region\nproperty\nsimulation\n:\nHaccSimulationParameters\n\nThe parameters of the simulation this dataset is drawn\nfrom.\nReturns\n:\nparameters\nReturn type\n:\nopencosmo.parameters.hacc.HaccSimulationParameters\nproperty\ndata\n:\nQTable\n|\nQuantity\n\nReturn the data in the dataset in astropy format. The value of this\nattribute is equivalent to the return value of\nDataset.get_data(\"astropy\")\n.\nReturns\n:\ndata\n– The data in the dataset.\nReturn type\n:\nastropy.table.Table or astropy.table.Column\nget_metadata\n(\ncolumns\n=\n[]\n)\n\nParameters\n:\ncolumns\n(\nstr\n|\nlist\n[\nstr\n]\n)\nget_data\n(\noutput\n=\n'astropy'\n,\nunpack\n=\nTrue\n,\nmetadata_columns\n=\n[]\n)\n\nGet the data in this dataset as an astropy table/column or as\nnumpy array(s). Note that a dataset does not load data from disk into\nmemory until this function is called. As a result, you should not call\nthis function until you have performed any transformations you plan to\non the data.\nThe method supports output into several different formats, including\n“astropy”, “numpy”, “pandas”, “polars”, and “pyarrow”. Although astropy\nand numpy are core dependencies of OpenCosmo, the remaining formats\nrequire you to have the relevant libraries installed in your python\nenvironment. This method will check that it can import the necessary\nlibraries before attempting to read data. Note that outputting as\n“polars” or “arrow” requires copying the data out of its original\nnumpy arrays, which will impact performance.\nIf the dataset only contains a single column, it will not be put in a table\nor dictionary. “astropy”, “numpy” and “arrow” will return a single array\nin this case, while “polars” and “pandas” will return a Series object.\nParameters\n:\noutput\n(\nstr\n,\ndefault=\"astropy\"\n) – The format to output the data in.\nCurrently supported are “astropy”, “numpy”, “pandas”, “polars”, “arrow”\nReturns\n:\ndata\n– The data in this dataset.\nReturn type\n:\nAny\nbound\n(\nregion\n,\nselect_by\n=\nNone\n)\n\nRestrict the dataset to some subregion. The subregion will always be evaluated\nin the same units as the current dataset. For example, if the dataset is\nin the default “comoving” unit convention, positions are always in units of\ncomoving Mpc. However Region objects themselves do not carry units.\nSee\nRegions\nfor details of how to construct regions.\nParameters\n:\nregion\n(\nopencosmo.spatial.Region\n) – The region to query.\nselect_by\n(\nOptional\n[\nstr\n]\n)\nReturns\n:\ndataset\n– The portion of the dataset inside the selected region\nReturn type\n:\nopencosmo.Dataset\nRaises\n:\nValueError\n– If the query region does not overlap with the region this dataset resides\n    in\nAttributeError:\n– If the dataset does not contain a spatial index\nevaluate\n(\nfunc\n,\nvectorize\n=\nFalse\n,\ninsert\n=\nTrue\n,\nformat\n=\n'astropy'\n,\nbatch_size\n=\n-1\n,\n**\nevaluate_kwargs\n)\n\nIterate over the rows in this dataset, apply\nfunc\nto each, and collect\nthe result as new columns in the dataset.\nThis function is the equivalent of\nwith_new_columns\nfor cases where the new column is not a simple algebraic combination of existing columns. Unlike\nwith_new_columns\n, this method will evaluate the results immediately and the resulting\ncolumns will not change under unit transformations. You may also choose to simply return the result\ninstead of adding it as a column.\nThe function should take in arguments with the same name as the columns in this dataset that\nare needed for the computation, and should return a dictionary of output values.\nThe dataset will automatically selected the needed columns to avoid reading unnecessarily reading\ndata from disk\nThe new columns will have the same names as the keys of the output dictionary\nSee\nEvaluating on Datasets\nfor more details.\nIf vectorize is set to True, the full columns will be pased to the dataset. Otherwise,\nrows will be passed to the function one at a time.\nIf the function returns None, this method will also return None as output. For example, the function\ncould simply produce plots and save the to files.\nParameters\n:\nfunc\n(\nCallable\n) – The function to evaluate on the rows in the dataset.\nvectorize\n(\nbool\n,\ndefault = False\n) – Whether to provide the values as full columns (True) or one row at a time (False). Ignored if\nbatch_size\nis set.\ninsert\n(\nbool\n,\ndefault = True\n) – If true, the data will be inserted as a column in this dataset. The new column will have the same name\nas the function. Otherwise the data will be returned directly.\nformat\n(\nstr\n,\ndefault = astropy\n) – Whether to provide data to your function as “astropy” quantities or “numpy” arrays/scalars. Default “astropy”. Note that\nthis method does not support all the formats available in\nget_data\nbatch_size\n(\nint\n,\ndefault = -1\n) – If set, feed data to the function in batches of the specified size. Default is -1, which disables batching. If\nset to another value, the\nvectorize\nflag is ignored.\n**evaluate_kwargs\n(\nany\n,\n) – Any additional arguments that are required for your function to run. These will be passed directly\nto the function as keyword arguments. If a kwarg is an array of values with the same length as the dataset,\nit will be treated as an additional column.\nReturns\n:\nresult\n– The new dataset with the evaluated column(s) or the results as numpy arrays or astropy quantities\nReturn type\n:\nDataset\n| dict[str, np.ndarray | astropy.units.Quantity]\nfilter\n(\n*\nmasks\n)\n\nFilter the dataset based on some criteria. See\nQuerying Based on Column Values\nfor more information.\nParameters\n:\n*masks\n(\nMask\n) – The masks to apply to dataset, constructed with\nopencosmo.col()\nReturns\n:\ndataset\n– The new dataset with the masks applied.\nReturn type\n:\nDataset\nRaises\n:\nValueError\n– If the given  refers to columns that are\n    not in the dataset, or the  would return zero rows.\nrows\n(\ninclude_units\n=\nTrue\n,\nmetadata_columns\n=\n[]\n)\n\nIterate over the rows in the dataset. Rows are returned as a dictionary\nFor performance, it is recommended to first select the columns you need to\nwork with.\nParameters\n:\noutput\n(\nstr\n,\ndefault = \"astropy\"\n) – Whether to return values as “astropy” quantities or “numpy” scalars\ninclude_units\n(\nbool\n)\nYields\n:\nrow\n(\ndict\n) – A dictionary of values for each row in the dataset with units.\nReturn type\n:\nGenerator[Mapping[str, float | units.Quantity | np.ndarray]]\nselect\n(\ncolumns\n)\n\nCreate a new dataset from a subset of columns in this dataset\nParameters\n:\ncolumns\n(\nstr\nor\nlist\n[\nstr\n]\n) – The column or columns to select.\nReturns\n:\ndataset\n– The new dataset with only the selected columns.\nReturn type\n:\nDataset\nRaises\n:\nValueError\n– If any of the given columns are not in the dataset.\ndrop\n(\ncolumns\n)\n\nCreate a new dataset without the provided columns.\nParameters\n:\ncolumns\n(\nstr\nor\nlist\n[\nstr\n]\n) – The columns to drop\nReturns\n:\ndataset\n– The new dataset without the droppedcolumns\nReturn type\n:\nDataset\nRaises\n:\nValueError\n– If any of the provided columns are not in the dataset.\nsort_by\n(\ncolumn\n,\ninvert\n=\nFalse\n)\n\nSort this dataset by the values in a given column. By default sorting is in\nascending order (least to greatest). Pass invert = True to sort in descending\norder (greatest to least).\nThis can be used to, for example, select largest halos in a given\ndataset:\ndataset\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\ndataset\n=\ndataset\n.\nsort_by\n(\n\"fof_halo_mass\"\n,\ninvert\n=\nTrue\n)\n.\ntake\n(\n100\n,\nat\n=\n\"start\"\n)\nParameters\n:\ncolumn\n(\nstr\n) – The column in the halo_properties or galaxy_properties dataset to\norder the collection by.\ninvert\n(\nbool\n,\ndefault = False\n) – If False (the default), ordering will be from least to greatest.\nOtherwise greatest to least.\nReturns\n:\nresult\n– A new Dataset ordered by the given column.\nReturn type\n:\nDataset\ntake\n(\nn\n,\nat\n=\n'random'\n)\n\nCreate a new dataset from some number of rows from this dataset.\nCan take the first n rows, the last n rows, or n random rows\ndepending on the value of ‘at’.\nParameters\n:\nn\n(\nint\n) – The number of rows to take.\nat\n(\nstr\n) – Where to take the rows from. One of “start”, “end”, or “random”.\nThe default is “random”.\nReturns\n:\ndataset\n– The new dataset with only the selected rows.\nReturn type\n:\nDataset\nRaises\n:\nValueError\n– If n is negative or greater than the number of rows in the dataset,\n    or if ‘at’ is invalid.\ntake_range\n(\nstart\n,\nend\n)\n\nCreate a new dataset from a row range in this dataset. We use standard\nindexing conventions, so the rows included will be start -> end - 1.\nParameters\n:\nstart\n(\nint\n) – The beginning of the range\nend\n(\nint\n) – The end of the range\nReturns\n:\ntable\n– The table with only the rows from start to end.\nReturn type\n:\nastropy.table.Table\nRaises\n:\nValueError\n– If start or end are negative or greater than the length of the dataset\n    or if end is greater than start.\ntake_rows\n(\nrows\n)\n\nTake the rows of a dataset specified by the\nrows\nargument.\nrows\nshould be an array of integers.\n## Parameters:\nParameters:\n\nrows: np.ndarray[int]\nreturns\n:\ndataset\n(\nThe dataset with only the specified rows included\n)\nRaises\n——-\nValueError\n– If any of the indices is less than 0 or greater than the length of the\ndataset.\nParameters\n:\nrows\n(\nnp.ndarray\n|\nDataIndex\n)\nwith_new_columns\n(\ndescriptions\n=\n{}\n,\n**\nnew_columns\n)\n\nCreate a new dataset with additional columns. These new columns can be derived\nfrom columns already in the dataset, a numpy array, or an astropy quantity\narray. When a column is derived from other columns, it will behave\nappropriately under unit transformations. Columns provided directly as astropy\nquantities will not change under unit transformations. See\nAdding Custom Columns\nfor examples.\nParameters\n:\ndescriptions\n(\nstr\n|\ndict\n[\nstr\n,\nstr\n]\n,\noptional\n) – A description for the new columns. These descriptions will be accessible through\nDataset.descriptions\n. If a dictionary,\nshould have keys matching the column names.\nnew_columns\n(\n**\n)\nReturns\n:\ndataset\n– This dataset with the columns added\nReturn type\n:\nopencosmo.Dataset\nwith_units\n(\nconvention\n=\nNone\n,\nconversions\n=\n{}\n,\n**\ncolumns\n)\n\nCreate a new dataset from this one with a different unit convention, and/or\nconvert one unit to another across the entire dataset, or convert individual\ncolumns.\nUnit conversions are always performed after a change of convention, and\nchanging conventions clears any existing unit conversions. Individual\ncolumn conversions always take precedence over blanket unit conversions.\nCalling this function without arguments will clear any existing unit conversions.\nFor more, see\nWorking with Units\n.\nimport\nastropy.units\nas\nu\n# this works\ndataset\n=\ndataset\n.\nwith_units\n(\nfof_halo_mass\n=\nu\n.\nkg\n)\n# this clears the previous conversion\ndataset\n=\ndataset\n.\nwith_units\n(\n\"scalefree\"\n)\n# This now fails, because the units of masses\n# are Msun / h, which cannot be converted to kg\ndataset\n=\ndataset\n.\nwith_units\n(\nfof_halo_mass\n=\nu\n.\nkg\n)\n# this will work, the units of halo mass in the \"physical\"\n# convention are Msun (no h).\ndataset\n=\ndataset\n.\nwith_units\n(\n\"physical\"\n,\nfof_halo_mass\n=\nu\n.\nkg\n,\nfof_halo_center_x\n=\nu\n.\nlyr\n)\n# Suppose you want all distances in lightyears, but the x coordinate of your\n# halo center in kilometers, for some reason ¯\\_(ツ)_/¯\nblanket_conversions\n=\n{\nu\n.\nMpc\n:\nu\n.\nlyr\n}\ndataset\n=\ndataset\n.\nwith_units\n(\nconversions\n=\nblanket_conversions\n,\nfof_halo_center_x\n=\nu\n.\nkm\n)\nParameters\n:\nconvention\n(\nstr\n,\noptional\n) – The unit convention to use. One of “physical”, “comoving”,\n“scalefree”, or “unitless”.\nconversions\n(\ndict\n[\nastropy.units.Unit\n,\nastropy.Units.Unit\n]\n) – Conversions that apply to all columns in the dataset with the\nunit given by the key.\n**column_conversions\n(\nastropy.units.Unit\n) – Custom unit conversions for one or more or of the columns\nin this dataset.\ncolumns\n(\nUnit\n)\nReturns\n:\ndataset\n– The new dataset with the requested unit convention and/or conversions.\nReturn type\n:\nDataset"}
{"url": "https://opencosmo.readthedocs.io/en/latest/collection_ref.html", "title": "Collections — OpenCosmo Documentation", "content": "# Collections\nCollections\n\nCollections within a single file can always be loaded with\nopencosmo.open()\n. Collections can be treated like read-only dictionaries. Dataset names can be retrieved with\nkeys()\n, the datasets can be accessed with\nvalues()\nor\nCollection[key]\n, and iteration can be done with\nitems()\n.\nclass\nopencosmo.\nLightcone\n(\ndatasets\n,\nz_range\n=\nNone\n,\nhidden\n=\nNone\n,\nordered_by\n=\nNone\n)\n\nA lightcone contains two or more datasets that are part of a lightcone. Typically\neach dataset will cover a specific redshift range. The Lightcone object\nhides these details, providing an API that is identical to the standard\nDataset API. Additionally, the lightcone contains some convinience functions\nfor standard operations.\nLightcones can be nested. In this case, the top level will split the datasets\nup by step, while the second level will split the datasets up by type. This nested\nscheme (at present) is used for Diffsky catalogs, which may contain both cores\nand synthetic cores that need to be adddressed (and more importantly, written)\nseperately from one another.\nParameters\n:\ndatasets\n(\nMapping\n[\nAny\n,\nDataset\n|\nLightcone\n]\n)\nz_range\n(\nOptional\n[\ntuple\n[\nfloat\n,\nfloat\n]\n]\n)\nhidden\n(\nOptional\n[\nset\n[\nstr\n]\n]\n)\nordered_by\n(\nOptional\n[\ntuple\n[\nstr\n,\nbool\n]\n]\n)\nproperty\nheader\n:\nOpenCosmoHeader\n\nThe header associated with this dataset.\nOpenCosmo headers generally contain information about the original data this\ndataset was produced from, as well as any analysis that was done along\nthe way.\nReturns\n:\nheader\nReturn type\n:\nopencosmo.header.OpenCosmoHeader\nproperty\ncolumns\n:\nlist\n[\nstr\n]\n\nThe names of the columns in this dataset.\nReturns\n:\ncolumns\nReturn type\n:\nlist[str]\nproperty\ndescriptions\n:\ndict\n[\nstr\n,\nstr\n|\nNone\n]\n\nReturn the descriptions (if any) of the columns in this lightcone as a dictonary.\nColumns without a description will be included in the dictionary with a value\nof None\nReturns\n:\ndescriptions\n– The column descriptions\nReturn type\n:\ndict[str, str | None]\nproperty\ncosmology\n:\nCosmology\n\nThe cosmology of the simulation this dataset is drawn from as\nan astropy.cosmology.Cosmology object.\nReturns\n:\ncosmology\nReturn type\n:\nastropy.cosmology.Cosmology\nproperty\ndtype\n:\nstr\n\nThe data type of this dataset.\nReturns\n:\ndtype\nReturn type\n:\nstr\nproperty\nregion\n:\nRegion\n\nThe region this dataset is contained in. If no spatial\nqueries have been performed, this will be the entire\nsimulation box for snapshots or the full sky for lightcones\nReturns\n:\nregion\nReturn type\n:\nopencosmo.spatial.Region\nproperty\nsimulation\n:\nHaccSimulationParameters\n\nThe parameters of the simulation this dataset is drawn\nfrom.\nReturns\n:\nparameters\nReturn type\n:\nopencosmo.parameters.hacc.HaccSimulationParameters\nproperty\nz_range\n\nThe redshift range of this lightcone.\nReturns\n:\nz_range\nReturn type\n:\ntuple[float, float]\nget_data\n(\noutput\n=\n'astropy'\n,\nunpack\n=\nFalse\n)\n\nGet the data in this dataset as an astropy table/column or as\nnumpy array(s). Note that a dataset does not load data from disk into\nmemory until this function is called. As a result, you should not call\nthis function until you have performed any transformations you plan to\non the data.\nYou can get the data in two formats, “astropy” (the default) and “numpy”.\n“astropy” format will return the data as an astropy table with associated\nunits. “numpy” will return the data as a dictionary of numpy arrays. The\nnumpy values will be in the associated unit convention, but no actual\nunits will be attached.\nIf the dataset only contains a single column, it will be returned as an\nastropy.table.Column or a single numpy array.\nParameters\n:\noutput\n(\nstr\n,\ndefault=\"astropy\"\n) – The format to output the data in. Currently supported are “astropy”, “numpy”,\n“pandas”, “polars”, and “arrow”\nunpack\n(\nbool\n)\nReturns\n:\ndata\n– The data in this dataset.\nReturn type\n:\nTable |\nColumn\n| dict[str, ndarray] | ndarray\nproperty\ndata\n\nReturn the data in the dataset in astropy format. The value of this\nattribute is equivalent to the return value of\nDataset.get_data(\"astropy\")\n.\nReturns\n:\ndata\n– The data in the dataset.\nReturn type\n:\nastropy.table.Table or astropy.table.Column\nwith_redshift_range\n(\nz_low\n,\nz_high\n)\n\nRestrict this lightcone to a specific redshift range. Lightcone datasets will\nalways contain a column titled “redshift.” This function is always operates on\nthis column.\nThis function also updates the value in\nLightcone.z_range\n,\nso you should always use it rather than filteringo n the column directly.\nParameters\n:\nz_low\n(\nfloat\n)\nz_high\n(\nfloat\n)\nbound\n(\nregion\n,\nselect_by\n=\nNone\n)\n\nRestrict the dataset to some subregion. The subregion will always be evaluated\nin the same units as the current dataset. For example, if the dataset is\nin the default “comoving” unit convention, positions are always in units of\ncomoving Mpc. However Region objects themselves do not carry units.\nSee\nRegions\nfor details of how to construct regions.\nParameters\n:\nregion\n(\nopencosmo.spatial.Region\n) – The region to query.\nselect_by\n(\nOptional\n[\nstr\n]\n)\nReturns\n:\ndataset\n– The portion of the dataset inside the selected region\nReturn type\n:\nopencosmo.Dataset\nRaises\n:\nValueError\n– If the query region does not overlap with the region this dataset resides\n    in\nAttributeError:\n– If the dataset does not contain a spatial index\ncone_search\n(\ncenter\n,\nradius\n)\n\nPerform a search for objects within some angular distance of some\ngiven point on the sky. This is a convinience function around\nbound\nand is exactly\nequivalent to\nregion\n=\noc\n.\nmake_cone\n(\ncenter\n,\nradius\n)\nds\n=\nds\n.\nbound\n(\nregion\n)\nParameters\n:\ncenter\n(\ntuple\n|\nSkyCoord\n) – The center of the region to search. If a tuple and no units are provided\nassumed to be RA and Dec in degrees.\nradius\n(\nfloat\n|\nastropy.units.Quantity\n) – The angular radius of the region to query. If no units are provided,\nassumed to be degrees.\nReturns\n:\nnew_lightcone\n– The rows in this lightcone that fall within the given region.\nReturn type\n:\nopencosmo.Lightcone\nevaluate\n(\nfunc\n,\nvectorize\n=\nFalse\n,\ninsert\n=\nTrue\n,\nformat\n=\n'astropy'\n,\nbatch_size\n=\n-1\n,\n**\nevaluate_kwargs\n)\n\nIterate over the rows in this collection, apply\nfunc\nto each, and collect\nthe result as new columns in the dataset. You may also choose to simply return thevalues\ninstead of inserting them as a column\nThis function is the equivalent of\nwith_new_columns\nfor cases where the new column is not a simple algebraic combination of existing columns. Unlike\nwith_new_columns\n, this method will evaluate the results immediately and the resulting\ncolumns will not change under unit transformations.\nThe function should take in arguments with the same name as the columns in this dataset that\nare needed for the computation, and should return a dictionary of output values.\nThe dataset will automatically select the needed columns to avoid unnecessarily reading\ndata from disk. The new columns will have the same names as the keys of the output dictionary\nSee\nEvaluating on Datasets\nfor more details.\nIf vectorize is set to True, the full columns will be pased to the dataset. Otherwise,\nrows will be passed to the function one at a time.\nIf a\nbatch_size\nis set, opencosmo will pass data to your function in batches of rows. In a lightcone,\nbatches may be smaller than the given chunk size but will never be larger. Exact batch sizes\nwill depend on the layout of the lightcone. Setting a batch size overrides the\nvectorize\nflag.\nThis function behaves (mostly) identically to\nDataset.evaluate\nParameters\n:\nfunc\n(\nCallable\n) – The function to evaluate on the rows in the dataset.\nformat\n(\nstr\n,\ndefault = \"astropy\"\n) – The format of the data that is provided to your function. If “astropy”, will be a dictionary of\nastropy quantities. If “numpy”, will be a dictionary of numpy arrays. Note that\nthis method does not support all the formats available in\nget_data\nvectorize\n(\nbool\n,\ndefault = False\n) – Whether to provide the values as full columns (True) or one row at a time (False)\ninsert\n(\nbool\n,\ndefault = True\n) – If true, the data will be inserted as a column in this dataset. Otherwise the data will be returned.\nbatch_size\n(\nint\n)\nReturns\n:\ndataset\n– The new lightcone dataset with the evaluated column(s)\nReturn type\n:\nLightcone\nfilter\n(\n*\nmasks\n,\n**\nkwargs\n)\n\nFilter the dataset based on some criteria. See\nQuerying Based on Column Values\nfor more information.\nParameters\n:\n*masks\n(\nMask\n) – The masks to apply to dataset, constructed with\nopencosmo.col()\nReturns\n:\ndataset\n– The new dataset with the masks applied.\nReturn type\n:\nDataset\nRaises\n:\nValueError\n– If the given  refers to columns that are\n    not in the dataset, or the  would return zero rows.\nrows\n(\n)\n\nIterate over the rows in the dataset. Rows are returned as a dictionary\nFor performance, it is recommended to first select the columns you need to\nwork with.\nYields\n:\nrow\n(\ndict\n) – A dictionary of values for each row in the dataset with units.\nReturn type\n:\nGenerator[dict[str, float | u.Quantity], None, None]\nselect\n(\ncolumns\n)\n\nCreate a new dataset from a subset of columns in this dataset.\nParameters\n:\ncolumns\n(\nstr\nor\nlist\n[\nstr\n]\n) – The column or columns to select.\nReturns\n:\ndataset\n– The new dataset with only the selected columns.\nReturn type\n:\nDataset\nRaises\n:\nValueError\n– If any of the given columns are not in the dataset.\ndrop\n(\ncolumns\n)\n\nProduce a new dataset by dropping columns from this dataset.\nParameters\n:\ncolumns\n(\nstr\nor\nlist\n[\nstr\n]\n) – The column or columns to drop.\nReturns\n:\ndataset\n– The new dataset without the dropped columns\nReturn type\n:\nDataset\nRaises\n:\nValueError\n– If any of the given columns are not in the dataset.\ntake\n(\nn\n,\nat\n=\n'random'\n)\n\nCreate a new dataset from some number of rows from this dataset.\nCan take the first n rows, the last n rows, or n random rows\ndepending on the value of ‘at’.\nParameters\n:\nn\n(\nint\n) – The number of rows to take.\nat\n(\nstr\n) – Where to take the rows from. One of “start”, “end”, or “random”.\nThe default is “random”.\nReturns\n:\ndataset\n– The new dataset with only the selected rows.\nReturn type\n:\nDataset\nRaises\n:\nValueError\n– If n is negative or greater than the number of rows in the dataset,\n    or if ‘at’ is invalid.\ntake_range\n(\nstart\n,\nend\n)\n\nCreate a new lightcone from a row range in this lightcone. We use standard\nindexing conventions, so the rows included will be start -> end - 1. Because\nlightcones are stacked by redshift, this operation effectively takes a\nredshift range. If you know the exact redshift range you want, use\nwith_redshift_range\n.\nParameters\n:\nstart\n(\nint\n) – The beginning of the range\nend\n(\nint\n) – The end of the range\nReturns\n:\nlightcone\n– The lightcone with only the specified range of rows.\nReturn type\n:\nopencosmo.Lightcone\nRaises\n:\nValueError\n– If start or end are negative or greater than the length of the dataset\n    or if end is greater than start.\ntake_rows\n(\nrows\n)\n\nTake the rows of a lightcone specified by the\nrows\nargument.\nrows\nshould be an array of integers.\nParameters\n:\nrows\n(\nnp.ndarray\n[\nint\n]\n) – The indices of the rows to take.\nReturns\n:\ndataset\n(\nThe dataset with only the specified rows included\n)\nRaises\n——-\nValueError\n– If any of the indices is less than 0 or greater than the length of the\nlightcone.\nwith_new_columns\n(\ndescriptions\n=\n{}\n,\n**\ncolumns\n)\n\nCreate a new dataset with additional columns. These new columns can be derived\nfrom columns already in the dataset, a numpy array, or an Astropy quantity\narray. When a column is derived from other columns, it will behave\nappropriately under unit transformations. See\nAdding Custom Columns\nand\nDataset.with_new_columns\nfor examples.\nParameters\n:\ndescriptions\n(\nstr\n|\ndict\n[\nstr\n,\nstr\n]\n,\noptional\n) – A description for the new columns. These descriptions will be accessible through\nLightcone.descriptions\n. If a dictionary,\nshould have keys matching the column names.\ncolumns\n(\n**\n) – The new columns\nReturns\n:\ndataset\n– This dataset with the columns added\nReturn type\n:\nopencosmo.Dataset\nsort_by\n(\ncolumn\n,\ninvert\n=\nFalse\n)\n\nSort this dataset by the values in a given column. By default sorting is in\nascending order (least to greatest). Pass invert = True to sort in descending\norder (greatest to least).\nThis can be used to, for example, select largest halos in a given\ndataset:\ndataset\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\ndataset\n=\ndataset\n.\nsort_by\n(\n\"fof_halo_mass\"\n)\n.\ntake\n(\n100\n,\nat\n=\n\"start\"\n)\nParameters\n:\ncolumn\n(\nstr\n) – The column in the halo_properties or galaxy_properties dataset to\norder the collection by.\ninvert\n(\nbool\n,\ndefault = False\n) – If False (the default), ordering will be from least to greatest.\nOtherwise greatest to least.\nReturns\n:\nresult\n– A new Dataset ordered by the given column.\nReturn type\n:\nDataset\nwith_units\n(\nconvention\n=\nNone\n,\nconversions\n=\n{}\n,\n**\ncolumns\n)\n\nCreate a new lightcone from this one with a different unit convention or\nwith certain columns converted to a different compatible unit.\nUnit conversions are always performed after a change of convention, and\nchanging conventions clears any existing unit conversions.\nFor more, see\nWorking with Units\n.\nimport\nastropy.units\nas\nu\n# this works\nlc\n=\nlc\n.\nwith_units\n(\nfof_halo_mass\n=\nu\n.\nkg\n)\n# this clears the previous conversion\nlc\n=\nlc\n.\nwith_units\n(\n\"scalefree\"\n)\n# This now fails, because the units of masses\n# are Msun / h, which cannot be converted to kg\nlc\n=\nlc\n.\nwith_units\n(\nfof_halo_mass\n=\nu\n.\nkg\n)\n# this will now work, wince the units of halo mass in the \"physical\"\n# convention are Msun (no h).\nlc\n=\nlc\n.\nwith_units\n(\n\"physical\"\n,\nfof_halo_mass\n=\nu\n.\nkg\n,\nfof_halo_center_x\n=\nu\n.\nlyr\n)\n# Suppose you want your distances in lightyears, but the x coordinate of your\n# halo center in kilometers, for some reason ¯\\_(ツ)_/¯\nblanket_conversions\n=\n{\nu\n.\nMpc\n:\nu\n.\nlyr\n}\nlc\n=\nlc\n.\nwith_units\n(\nconversions\n=\nblanket_conversions\n,\nfof_halo_center_x\n=\nu\n.\nkm\n)\nParameters\n:\nconvention\n(\nstr\n,\noptional\n) – The unit convention to use. One of “physical”, “comoving”,\n“scalefree”, or “unitless”.\nconversions\n(\ndict\n[\nastropy.units.Unit\n,\nastropy.units.Unit\n]\n) – Conversions that apply to all columns in the lightcone with the\nunit given by the key.\n**column_conversions\n(\nastropy.units.Unit\n) – Custom unit conversions for specific columns\nin this dataset.\ncolumns\n(\nu.Unit\n)\nReturns\n:\nlightcone\n– The new lightcone with the requested unit convention and/or conversions.\nReturn type\n:\nLightcone\nclass\nopencosmo.\nSimulationCollection\n(\ndatasets\n)\n\nA collection of datasets of the same type from different\nsimulations. In general this exposes the exact same API\nas the individual datasets, but maps the results across\nall of them.\nParameters\n:\ndatasets\n(\nMapping\n[\nstr\n,\nDataset\n|\nCollection\n]\n)\nmake_schema\n(\n)\n\nReturn type\n:\nSchema\nproperty\ndtype\n:\ndict\n[\nstr\n,\nstr\n]\n\nproperty\nheader\n:\ndict\n[\nstr\n,\nOpenCosmoHeader\n]\n\nproperty\ncosmology\n:\ndict\n[\nstr\n,\nCosmology\n]\n\nGet the cosmologies of the simulations in the collection\nReturns\n:\ncosmologies\nReturn type\n:\ndict[str, astropy.cosmology.Cosmology]\nproperty\nredshift\n:\ndict\n[\nstr\n,\nfloat\n|\ntuple\n[\nfloat\n,\nfloat\n]\n]\n\nGet the redshift slices or ranges for the simulations in the collection\nReturns\n:\nredshifts\nReturn type\n:\ndict[str, float | tuple[float,float]]\nproperty\nsimulation\n:\ndict\n[\nstr\n,\nHaccSimulationParameters\n]\n\nGet the simulation parameters for the simulations in the collection\nReturns\n:\nsimulation_parameters\nReturn type\n:\ndict[str, opencosmo.parameters.HaccSimulationParameters]\nbound\n(\nregion\n,\nselect_by\n=\nNone\n)\n\nRestrict the datasets to some region. Note that the SimulationCollection does\nnot do any checking to ensure its members have identical boxes. As a result\nthis method can in principle fail for some of the simulations in the\ncollection and not others. This should never happen when working with official\nOpenCosmo data products.\nSee\nRegions\nfor details of how to construct regions.\nParameters\n:\nregion\n(\nopencosmo.spatial.Region\n) – The region to query\nselect_by\n(\nOptional\n[\nstr\n]\n)\nReturns\n:\ndataset\n– The portion of each dataset inside the selected region\nReturn type\n:\nopencosmo.SimulationCollection\nfilter\n(\n*\nmasks\n,\n**\nkwargs\n)\n\nFilter the datasets in the collection. This method behaves\nexactly like\nopencosmo.Dataset.filter()\nor\nopencosmo.StructureCollection.filter()\n, but\nit applies the filter to all the datasets or collections\nwithin this collection. The result is a new collection.\nParameters\n:\nfilters\n– The filters constructed with\nopencosmo.col()\nmasks\n(\nColumnMask\n)\nReturns\n:\nA new collection with the same datasets, but only the\nparticles that pass the filter.\nReturn type\n:\nSimulationCollection\nselect\n(\n*\nargs\n,\n**\nkwargs\n)\n\nSelect a set of columns in the datasets in this collection. This method\ncalls the underlying method in\nopencosmo.Dataset\n, or\nopencosmo.Collection\ndepending on the context. As such\nits behavior and arguments can vary depending on what this collection\ncontains.\nParameters\n:\nargs\n– The arguments to pass to the select method. This is\nusually a list of column names to select.\nkwargs\n– The keyword arguments to pass to the select method.\nThis is usually a dictionary of column names to select.\nReturns\n:\nA new collection with only the specified columns\nReturn type\n:\nSimulationCollection\ndrop\n(\n*\nargs\n,\n**\nkwargs\n)\n\nDrop a set of columns from the datasets in the collection. This method\ncalls the underlying method in\nopencosmo.Dataset\n, or\nopencosmo.Collection\ndepending on the context. As such\nits behavior and arguments can vary depending on what this collection\ncontains.\nParameters\n:\nargs\n– The arguments to pass to the select method. This is\nusually a list of column names to drop.\nkwargs\n– The keyword arguments to pass to the select method.\nThis is usually a dictionary of column names to select.\nReturn type\n:\nSelf\ntake\n(\nn\n,\nat\n=\n'random'\n)\n\nTake a subest of rows from all datasets or collections in this collection.\nThis method will delegate to the underlying method in\nopencosmo.Dataset\n, or\nopencosmo.StructureCollection\ndepending\non  the context. As such, behavior may vary depending on what this collection\ncontains. See their documentation for more info.\nParameters\n:\nn\n(\nint\n) – The number of rows to take\nat\n(\nstr\n,\ndefault = \"random\"\n) – The method to use to take rows. Must be one of “start”, “end”, “random”.\nReturn type\n:\nSelf\ntake_range\n(\nstart\n,\nend\n)\n\nTake a range of rows from all datasets or collections in this collection.\nThis method will fail if\nstart\n< 0, or any of the datasets are not at least\nend\nlong.\nParameters\n:\nn\n(\nint\n) – The number of rows to take\nat\n(\nstr\n,\ndefault = \"random\"\n) – The method to use to take rows. Must be one of “start”, “end”, “random”.\nstart\n(\nint\n)\nend\n(\nint\n)\nReturns\n:\nThe new simulation collection with only the specified rows.\nReturn type\n:\nSimulationCollection\nwith_new_columns\n(\n*\nargs\n,\ndatasets\n=\nNone\n,\ndescriptions\n=\n{}\n,\n**\nkwargs\n)\n\nUpdate the datasets within this collection with a set of new columns.\nThis method simply calls\nopencosmo.Dataset.with_new_columns()\nor\nopencosmo.StructureCollection.with_new_columns()\n, as appropriate.\nYou can also optionally pass the “datasets” keyword argument to specify that the\noperation should only be performed on a subset of the datasets.\nIf passing in numpy arrays or astropy quantities, they should be provided\nas a dictionary where the keys are the same as the keys in this dataset.\nParameters\n:\ndatasets\n(\nstr\n|\nlist\n[\nstr\n]\n,\noptional\n) – The datasets to add the columns to.\ndescriptions\n(\nstr\n|\ndict\n[\nstr\n,\nstr\n]\n,\noptional\n) – A description for the new columns. These descriptions will be accessible through\nSimulationCollection(datasets).descriptions\n.\nIf a dictionary, should have keys matching the column names.\ncolumns\n(\n**\n) – The new columns\nevaluate\n(\nfunc\n,\ndatasets\n=\nNone\n,\nformat\n=\n'astropy'\n,\nvectorize\n=\nFalse\n,\ninsert\n=\nFalse\n,\n**\nevaluate_kwargs\n)\n\nEvaluate the function\nfunc\non each of the datasets or collections\nheld by this SimulationCollection. This function simply delegates to the\neither\nStructureCollection.evaluate\nor\nDataset.evaluate\nas appropriate. Refer\nto\nEvaluating Complex Expressions on Datasets and Collections\nfor more details.\nIf “datasets” is provided, the evaluation will only be performed on the provided\ndatasets.\nParameters\n:\nfunc\n(\nCallable\n) – The function to evaluate\ndatasets\n(\nstr\n|\nlist\n[\nstr\n]\n,\noptional\n) – The datasets to evaluate on. If not provided, will be evaluated on all datasets\nformat\n(\nstr\n,\ndefault = \"astropy\"\n) – The format of the data that is provided to your function. If “astropy”, will be a dictionary of\nastropy quantities. If “numpy”, will be a dictionary of numpy arrays. Note that\nthis method does not support all the formats available in\nget_data\nvectorize\n(\nbool\n,\ndefault = False\n) – Whether to vectorize the computation. See\nStructureCollection.evaluate\nand/or\nDataset.evaluate\nfor more details.\ninsert\n(\nbool\n,\ndefault = True\n) – Whether or not to insert the results as columns in the datasets. If false, the results will\nbe returned directly. If true, this method will return a new Simulation Collection.\nReturns\n:\nresults\n– The results of the computation, or a new simulation collection with the results inserted.\nReturn type\n:\nSimulationCollection\n| dict[str, np.ndarray] | dict[str, astropy.units.Quantity]\nsort_by\n(\ncolumn\n,\ninvert\n=\nFalse\n)\n\nRe-order the individual datasets in the collection based on a column. See\nDataset.sort_by\nfor usage details.\nParameters\n:\ncolumn\n(\nstr\n) – The column in the halo_properties or galaxy_properties dataset to\norder the collection by.\ninvert\n(\nbool\n,\ndefault = False\n) – If False (the default) ordering will be done from least to greatest.\nOtherwise greatest to least.\nReturns\n:\nresult\n– A new SimulationCollection with the datasets ordered by the given column.\nReturn type\n:\nSimulationCollection\nwith_units\n(\nconvention\n=\nNone\n,\nconversions\n=\n{}\n,\n**\ncolumns\n)\n\nTransform all datasets or collections to use the given unit convention, convert\nall columns with a given unit into a different unit, and/or convert specific column(s)\nto a compatible unit. This method behaves exactly like\nopencosmo.Dataset.with_units()\n.\nParameters\n:\nconvention\n(\nstr\n) – The unit convention to use. One of “unitless”,\n“scalefree”, “comoving”, or “physical”.\nconversions\n(\ndict\n[\nastropy.units.Unit\n,\nastropy.units.Unit\n]\n) – Conversions that apply to all columns in the collection with the\nunit given by the key.\n**column_conversions\n(\nastropy.units.Unit\n) – Custom unit conversions for any column with a specific\nname in the datasets in this collection.\ncolumns\n(\nu.Unit\n)\nReturns\n:\nA new simulation collection with the requested unit conventions and conversions.\nReturn type\n:\ncollection\nclass\nopencosmo.\nStructureCollection\n(\nsource\n,\nheader\n,\ndatasets\n,\nhide_source\n=\nFalse\n,\nlink_handler\n=\nNone\n,\nderived_columns\n=\nNone\n,\n**\nkwargs\n)\n\nA collection of datasets that contain both high-level properties\nand lower level information (such as particles) for structures\nin the simulation. Currently these structures include halos\nand galaxies.\nEvery structure collection has a halo_properties or galaxy_properties dataset\nthat contains the high-level measured attribute of the structures. Certain\noperations (e.g.\nsort_by\noperate on this dataset.\nParameters\n:\nsource\n(\noc.Dataset\n)\nheader\n(\noc.header.OpenCosmoHeader\n)\ndatasets\n(\nMapping\n[\nstr\n,\noc.Dataset\n|\nStructureCollection\n]\n)\nhide_source\n(\nbool\n)\nlink_handler\n(\nOptional\n[\nLinkHandler\n]\n)\nderived_columns\n(\nOptional\n[\nset\n[\nstr\n]\n]\n)\nproperty\nheader\n\nproperty\ndtype\n\nproperty\ncosmology\n:\nastropy.cosmology.Cosmology\n\nThe cosmology of the structure collection\nproperty\nproperties\n:\nlist\n[\nstr\n]\n\nThe high-level properties that are available as part of the\nhalo_properties or galaxy_properties dataset.\nproperty\nredshift\n:\nfloat\n|\ntuple\n[\nfloat\n,\nfloat\n]\n|\nNone\n\nFor snapshots, return the redshift or redshift range\nthis dataset was drawn from.\nReturns\n:\nredshift\nReturn type\n:\nfloat | tuple[float, float]\nproperty\nsimulation\n:\nHaccSimulationParameters\n\nGet the parameters of the simulation this dataset is drawn\nfrom.\nReturns\n:\nparameters\nReturn type\n:\nopencosmo.parameters.HaccSimulationParameters\nkeys\n(\n)\n\nReturn the names of the datasets in this collection.\nReturn type\n:\nlist[str]\nvalues\n(\n)\n\nReturn the datasets in this collection.\nReturn type\n:\nlist[\nDataset\n|\nStructureCollection\n]\nitems\n(\n)\n\nReturn the names and datasets as key-value pairs.\nReturn type\n:\nGenerator\n[tuple[str,\nDataset\n|\nStructureCollection\n], None, None]\nproperty\nregion\n\nbound\n(\nregion\n,\nselect_by\n=\nNone\n)\n\nRestrict this collection to only contain structures in the specified region.\nQuerying will be done based on the halo  or galaxy centers, meaning some\nparticles may fall outside the given region.\nSee\nRegions\nfor details of how to construct regions.\nParameters\n:\nregion\n(\nopencosmo.spatial.Region\n)\nselect_by\n(\nOptional\n[\nstr\n]\n)\nReturns\n:\ndataset\n– The portion of the dataset inside the selected region\nReturn type\n:\nopencosmo.Dataset\nRaises\n:\nValueError\n– If the query region does not overlap with the region this dataset resides\n    in\nAttributeError:\n– If the dataset does not contain a spatial index\nevaluate\n(\nfunc\n,\ndataset\n=\nNone\n,\nformat\n=\n'astropy'\n,\ninsert\n=\nTrue\n,\n**\nevaluate_kwargs\n)\n\nIterate over the structures in this collection and apply func to each,\ncollecting the results into a new column. These values will be computed\nimmediately rather than lazily. If your new column can be created from a\nsimple algebraic combination of existing columns, use\nwith_new_columns\n.\nYou can substantially improve the performance of this method by specifying\nwhich data is actually needed to do the computation. This method will\nautomatically select the requested data, avoiding reading unneeded data\nfrom disk. The semantics for specifying the columns is identical to\nselect\n.\nThe function passed to this method must take arguments that match the names\nof datasets that are stored in this collection. You can specify specific\ncolumns that are needed with keyword arguments to this function. For example:\nimport\nopencosmo\nas\noc\nimport\nnumpy\nas\nnp\ncollection\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n)\ndef\ncomputation\n(\nhalo_properties\n,\ndm_particles\n):\ndx\n=\nnp\n.\nmean\n(\ndm_particles\n.\ndata\n[\n\"x\"\n])\n-\nhalo_properties\n[\n\"fof_halo_center_x\"\n]\ndy\n=\nnp\n.\nmean\n(\ndm_particles\n.\ndata\n[\n\"y\"\n])\n-\nhalo_properties\n[\n\"fof_halo_center_y\"\n]\ndz\n=\nnp\n.\nmean\n(\ndm_particles\n.\ndata\n[\n\"z\"\n])\n-\nhalo_properties\n[\n\"fof_halo_center_z\"\n]\noffset\n=\nnp\n.\nsqrt\n(\ndx\n**\n2\n+\ndy\n**\n2\n+\ndz\n**\n2\n)\nreturn\noffset\n/\nhalo_properties\n[\n\"sod_halo_radius\"\n]\ncollection\n=\ncollection\n.\nevaluate\n(\ncomputation\n,\nname\n=\n\"offset\"\n,\nhalo_properties\n=\n[\n\"fof_halo_center_x\"\n,\n\"fof_halo_center_y\"\n,\n\"fof_halo_center_z\"\n\"sod_halo_radius\"\n],\ndm_particles\n=\n[\n\"x\"\n,\n\"y\"\n,\n\"z\"\n]\n)\nThe collection will now contain a column named “offset” with the results of the\ncomputation applied to each halo in the collection.\nIt is not required to pass a list of column names for a given dataset. If a list\nis not provided, all columns will be passed to the computation function. Data will\nbe passed into the function as numpy arrays or astropy tables, depending on the\nvalue of the “format” argument. However if the evaluation involes a nested\nstructure collection (e.g. a galaxy collection inside a structure collection)\nin addition to other datasets, the nested collection will be passed to your\nfunction as a StructureCollection.\nFor more details and advanced usage see\nEvaluating on Structure Collections\nParameters\n:\nfunc\n(\nCallable\n) – The function to evaluate on the rows in the dataset.\ndataset\n(\nOptional\n[\nstr\n]\n,\ndefault = None\n) – The dataset inside this collection to evaluate the function on. If none, assumes the function requires data from\nmultiple datasets. You can visit a dataset inside a nested structure collection by passing the path\nseparated by dots, for example “galaxies.star_particles”. Data will be fed to the function on a structure-by-structure\nbasis, and the output should be the same length as the input data.\ninsert\n(\nbool\n,\ndefault = True\n) – If true, the data will be inserted as a column in the specified dataset. If no dataset is specified, insert\ninto the “halo_properties” dataset if this collection contains halos, or the “galaxy properties” if this\ncollection contains galaxies. If False, simply return the data.\nformat\n(\nstr\n,\ndefault = astropy\n) – Whether to provide data to your function as “astropy” quantities or “numpy” arrays/scalars. Default “astropy”. Note that\nthis method does not support all the formats available in\nget_data\n**evaluate_kwargs\n(\nany\n,\n) – Any additional arguments that are required for your function to run. These will be passed directly\nto the function as keyword arguments. If a kwarg is an array of values with the same length as the dataset,\nit will be treated as an additional column.\nevaluate_on_dataset\n(\nfunc\n,\ndataset\n=\nNone\n,\nvectorize\n=\nFalse\n,\nformat\n=\n'astropy'\n,\ninsert\n=\nTrue\n,\nbatch_size\n=\n-1\n,\n**\nevaluate_kwargs\n)\n\nEvaluate an expression on a specific dataset in this collection. This method is different from calling\nevaulate\nwith a\ndataset\nargument\nin that this method does not apply the function on a per-structure basis. It is roughtly equivalent\nto the following code:\nresults\n=\ncollection\n[\ndataset_name\n]\n.\nevaluate\n(\nfunc\n,\nformat\n,\nvectorize\n,\ninsert\n=\nFalse\n)\ncollection\n=\ncollection\n.\nwith_new_columns\n(\ndataset_name\n,\nmy_computed_value\n=\nresults\n)\nKeep in mind that the following code:\ncollection\n[\ndataset_name\n]\n.\nevaluate\n(\nfunc\n,\nformat\n,\nvectorize\n,\ninsert\n=\ntrue\n)\ndoes\nproduces a new dataset with the given new column, but this dataset will not be a part of the\noriginal collection.\nParameters\n:\nfunc\n(\nCallable\n) – The function to evaluate on the rows in the dataset.\ndataset\n(\nOptional\n[\nstr\n]\n= None\n,\n) – The dataset to perform the evaluation on. If None, defaults to the halo_properties or galaxy_properties dataset.\nvectorize\n(\nbool\n,\ndefault = False\n) – Whether to provide the values as full columns (True) or one row at a time (False). Ignored if\nbatch_size\nis set.\nformat\n(\nstr\n,\ndefault = astropy\n) – Whether to provide data to your function as “astropy” quantities or “numpy” arrays/scalars. Default “astropy”. Note that\nthis method does not support all the formats available in\nget_data\ninsert\n(\nbool\n,\ndefault = True\n) – If true, the data will be inserted as a column in this dataset. The new column will have the same name\nas the function. Otherwise the data will be returned directly.\nbatch_size\n(\nint\n,\ndefault = -1\n) – If set, feed data to the function in batches of the specified size. Default is -1, which disables batching. If\nset to another value, the\nvectorize\nflag is ignored.\n**evaluate_kwargs\n(\nany\n,\n) – Any additional arguments that are required for your function to run. These will be passed directly\nto the function as keyword arguments. If a kwarg is an array of values with the same length as the dataset,\nit will be treated as an additional column.\nfilter\n(\n*\nmasks\n,\non_galaxies\n=\nFalse\n)\n\nApply a filter to the halo or galaxy properties. Filters are constructed with\nopencosmo.col()\nand behave exactly as they would in\nopencosmo.Dataset.filter()\n.\nIf the collection contains both halos and galaxies, the filter can be applied to\nthe galaxy properties dataset by setting\non_galaxies=True\n. However this will\nfilter for\nhalos\nthat host galaxies that match this filter. As a result,\ngalxies that do not match this filter will remain if another galaxy in their\nhost halo does match.\nSee\nQuerying In Collections\nfor some examples.\nParameters\n:\n*filters\n(\nMask\n) – The filters to apply to the properties dataset constructed with\nopencosmo.col()\n.\non_galaxies\n(\nbool\n,\noptional\n) – If True, the filter is applied to the galaxy properties dataset.\nReturns\n:\nA new collection filtered by the given masks.\nReturn type\n:\nStructureCollection\nRaises\n:\nValueError\n– If on_galaxies is True but the collection does not contain\n    a galaxy properties dataset.\nselect\n(\n**\ncolumn_selections\n)\n\nUpdate a dataset in the collection collection to only include the\ncolumns specified. The name of the arguments to this function should be\ndataset names. For example:\ncollection\n=\ncollection\n.\nselect\n(\nhalo_properties\n=\n[\n\"fof_halo_mass\"\n,\n\"sod_halo_mass\"\n,\n\"sod_halo_cdelta\"\n],\ndm_particles\n=\n[\n\"x\"\n,\n\"y\"\n,\n\"z\"\n]\n)\nDatasets that do not appear in the argument list will not be modified. You can\nremove entire datasets from the collection with\nwith_datasets\nFor nested structure collections, such as galaxies within halos, you can pass\na nested dictionary:\ncollection\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n,\n\"haloparticles.hdf5\"\n,\n\"galaxyproperties.hdf5\"\n,\n\"galaxyparticles.hdf5\"\n)\ncollection\n=\ncollection\n.\nselect\n(\nhalo_properties\n=\n[\n\"fof_halo_mass\"\n,\n\"sod_halo_mass\"\n,\n\"sod_halo_cdelta\"\n],\ndm_particles\n=\n[\n\"x\"\n,\n\"y\"\n,\n\"z\"\n]\ngalaxies\n=\n{\n\"galaxy_properties\"\n:\n[\n\"gal_mass_bar\"\n,\n\"gal_mass_star\"\n],\n\"star_particles\"\n:\n[\n\"x\"\n,\n\"y\"\n,\n\"z\"\n]\n}\n)\nParameters\n:\n**column_selections\n(\nstr\n|\nIterable\n[\nstr\n]\n|\ndict\n[\nstr\n,\nIterable\n[\nstr\n]\n]\n) – The columns to select from a given dataset or sub-collection\ndataset\n(\nstr\n) – The dataset to select from.\nReturns\n:\nA new collection with only the selected columns for the specified dataset.\nReturn type\n:\nStructureCollection\nRaises\n:\nValueError\n– If the specified dataset is not found in the collection.\ndrop\n(\n**\ncolumns_to_drop\n)\n\nUpdate the linked collection by dropping the specified columns\nin the specified datasets. This method follows the exact same semantics as\nStructureCollection.select\n.\nArgument names should be datasets in this collection, and the argument\nvalues should be a string, list of strings, or dictionary.\nDatasets that are not included will not be modified. You can drop\nentire datasets with\nwith_datasets\nParameters\n:\n**columns_to_drop\n(\nstr\n|\nIterable\n[\nstr\n]\n) – The columns to drop from the dataset.\ndataset\n(\nstr\n,\noptional\n) – The dataset to select from. If None, the properties dataset is used.\nReturns\n:\nA new collection with only the selected columns for the specified dataset.\nReturn type\n:\nStructureCollection\nRaises\n:\nValueError\n– If the specified dataset is not found in the collection.\nsort_by\n(\ncolumn\n,\ninvert\n=\nFalse\n)\n\nRe-order the collection based on one of the structure collection’s properties. Each\nStructureCollection contains a halo_properties or galaxy_properties dataset that\ncontains the high-level measured properties of the structures in this collection.\nThis method always operates on that dataset.\nParameters\n:\ncolumn\n(\nstr\n) – The column in the halo_properties or galaxy_properties dataset to\norder the collection by.\ninvert\n(\nbool\n,\ndefault = False\n) – If False (the default), ordering will be from least to greatest.\nOtherwise greatest to least.\nReturns\n:\nresult\n– A new StructureCollection ordered by the given column.\nReturn type\n:\nStructureCollection\nwith_units\n(\nconvention\n=\nNone\n,\nconversions\n=\n{}\n,\n**\ndataset_conversions\n)\n\nApply the given unit convention to the collection, or convert a subset\nof the columns in one or more of these datasets into a compatible\nunit.\nBecause this collection contains several datasets, you must specify\nthe dataset when performing conversions. For example, the equivalent\nunit conversion to the final one in the example in\nopencosmo.Dataset.with_units()\nlooks like this:\nimport\nastropy.units\nas\nu\nstructures\n=\nstructures\n.\nwith_units\n(\n\"physical\"\n,\nhalo_properties\n=\n{\n\"fof_halo_mass\"\n:\nu\n.\nkg\n,\n\"fof_halo_center_x\"\n:\nu\n.\nly\n}\n)\nYou can use\nconversions\nto specify a conversion that applies to all\ncolumns in the collection with the given unit, or specify per-dataset conversions.\nPer-dataset conversions always take precedent over collection-wide conversions.\nFor example:\nimport\nastropy.units\nas\nu\nconversions\n=\n{\nu\n.\nMpc\n:\nu\n.\nlyr\n}\nstructures\n=\nstructures\n.\nwith_units\n(\nconversions\n=\nconversions\nhalo_properties\n=\n{\n\"conversions\"\n:\n{\nu\n.\nMpc\n:\nu\n.\nkm\n},\n\"fof_halo_center_x\"\n:\nu\n.\nm\n}\n)\nIn this example, all values in Mpc will be converted to lightyears, except in the “halo_properties” dataset,\nwhere they will be converted to kilometers. The column “fof_halo_center_x” in “halo_properties” will\nbe converted to meters instead.\nFor more information, see\nWorking with Units\nParameters\n:\nconvention\n(\nstr\n) – The unit convention to apply. One of “unitless”, “scalefree”,\n“comoving”, or “physical”.\nconversions\n(\ndict\n[\nastropy.units.Unit\n,\nastropy.units.Unit\n]\n) – Unit conversions to apply across all columns in the collection\n**dataset_conversion\n(\ndict\n) – Unit conversions apply to specific datasets in the collection.\ndataset_conversions\n(\ndict\n)\nReturns\n:\nA new collection with the unit convention applied.\nReturn type\n:\nStructureCollection\ntake\n(\nn\n,\nat\n=\n'random'\n)\n\nTake some number of structures from the collection.\nSee\nopencosmo.Dataset.take()\n.\nParameters\n:\nn\n(\nint\n) – The number of structures to take from the collection.\nat\n(\nstr\n,\noptional\n) – The method to use to take the structures. One of “random”, “first”,\nor “last”. Default is “random”.\nReturns\n:\nA new collection with the structures taken from the original.\nReturn type\n:\nStructureCollection\ntake_range\n(\nstart\n,\nend\n)\n\nCreate a new collection from a row range in this collection. We use standard\nindexing conventions, so the rows included will be start -> end - 1.\nParameters\n:\nstart\n(\nint\n) – The first row to get.\nend\n(\nint\n) – The last row to get.\nReturns\n:\ntable\n– The table with only the rows from start to end.\nReturn type\n:\nastropy.table.Table\nRaises\n:\nValueError\n– If start or end are negative or greater than the length of the dataset\n    or if end is greater than start.\ntake_rows\n(\nrows\n)\n\nTake the rows of this collection  specified by the\nrows\nargument.\nrows\nshould be an array of integers.\n## Parameters:\nParameters:\n\nrows: np.ndarray[int]\nreturns\n:\ndataset\n(\nThe dataset with only the specified rows included\n)\nRaises\n——-\nValueError\n– If any of the indices is less than 0 or greater than the length of the\ndataset.\nParameters\n:\nrows\n(\nnp.ndarray\n|\nDataIndex\n)\nwith_new_columns\n(\ndataset\n,\ndescriptions\n=\n{}\n,\n**\nnew_columns\n)\n\nAdd new column(s) to one of the datasets in this collection. This behaves\nexactly like\noc.Dataset.with_new_columns()\n, except that you must\nspecify which dataset the columns should refer too.\npe\n=\noc\n.\ncol\n(\n\"phi\"\n)\n*\noc\n.\ncol\n(\n\"mass\"\n)\ncollection\n=\ncollection\n.\nwith_new_columns\n(\n\"dm_particles\"\n,\npe\n=\npe\n)\nStructure collections can hold other structure collections. For example, a\ncollection of Halos may hold a structure collection that contians the galaxies\nof those halos. To update datasets within these collections, use dot syntax\nto specify a path:\npe\n=\noc\n.\ncol\n(\n\"phi\"\n)\n*\noc\n.\ncol\n(\n\"mass\"\n)\ncollection\n=\ncollection\n.\nwith_new_columns\n(\n\"galaxies.star_particles\"\n,\npe\n=\npe\n)\nYou can also pass numpy arrays or astropy quantities:\nrandom_value\n=\nnp\n.\nrandom\n.\nrandint\n(\n0\n,\n90\n,\nsize\n=\nlen\n(\ncollection\n))\nrandom_quantity\n=\nrandom_value\n*\nu\n.\ndeg\ncollection\n=\ncollection\n.\nwith_new_columns\n(\n\"halo_properties\"\n,\nrandom_quantity\n=\nrandom_quantity\n)\nSee\nAdding Custom Columns\nfor more examples.\nParameters\n:\ndataset\n(\nstr\n) – The name of the dataset to add columns to\ndescriptions\n(\nstr\n|\ndict\n[\nstr\n,\nstr\n]\n,\noptional\n) – Descriptions for the new columns. These descriptions will be accessible through\nDataset.descriptions\n. If a dictionary,\nshould have keys matching the column names.\ncolumns\n(\n**\n) – The new columns\nnew_columns\n(\nDerivedColumn\n)\nReturns\n:\nnew_collection\n– This collection with the additional columns added\nReturn type\n:\nopencosmo.StructureCollection\nRaises\n:\nValueError\n– If the dataset is not found in this collection\nobjects\n(\ndata_types\n=\nNone\n,\nignore_empty\n=\nTrue\n)\n\nIterate over the objects in this collection as pairs of\n(properties, datasets). For example, a halo collection could yield\nthe halo properties and datasets for each of the associated partcles.\nIf you don’t need all the datasets, you can specify a list of data types\nfor example:\nfor\nhalo\nin\ncollection\n.\nobjects\n(\ndata_types\n=\n[\n\"halo_properties\"\n,\n\"gas_particles\"\n,\n\"star_particles\"\n]):\n# do work\nAt each iteration,\nhalo\nwill be a dictionary with halo properties, gas_particles,\nand star particles. The “halo_properties” entry will itself be a dictionary with the halo’s properties,\nwhile “gas_particles” and “star_particles” will be full\nDatasets\n.\nParameters\n:\ndata_types\n(\nIterable\n[\nstr\n]\n|\nNone\n)\nReturn type\n:\nIterable\n[dict[str,\nAny\n]]\nwith_datasets\n(\ndatasets\n)\n\nCreate a new collection out of a subset of the datasets in this collection.\nIt is also possible to do this when you iterate over the collection with\nStructureCollection.objects\n,\nhowever doing it up front may be more desirable if you don’t plan to use\nthe dropped datasets at any point.\nParameters\n:\ndatasets\n(\nlist\n[\nstr\n]\n)\nhalos\n(\n*\nargs\n,\n**\nkwargs\n)\n\nAlias for “objects” in the case that this StructureCollection contains halos.\ngalaxies\n(\n*\nargs\n,\n**\nkwargs\n)\n\nAlias for “objects” in the case that this StructureCollection contains galaxies\nmake_schema\n(\nname\n=\nNone\n)\n\nParameters\n:\nname\n(\nOptional\n[\nstr\n]\n)\nReturn type\n:\nSchema\nclass\nopencosmo.\nHealpixMap\n(\ndatasets\n,\nnside\n,\nnside_lr\n,\nordering\n,\nfull_sky\n,\nz_range\n,\nhidden\n=\nNone\n,\nordered_by\n=\nNone\n,\nregion\n=\nNone\n)\n\nA HealpixMap contains one or more datasets of map format. Each dataset will\ntypically contain a different type of data over a specified integrated\nredshift range. The HealpixMap object provides an API identical to the standard\nDataset API, however the data that is provided is returned in healpix or healsparse\nformat, which are different than other opencosmo datasets. This also contains some\nconvenience functions for standard operations.\nParameters\n:\ndatasets\n(\ndict\n[\nstr\n,\nDataset\n]\n)\nnside\n(\nint\n)\nnside_lr\n(\nint\n)\nordering\n(\nstr\n)\nfull_sky\n(\nbool\n)\nz_range\n(\ntuple\n[\nfloat\n,\nfloat\n]\n)\nhidden\n(\nOptional\n[\nset\n[\nstr\n]\n]\n)\nordered_by\n(\nOptional\n[\ntuple\n[\nstr\n,\nbool\n]\n]\n)\nregion\n(\nOptional\n[\nRegion\n]\n)\nproperty\nnside\n\nThe healpix nside resolution parameter for this map\nReturns\n:\ndtype\nReturn type\n:\nint\nproperty\npixels\n\nThe healoix pixels that are included in this map\nproperty\nnside_lr\n\nThe low resolution nside resolution parameter used to\naccess this map with healsparse.\n:returns:\ndtype\n:rtype: int\nproperty\nordering\n\nThe order of pixelization for the map. Either\nNESTED or RING. Maps are currently always saved\nin NESTED format.\nReturns\n:\ndtype\nReturn type\n:\nstr\nproperty\nfull_sky\n\nWhether the map has full-sky coverage or not\n(note if not you must ask for the data in\nhealsparse format and not full healpix format)\n:returns:\ndtype\n:rtype: bool\nproperty\nheader\n:\nOpenCosmoHeader\n\nThe header associated with this dataset.\nOpenCosmo headers generally contain information about the original data this\ndataset was produced from, as well as any analysis that was done along\nthe way.\nReturns\n:\nheader\nReturn type\n:\nopencosmo.header.OpenCosmoHeader\nproperty\ncolumns\n:\nlist\n[\nstr\n]\n\nThe names of the columns in this map.\nReturns\n:\ncolumns\nReturn type\n:\nlist[str]\nproperty\ndescriptions\n:\ndict\n[\nstr\n,\nstr\n|\nNone\n]\n\nReturn the descriptions (if any) of the columns in this map as a dictonary.\nColumns without a description will be included in the dictionary with a value\nof None\nReturns\n:\ndescriptions\n– The column descriptions\nReturn type\n:\ndict[str, str | None]\nproperty\ncosmology\n:\nCosmology\n\nThe cosmology of the simulation this dataset is drawn from as\nan astropy.cosmology.Cosmology object.\nReturns\n:\ncosmology\nReturn type\n:\nastropy.cosmology.Cosmology\nproperty\nregion\n:\nRegion\n\nThe region this dataset is contained in. If no spatial\nqueries have been performed, this will be the full sky for\nlightcone maps.\nReturns\n:\nregion\nReturn type\n:\nopencosmo.spatial.Region\nproperty\nsimulation\n:\nHaccSimulationParameters\n\nThe parameters of the simulation this dataset is drawn\nfrom.\nReturns\n:\nparameters\nReturn type\n:\nopencosmo.parameters.hacc.HaccSimulationParameters\nproperty\nz_range\n\nThe redshift range of the data which created this map.\nReturns\n:\nz_range\nReturn type\n:\ntuple[float, float]\nget_data\n(\noutput\n=\n'healsparse'\n,\nnside_out\n=\nNone\n)\n\nGet the data in this dataset as healsparse map or as healpix maps\n(nest-ordered numpy array). Note that a dataset does not load data from\ndisk into memory until this function is called. As a result, you should\nnot call this function until you have performed any transformations you\nplan to on the data.\nYou can get the data in two formats, “healsparse” (the default) and “healpix”.\n“healsparse” format will return the data as a healsparse sparse map.\n“healpix” will return the data as a dictionary of numpy arrays. For map data,\ndue to format requirements, no units will be attached to the data itself,\nalthough these will match the units from the data attributes.\nParameters\n:\noutput\n(\nstr\n,\ndefault=\"healsparse\"\n) – The format to output the data in\nnside_out\n(\nint\n|\nNone\n)\nReturns\n:\ndata\n– The data in this dataset.\nReturn type\n:\nHealsparseMap |\nColumn\n| dict[str, ndarray] | ndarray\nproperty\ndata\n\nReturn the data in the dataset in healsparse format. The value of this\nattribute is equivalent to the return value of\nDataset.get_data(\"healsparse\")\n.\nReturns\n:\ndata\n– The data in the dataset.\nReturn type\n:\nHealsparseMap\nwith_resolution\n(\nnside\n)\n\nReturn a copy of the map with a new nside resolution.\nThe new resolution must be strictly less than the current\nresolution.\nReturn type\n:\nHealpixMap\nmake_schema\n(\n)\n\nReturn type\n:\nSchema\nbound\n(\nregion\n,\ninclusive\n=\nFalse\n)\n\nRestrict this map to some subregion. Be default this will\ninclude all pixels whose centers fall within the subregion. You can additionally\ninclude pixels that overalp without there centers being within the\nspecified region by passing\ninclusive=True\nIf trying to query in a circular region, consider using\ncone_search\nfor simplicity.\nParameters\n:\nregion\n(\nopencosmo.spatial.Region\n) – The region to query.\nincluive\n(\nbool\n,\ndefault = Flase\n) – Whether to include pixels that overlap but whose centers are not in the region1\ninclusive\n(\nbool\n)\nReturns\n:\nnew_map\n– The map including the pixels within the region.\nReturn type\n:\nopencosmo.HealpixMap\nRaises\n:\nValueError\n– If the query region does not overlap with the coverage of this map\n    in\ncone_search\n(\ncenter\n,\nradius\n)\n\nPerform a search for objects within some angular distance of some\ngiven point on the sky. This is a convinience function around\nbound\nand is exactly\nequivalent to\nregion\n=\noc\n.\nmake_cone\n(\ncenter\n,\nradius\n)\nds\n=\nds\n.\nbound\n(\nregion\n)\nParameters\n:\ncenter\n(\ntuple\n|\nSkyCoord\n) – The center of the region to search. If a tuple and no units are provided\nassumed to be RA and Dec in degrees.\nradius\n(\nfloat\n|\nastropy.units.Quantity\n) – The angular radius of the region to query. If no units are provided,\nassumed to be degrees.\nReturns\n:\nnew_map\n– The pixels in these maps that fall within the given region.\nReturn type\n:\nopencosmo.HealpixMap\nevaluate\n(\nfunc\n,\nformat\n=\n'numpy'\n,\nvectorize\n=\nFalse\n,\ninsert\n=\nTrue\n,\n**\nevaluate_kwargs\n)\n\nIterate over the rows in this collection, apply\nfunc\nto each, and collect\nthe result as new columns in the dataset. You may also choose to simply return thevalues\ninstead of inserting them as a column\nThis function is the equivalent of\nwith_new_columns\nfor cases where the new column is not a simple algebraic combination of existing columns. Unlike\nwith_new_columns\n, this method will evaluate the results immediately and the resulting\ncolumns will not change under unit transformations.\nThe function should take in arguments with the same name as the columns in this dataset that\nare needed for the computation, and should return a dictionary of output values.\nThe dataset will automatically select the needed columns to avoid unnecessarily reading\ndata from disk. The new columns will have the same names as the keys of the output dictionary\nSee\nEvaluating on Datasets\nfor more details.\nIf vectorize is set to True, the full columns will be pased to the dataset. Otherwise,\nrows will be passed to the function one at a time.\nThis function behaves identically to\nDataset.evaluate\nParameters\n:\nfunc\n(\nCallable\n) – The function to evaluate on the rows in the dataset.\nformat\n(\nstr\n,\ndefault = \"numpy\"\n) – The format of the data that is provided to your function. If “astropy”, will be a dictionary of\nastropy quantities. If “numpy”, will be a dictionary of numpy arrays.\nvectorize\n(\nbool\n,\ndefault = False\n) – Whether to provide the values as full columns (True) or one row at a time (False)\ninsert\n(\nbool\n,\ndefault = True\n) – If true, the data will be inserted as a column in this dataset. Otherwise the data will be returned.\nReturns\n:\ndataset\n– The new lightcone dataset with the evaluated column(s)\nReturn type\n:\nHealpixMap\nfilter\n(\n*\nmasks\n,\n**\nkwargs\n)\n\nFilter the map based on some criteria. See\nQuerying Based on Column Values\nfor more information.\nParameters\n:\n*masks\n(\nMask\n) – The masks to apply to dataset, constructed with\nopencosmo.col()\nReturns\n:\ndataset\n– The new dataset with the masks applied.\nReturn type\n:\nDataset\nRaises\n:\nValueError\n– If the given  refers to columns that are\n    not in the dataset, or the  would return zero rows.\nrows\n(\n)\n\nIterate over the pixels in the map, returning their individual values.\nRows are returned as a dictionary. For performance, it is recommended\nto first select the columns you need to work with.\nYields\n:\nrow\n(\ndict\n) – A dictionary of values for each row in the dataset with units.\nReturn type\n:\nGenerator\n[dict[str, float |\nQuantity\n], None, None]\nselect\n(\ncolumns\n)\n\nCreate a new map from a subset of columns in this map.\nParameters\n:\ncolumns\n(\nstr\nor\nlist\n[\nstr\n]\n) – The column or columns to select.\nReturns\n:\ndataset\n– The new dataset with only the selected columns.\nReturn type\n:\nDataset\nRaises\n:\nValueError\n– If any of the given columns are not in the dataset.\ndrop\n(\ncolumns\n)\n\nProduce a new dataset by dropping columns from this map.\nParameters\n:\ncolumns\n(\nstr\nor\nlist\n[\nstr\n]\n) – The column or columns to drop.\nReturns\n:\ndataset\n– The new dataset without the dropped columns\nReturn type\n:\nDataset\nRaises\n:\nValueError\n– If any of the given columns are not in the dataset.\ntake\n(\nn\n,\nat\n=\n'random'\n)\n\nCreate a new dataset from some number of rows from this map.\nCan take the first n rows, the last n rows, or n random rows\ndepending on the value of ‘at’.\nParameters\n:\nn\n(\nint\n) – The number of rows to take.\nat\n(\nstr\n) – Where to take the rows from. One of “start”, “end”, or “random”.\nThe default is “random”.\nReturns\n:\ndataset\n– The new dataset with only the selected rows.\nReturn type\n:\nDataset\nRaises\n:\nValueError\n– If n is negative or greater than the number of rows in the dataset,\n    or if ‘at’ is invalid.\ntake_range\n(\nstart\n,\nend\n)\n\nParameters\n:\nstart\n(\nint\n)\nend\n(\nint\n)\ntake_rows\n(\nrows\n)\n\nTake the rows of a map specified by the\nrows\nargument.\nrows\nshould be an array of integers. Note that for healpix\nmaps the rows refers to the pixel indices.\nParameters\n:\nrows\n(\nnp.ndarray\n[\nint\n]\n) – The indices of the rows to take.\nReturns\n:\ndataset\n(\nThe dataset with only the specified rows included\n)\nRaises\n——-\nValueError\n– If any of the indices is less than 0 or greater than the length of the\nmap.\nwith_new_columns\n(\ndescriptions\n=\n{}\n,\n**\ncolumns\n)\n\nCreate a new map with additional columns. These new columns can be derived\nfrom columns already in the dataset, or a numpy array.  See\nAdding Custom Columns\nand\nDataset.with_new_columns\nfor examples.\nParameters\n:\ndescriptions\n(\nstr\n|\ndict\n[\nstr\n,\nstr\n]\n,\noptional\n) – A description for the new columns. These descriptions will be accessible through\nHealpixMap.descriptions\n. If a dictionary,\nshould have keys matching the column names.\ncolumns\n(\n**\n) – The new columns\nReturns\n:\ndataset\n– This dataset with the columns added\nReturn type\n:\nopencosmo.Dataset\nsort_by\n(\ncolumn\n,\ninvert\n=\nFalse\n)\n\nSort this map by the values in a given column. By default sorting is in\nascending order (least to greatest). Pass invert = True to sort in descending\norder (greatest to least).\nThis is not generally particular useful in map queries, but can be used to\nenforce ordering schemes or find outlier pixels.\nParameters\n:\ncolumn\n(\nstr\n) – The column in the map dataset to\norder the collection by.\ninvert\n(\nbool\n,\ndefault = False\n) – If False (the default), ordering will be from least to greatest.\nOtherwise greatest to least.\nReturns\n:\nresult\n– A new Dataset ordered by the given column.\nReturn type\n:\nDataset\nwith_units\n(\nconvention\n=\nNone\n,\nconversions\n=\n{}\n,\n**\ncolumns\n)\n\nUnit conversion is usually supported for OpenCosmo datasets, however maps tend to be integrated\nquantities over a range of redshifts which correspond to observed units so applying unit conversions\nis not generally easy or appropriate.\nParameters\n:\nconvention\n(\nstr\n|\nNone\n)\nconversions\n(\ndict\n[\nUnit\n,\nUnit\n]\n)\ncolumns\n(\nUnit\n)\nReturn type\n:\nSelf"}
{"url": "https://opencosmo.readthedocs.io/en/latest/column_ref.html", "title": "Columns — OpenCosmo Documentation", "content": "# Columns\nColumns\n\nIn OpenCosmo, references to columns are created independently of the datasets that contain them. You can\ncreate combinations of columns with basic arithmetic, as well as …\nColumns created this way can be added to datasets or collections with the\nwith_new_columns\nmethod. The actual\nvalues\nin these columns are evaluated lazily, so it is fine to create these new columns at the beginning of your analysis even if you plan to filter out significant numbers of the rows.\nopencosmo.\ncol\n(\ncolumn_name\n)\n\nCreate a reference to a column with a given name. These references can be combined\nto produce new columns or express queries that operate on the values in a given\ndataset. For example:\nimport\nopencosmo\nas\noc\nds\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nquery\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n>\n1e14\npx\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n*\noc\n.\ncol\n(\n\"fof_halo_com_vx\"\n)\nds\n=\nds\n.\nwith_new_columns\n(\nfof_halo_com_px\n=\npx\n)\n.\nfilter\n(\nquery\n)\nFor more advanced usage, see\nWorking with Columns\nParameters\n:\ncolumn_name\n(\nstr\n)\nReturn type\n:\nColumn\nclass\nopencosmo.column.\nColumn\n(\ncolumn_name\n)\n\nRepresents a reference to a column with a given name. Column reference\nare created independently of the datasets that actually contain data.\nYou should not create this class directly, instead use\nopencosmo.col()\n.\nColumns can be combined, and support comparison operators for masking datasets.\nCombinations:\nBasic arithmetic with +, -, *, and /\nPowers with\n**\n, and\ncolumn.sqrt()\nlog and exponentiation with\ncolumn.log10()\nand\ncolumn.exp10()\nComparison operators:\nArithmetic comparisons such as <, <=, >, ==, !=\nMembership with\ncolumn.isin\nIn general, combinations of columns produce a\nDerivedColumn\n, which can be treated\nthe exact same was as basic Columns.\nFor example, to compute the x-component of a halo’s momentum, and then filter out\nhalos below a certain value of that momentum\nimport\nopencosmo\nas\noc\ndataset\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\nhalo_px\n=\noc\n.\ncol\n(\n\"fof_halo_mass\"\n)\n*\noc\n.\ncol\n(\n\"fof_halo_com_vx\"\n)\ndataset\n=\ndataset\n.\nwith_new_columns\n(\nfof_halo_com_px\n=\nhalo_px\n)\nmin_momentum_filter\n=\noc\n.\ncol\n(\n\"fof_halo_com_px) > 10**14\ndataset\n=\ndataset\n.\nfilter\n(\nmin_momentum_filter\n)\nParameters\n:\ncolumn_name\n(\nstr\n)\nexp10\n(\nexpected_unit_container\n=\n<class\n'astropy.units.function.logarithmic.DexUnit'>\n)\n\nCreate a derived column that will contain the base-10 exponentiation of the\ngiven column. If the column being exponentiated contains units, it must be an\nastropy LogUnit (e.g. Dex or Mag)\nYou can specify the type of LogUnit container you expect the column to have with\nexpected_unit_container. Defaults to DexUnit.\nParameters\n:\nexpected_unit_container\n(\nLogUnit\n)\nReturn type\n:\nDerivedColumn\nlog10\n(\nunit_container\n=\n<class\n'astropy.units.function.logarithmic.DexUnit'>\n)\n\nCreate a derived column that will compute the log of a given column. If\nthe column contains units, the units must not be an astropy LogUnit\n(such as Dex or Mag)\nIf you want the units of the new column to be a particular type of LogUnit,\nyou can pass that type to the\nunit_container\nargument. Defaults\nto DexUnit.\nParameters\n:\nunit_container\n(\nLogUnit\n)\nReturn type\n:\nDerivedColumn\nsqrt\n(\n)\n\nCreate a derived column that will contain the square root of the given column.\nReturn type\n:\nDerivedColumn\n## Provided Column Combinations\nProvided Column Combinations\n\nThere are a number of basic column combinations that are\nopencosmo.column.\nnorm_cols\n(\n*\ncolumns\n)\n\nGet the euclidian norm of any number of columns. This function takes in the names\nof the magnitude columns, and produces a DerivedColumn that can be passed into\nwith_new_columns\nThis function will never fail, but\nwith_new_columns\nwill if the columns do not have the same units.\nParameters\n:\n*columns\n(\nstr\n|\nColumn\n|\nDerivedColumn\n) – Any number of columns. You can pass in simple column names, columns constructred\nwith\nopencosmo.col()\n, or columns created from combinations of other columns\nReturns\n:\nnew_column\n– A new derived column that can be passed into\nwith_new_columns\nReturn type\n:\nDerivedColumn\nopencosmo.column.\nadd_mag_cols\n(\n*\nmagnitudes\n)\n\nAdd together any number of magnitude columns to get a total magnitude. This function\ntakes in the names of the magnitude columns, and produces a DerivedColumn that can be\npassed into\nwith_new_columns\nThis function will never fail, but\nwith_new_columns\nwill if you include columns that are not magnitudes.\nimport\nopencosmo\nas\noc\nfrom\nopencosmo.column\nimport\nadd_mag_cols\ndataset\n=\noc\n.\nopen\n(\n\"catalog.hdf5\"\n)\nmag_total\n=\nadd_mag_cols\n(\n\"mag_g\"\n,\n\"mag_r\"\n,\n\"mag_i\"\n,\n\"mag_z\"\n,\n\"mag_y\"\n)\ndataset\n=\ndataset\n.\nwith_new_columns\n(\nmag_total\n=\nmag_total\n)\nParameters\n:\n*magnitudes\n(\nstr\n|\nColumn\n|\nDerivedColumn\n) – Any number of magnitude columns. You can pass in simple column names, columns constructred\nwith\nopencosmo.col()\n, or columns created from combinations of other columns\nReturns\n:\nnew_column\n– A new derived column that can be passed into\nwith_new_columns\nReturn type\n:\nDerivedColumn\nopencosmo.column.\noffset_3d\n(\ncoord_name_a\n,\ncoord_name_b\n,\nlabels\n=\n['x',\n'y',\n'z']\n)\n\nCreate a derived column that contains the magnitude of the offset between two sets of 3d coordinates.\nFor exmaple, to get the magnitude of the difference between the FoF halo centers and the SOD halo centers:\nfrom\nopencosmo.column\nimport\noffset_3d\nimport\nopencosmo\nas\noc\ndataset\n=\noc\n.\nopen\n(\n\"haloproperties.hdf5\"\n)\noffset_column\n=\noffset_3d\n(\n\"fof_halo_com\"\n,\n\"sod_halo_com\"\n)\ndataset\n=\ndataset\n.\nwith_new_columns\n(\noffset\n=\noffset_column\n)\nThis function assumes that the columns are named “fof_halo_center_{x, y, z}” and “sod_halo_center_{x, y, z}”,\nyou can choose different labels by setting the\nlabels\nargument.\nThis function outputs a derived column that can be passed into\nwith_new_columns\nwill if the columns do not all have the same units.\nParameters\n:\ncoord_name_a\n(\nstr\n) – The base name of the first coordinate\ncoord_name_b\n(\nstr\n) – The base name of the second coordinate\nlabels\n(\nIterable\n[\nstr\n]\n,\ndefault =\n[\n\"x\"\n,\n\"y\"\n,\n\"z\"\n]\n) – The coordinate labels. The names of the columns are assumed to be “{coord_name_a}_{labels}” and\n“{coord_name_b}_{labels}”"}
{"url": "https://opencosmo.readthedocs.io/en/latest/parameters_ref.html", "title": "Parameters and the OpenCosmoHeader — OpenCosmo Documentation", "content": "# Parameters and the OpenCosmoHeader\nParameters and the OpenCosmoHeader\n\nEvery OpenCosmo files comes with a set of parameters that describe the dataset it was draw from and other relevant information. These parameters are parsed into an\nOpenCosmoHeader\nobject which can be used throughout the code to verify properties of the dataset, or for reference.\nDatasets and collections will allways make their header avaliable with the\n.header\nattribute. You can acccess the parameters in a given header with the\n.parameters\nattribute. The actual parameters available will vary depending on the type of data in the file.\nclass\nopencosmo.header.\nOpenCosmoHeader\n(\nfile_pars\n,\nrequired_origin_parameters\n,\noptional_origin_parameters\n,\ndtype_parameters\n,\nunit_convention\n=\nUnitConvention.SCALEFREE\n)\n\nA class to represent the header of an OpenCosmo file. The header contains\ninformation about the simulation the data is a part of, as well as other\nmeatadata that are useful to the library in various contexts. Most files\nwill have a single unique header, but it is possible to have multiple\nheaders in a SimulationCollection.\nParameters\n:\nfile_pars\n(\nFileParameters\n)\nrequired_origin_parameters\n(\ndict\n[\nstr\n,\nBaseModel\n]\n)\noptional_origin_parameters\n(\ndict\n[\nstr\n,\nBaseModel\n]\n)\ndtype_parameters\n(\ndict\n[\nstr\n,\nBaseModel\n]\n)\nunit_convention\n(\nUnitConvention\n)\nwith_units\n(\nconvention\n)\n\nParameters\n:\nconvention\n(\nUnitConvention\n|\nstr\n)\nproperty\nparameters\n\nReturn the parametrs stored in this header as\nkey-value pairs. The values will be Pydantic models.\nAny block of parameters that is returned from this method can\nalso be accessed with standard dot notation. For example, HACC\ndata contains a “simulation” block that contains the parameters that\nwere used to run the original simulation. The following calls are\nequivalent:\nheader\n.\nsimulation\nheader\n.\nparameters\n[\n\"simulation\"\n]\nReturns\n:\nparameters\n– The parameter blocks associated with this header\nReturn type\n:\ndict[str, pydantic.BaseModel]\nwith_parameter\n(\nkey\n,\nvalue\n)\n\nUpdate a dtype parameter with a new value. This in general should never\nbe called by the user. Returns a copy.\nParameters\n:\nkey\n(\nstr\n)\nvalue\n(\nAny\n)\nwith_parameters\n(\nupdates\n)\n\nParameters\n:\nupdates\n(\ndict\n[\nstr\n,\nAny\n]\n)\ndump\n(\n)\n\nReturn type\n:\nSchema\nproperty\nfile\n:\nFileParameters\n\nAll files must at minimum have a “file” block in their header. This block\ncontains basic information like the original source of the data and\nits data type.\n## Cosmology\nCosmology\n\nMost OpenCosmo files will contain cosmology parameters, which describe the cosmology the simulation was run under. In general you will not interact with this parameter block directly. Instead, requiresting it will return an astropy.cosmology.Cosmology object. Dataset and collections will generally make this object available directly with the\n.cosmology\nattribute.\nclass\nopencosmo.parameters.cosmology.\nCosmologyParameters\n(\n*\n,\nh\n,\nomega_m\n,\nomega_b\n,\nomega_l\n,\nn_eff_massless\n,\nn_eff_massive\n=\n0\n,\nsigma_8\n,\nw_0\n,\nw_a\n)\n\nResponsible for validating cosmology parameters and handling differences in\nnaming conventions between OpenCosmo and astropy.cosmology. Generally should\nnot be used by the user directly\nParameters\n:\nh\n(\nfloat\n)\nomega_m\n(\nAnnotated\n[\nfloat\n,\nGe\n(\nge=0\n)\n]\n)\nomega_b\n(\nAnnotated\n[\nfloat\n,\nGe\n(\nge=0\n)\n]\n)\nomega_l\n(\nAnnotated\n[\nfloat\n,\nGe\n(\nge=0\n)\n]\n)\nn_eff_massless\n(\nAnnotated\n[\nfloat\n,\nGt\n(\ngt=0\n)\n]\n)\nn_eff_massive\n(\nfloat\n)\nsigma_8\n(\nfloat\n)\nw_0\n(\nfloat\n)\nw_a\n(\nfloat\n)\nfield\nh\n:\nfloat\n[Required]\n\nReduced Hubble constant\nConstraints\n:\nge\n= 0.0\nproperty\nH0\n:\nfloat\n\nHubble constant in km/s/Mpc\nfield\nOm0\n:\nfloat\n[Required]\n(alias\n'omega_m')\n\nTotal matter density\nConstraints\n:\nge\n= 0.0\nfield\nOb0\n:\nfloat\n[Required]\n(alias\n'omega_b')\n\nBaryon density\nConstraints\n:\nge\n= 0.0\nfield\nOde0\n:\nfloat\n[Required]\n(alias\n'omega_l')\n\nDark energy density\nConstraints\n:\nge\n= 0.0\nfield\nNeff\n:\nfloat\n[Required]\n(alias\n'n_eff_massless')\n\nEffective number of neutrinos\nConstraints\n:\ngt\n= 0.0\nfield\nn_eff_massive\n:\nfloat\n=\n0\n\nEffective number of massive neutrinos\nConstraints\n:\nge\n= 0.0\nfield\nsigma_8\n:\nfloat\n[Required]\n\nRMS mass fluctuation at 8 Mpc/h\nConstraints\n:\nge\n= 0.0\nfield\nw0\n:\nfloat\n[Required]\n(alias\n'w_0')\n\nDark energy equation of state\nfield\nwa\n:\nfloat\n[Required]\n(alias\n'w_a')\n\nDark energy equation of state evolution\n## Simulation Parameters\nSimulation Parameters\n\nData that was originally produced by HACC will contain the parameters that were used to initialize the simulation. Datasets and collections will generally make these paramters available with the\n.simulation\nattribute.\nclass\nopencosmo.parameters.hacc.\nHaccSimulationParameters\n(\n*\n,\nbox_size\n,\nz_ini\n,\nz_end\n,\nn_gravity\n,\nn_steps\n,\npm_grid\n,\noffset_gravity_ini\n)\n\nParameters\n:\nbox_size\n(\nfloat\n)\nz_ini\n(\nfloat\n)\nz_end\n(\nfloat\n)\nn_gravity\n(\nint\n|\nNone\n)\nn_steps\n(\nint\n)\npm_grid\n(\nint\n)\noffset_gravity_ini\n(\nfloat\n|\nNone\n)\nfield\nbox_size\n:\nfloat\n[Required]\n\nSize of the simulation box (Mpc/h)\nConstraints\n:\nge\n= 0\nfield\nz_ini\n:\nfloat\n[Required]\n\nInitial redshift of the simulation\nConstraints\n:\nge\n= 0.01\nfield\nz_end\n:\nfloat\n[Required]\n\nFinal redshift of the simulation\nConstraints\n:\nge\n= 0.0\nfield\nn_gravity\n:\nint\n|\nNone\n[Required]\n\nNumber of gravity-only particles (per dimension). In hydrodynamic simulations, this parameter will not be set.\nConstraints\n:\nge\n= 2\nfield\nn_steps\n:\nint\n[Required]\n\nNumber of time steps\nConstraints\n:\nge\n= 1\nfield\npm_grid\n:\nint\n[Required]\n\nNumber of grid points (per dimension)\nConstraints\n:\nge\n= 2\nfield\noffset_gravity_ini\n:\nfloat\n|\nNone\n[Required]\n\nLagrangian offset for gravity-only particles\nproperty\nstep_zs\n:\nlist\n[\nfloat\n]\n\nGet the redshift of the steps in this simulation. Outputs such that\nredshift[step_number] returns the redshift for that step. Keep in\nmind that steps go from high z -> low z.\nclass\nopencosmo.parameters.hacc.\nHaccHydroSimulationParameters\n(\n*\n,\nbox_size\n,\nz_ini\n,\nz_end\n,\nn_gravity\n,\nn_steps\n,\npm_grid\n,\noffset_gravity_ini\n,\nn_bar\n,\nn_dm\n,\noffset_bar_ini\n,\noffset_dm_ini\n,\nsubgrid_agn_kinetic_eps\n,\nsubgrid_agn_kinetic_jet_vel\n,\nsubgrid_agn_nperh\n,\nsubgrid_agn_seed_mass\n,\nsubgrid_wind_egy_w\n,\nsubgrid_wind_kappa_w\n)\n\nParameters\n:\nbox_size\n(\nAnnotated\n[\nfloat\n,\nGe\n(\nge=0\n)\n]\n)\nz_ini\n(\nAnnotated\n[\nfloat\n,\nGe\n(\nge=0.01\n)\n]\n)\nz_end\n(\nAnnotated\n[\nfloat\n,\nGe\n(\nge=0\n)\n]\n)\nn_gravity\n(\nAnnotated\n[\nint\n|\nNone\n,\nGe\n(\nge=2\n)\n]\n)\nn_steps\n(\nAnnotated\n[\nint\n,\nGe\n(\nge=1\n)\n]\n)\npm_grid\n(\nAnnotated\n[\nint\n,\nGe\n(\nge=2\n)\n]\n)\noffset_gravity_ini\n(\nfloat\n|\nNone\n)\nn_bar\n(\nint\n)\nn_dm\n(\nAnnotated\n[\nint\n,\nGe\n(\nge=2\n)\n]\n)\noffset_bar_ini\n(\nfloat\n)\noffset_dm_ini\n(\nfloat\n)\nsubgrid_agn_kinetic_eps\n(\nfloat\n)\nsubgrid_agn_kinetic_jet_vel\n(\nfloat\n)\nsubgrid_agn_nperh\n(\nfloat\n)\nsubgrid_agn_seed_mass\n(\nfloat\n)\nsubgrid_wind_egy_w\n(\nfloat\n)\nsubgrid_wind_kappa_w\n(\nfloat\n)\nfield\nn_gas\n:\nint\n[Required]\n(alias\n'n_bar')\n\nNumber of gas particles (per dimension)\nfield\nn_dm\n:\nint\n[Required]\n\nNumber of dark matter particles (per dimension)\nConstraints\n:\nge\n= 2\nfield\noffset_gas_ini\n:\nfloat\n[Required]\n(alias\n'offset_bar_ini')\n\nLagrangian offset for gas particles\nfield\noffset_dm_ini\n:\nfloat\n[Required]\n\nLagrangian offset for dark matter particles\nfield\nagn_kinetic_eps\n:\nfloat\n[Required]\n(alias\n'subgrid_agn_kinetic_eps')\n\nAGN feedback efficiency\nfield\nagn_kinetic_jet_vel\n:\nfloat\n[Required]\n(alias\n'subgrid_agn_kinetic_jet_vel')\n\nAGN feedback velocity\nfield\nagn_nperh\n:\nfloat\n[Required]\n(alias\n'subgrid_agn_nperh')\n\nAGN sphere of influence\nfield\nagn_seed_mass\n:\nfloat\n[Required]\n(alias\n'subgrid_agn_seed_mass')\n\nAGN seed mass (Msun / h)\nfield\nwind_egy_w\n:\nfloat\n[Required]\n(alias\n'subgrid_wind_egy_w')\n\nWind mass loading factor\nfield\nwind_kappa_w\n:\nfloat\n[Required]\n(alias\n'subgrid_wind_kappa_w')\n\nWind velocity"}
{"url": "https://opencosmo.readthedocs.io/en/latest/spatial_ref.html", "title": "Regions — OpenCosmo Documentation", "content": "# Regions\nRegions\n\nRegions can be used to spatially query datasets and collections. Regions are constructed with\nopencosmo.make_box()\nor\nopencosmo.make_cone()\n.\nopencosmo.\nmake_box\n(\np1\n,\np2\n)\n\nCreate a 3-Dimensional box region of arbitrary size.\nThe quantities of the box region are unitless, but will be converted\nto the unit convention of any dataset they interact with.\nParameters\n:\np1\n(\n(\nfloat\n,\nfloat\n,\nfloat\n)\n) – 3D Point definining one corner of the box\np1\n– 3D Point definining the other corner of the box\np2\n(\nPoint3d\n)\nReturns\n:\nregion\n– The constructed region\nReturn type\n:\nopencosmo.spatial.BoxRegion\nRaises\n:\nValueError\n– If the region has zero length in any dimension\nopencosmo.\nmake_cone\n(\ncenter\n,\nradius\n)\n\nConstruct a cone region used for querying lightcones. A cone\nregion is defined by a location on the sky and an angular size,\nwhich is used as a radius.\nParameters\n:\ncenter\n(\nastropy.coordinates.SkyCord\n|\ntuple\n[\nfloat\n,\nfloat\n]\n) – The center of the cone region. If a unitless tuple is passed,\nthe values are assumed to be in degrees.\nradius\n(\nastropy.units.Quantity\n|\nfloat\n) – The radius of the region. If a unitless value is passed,\nit is assumed to be in degrees.\nReturns\n:\nregion\n– The constructed region\nReturn type\n:\nopencosmo.spatial.ConeRegion\nclass\nopencosmo.spatial.\nBoxRegion\n(\ncenter\n,\nhalfwidths\n)\n\nA region representing a 3-dimensional box of arbitrary length, width, and depth. A\nBoxRegion can be used to query snapshot data. BoxRegions can be constructed with\nopencosmo.make_box()\nWhen used in a spatial query, the values defining a box region will always be\ninterpreted in the same unit convention as the dataset it is being used to query.\nThis means two queries on the same dataset with the same box region can return\ndifferent results if the unit convention changes between them.\nParameters\n:\ncenter\n(\nPoint3d\n)\nhalfwidths\n(\nBoxSize\n)\nproperty\nbounds\n\nThe bounds of this region in the form [(min,max) …]\nReturns\n:\nbounds\nReturn type\n:\nlist[tuple(float,float), ….]\ncontains\n(\nother\n)\n\nCheck if this box region contains another region or set of points.\nPoints should be passed in as a numpy array with shape (3, n_points). Note\nthat this method requires that the bounds of the test region be fully inside the\nother region. Functionally this means that a region does not contain itself.\nParameters\n:\nother\n(\nBoxRegion\n|\nnp.ndarray\n) – The region or points to check\nReturns\n:\ncontained\n– Whether the region or points are contained in the region\nReturn type\n:\nbool or np.ndarray[bool]\nRaises\n:\nValueError\n– If the input not a region or points\nintersects\n(\nother\n)\n\nCheck if this BoxRegion intersects another BoxRegion. This function\nreturns true of the regions intersect in any way, including if one\ncontains the other. Regions that share a bound but do not cross\nwill return False.\nParameters\n:\nother\n(\nBoxRegion\n) – The region to check\nReturns\n:\nintersects\n– Whether the regions intersect\nReturn type\n:\nbool\nRaises\n:\nValueError:\n– If the input is not a BoxRegion\nclass\nopencosmo.spatial.\nConeRegion\n(\ncenter\n,\nradius\n)\n\nCone region for querying lightcones. Defined by RA/Dec coordinate and an angular\nsize. Should always be constructed with\nopencosmo.make_cone()\nNote that the underlying coordinate representation of the data may or may not be\nin RA and Dec. Spatial queries handle all the necessary conversions, but this may\nmean that the coordinates in the query output do not appear to match the coordinates\nof the region used to perform the query.\nParameters\n:\ncenter\n(\nSkyCoord\n)\nradius\n(\nu.Quantity\n)\nproperty\ncenter\n\nThe center of this region.\nReturns\n:\ncoordinate\nReturn type\n:\nastropy.coordinates.SkyCoord\nproperty\nradius\n\nThe angular radius of the region.\nReturns\n:\nradius\nReturn type\n:\nastropy.units.Quantity\ncontains\n(\nother\n)\n\nCheck if this ConeRegion contains another ConeRegion. This region must\nbe fully outside the other for this function to return True. Functionally,\nthis means a region does not contain itself.\nParameters\n:\nother\n(\nopencosmo.spatial.ConeRegion\n)\nintersects\n(\nother\n)\n\nCheck if this ConeRegion intersects with another cone region. Regions which\ntouch at a single point but do not cross will return False.\nParameters\n:\nother\n(\nopencosmo.spatial.ConeRegion\n) – The other region to query.\nReturns\n:\nintersects\n– Whether the two regions intersect\nReturn type\n:\nbool"}
{"url": "https://opencosmo.readthedocs.io/en/latest/analysis_ref.html", "title": "Analysis — OpenCosmo Documentation", "content": "# Analysis\nAnalysis\n\nopencosmo.analysis.\ncreate_yt_dataset\n(\ndata\n,\ncompute_xray_fields\n=\nFalse\n,\nreturn_source_model\n=\nFalse\n,\nsource_model_kwargs\n=\n{}\n)\n\nConverts particle data to a\nyt\ndataset. Note that\nyt\nis generally developed with AMR codes in mind, but support for\nSPH codes is continually being added.\nyt’s\ndocumentation can\nbe found\nhere\n.\nIf\ncompute_xray_fields\nis enabled, X-ray emissivity and luminosity fields\nwill be added using\npyxsim\n,\nwhich generates photon samples from gas properties.\nParameters\n:\ndata\n(\ndict\nof\nastropy.table.Table\n) – A dictionary of particle datasets. Must include at least positions and masses.\ncompute_xray_fields\n(\nbool\n,\noptional\n) – Whether or not to compute X-ray luminosities with\npyxsim\n.\nUses\nCIESourceModel\n, which considers thermal emission from gas assuming\ncollisional ionization equilibrium.\nreturn_source_model\n(\nbool\n,\noptional\n) – Whether or not to return the\npyxsim\nsource model for further interaction,\nsuch as computing additional luminosities in different frequency bands\nor generating synthetic observations.\nsource_model_kwargs\n(\ndict\n,\noptional\n) – Keyword arguments passed to the\nCIESourceModel\nconstructor in\npyxsim\n.\nThese can include parameters like\nemin\n,\nemax\n,\nnbins\n,\nabund_table\n, etc.,\nto control the spectral resolution and emission model behavior. If\nNone\n,\ndefault values will be used for all source model parameters.\nReturns\n:\nds\n(\nyt.data_objects.static_output.Dataset\n) – A\nyt\ndataset built from the input particle data, with additional fields\n(e.g., X-ray luminosities) if requested.\nsource_model\n(\npyxsim.source_models.CIESourceModel, optional\n) – Returned only if\nreturn_source_model=True\n.\nReturn type\n:\nUnion[YT_Dataset, Tuple[YT_Dataset, CIESourceModel]]\nopencosmo.analysis.\nvisualize_halo\n(\nhalo_id\n,\ndata\n,\nprojection_axis\n=\n'z'\n,\nlength_scale\n=\n'top\nleft'\n,\ntext_color\n=\n'gray'\n,\nwidth\n=\nNone\n)\n\nCreates a figure showing particle projections of dark matter, stars, gas, and/or gas temperature\nfor given halo. If any of the listed particle types are not present in the dataset, this will\ncreate a horizontal arrangement with only the particles/fields that are present. Otherwise,\ncreates a 2x2-panel figure. Each panel is an 800x800 pixel array.\nTo customize the arrangement of panels, fields, colormaps, etc., see\nhalo_projection_array()\n.\nParameters\n:\nhalo_id\n(\nint\n) – Identifier of the halo to be visualized.\ndata\n(\nopencosmo.StructureCollection\n) – OpenCosmo StructureCollection object containing both halo properties and particle data\n(e.g. output of\nopencosmo.open([haloproperties,\nsodbighaloparticles])\n).\nprojection_axis\n(\nstr\n,\noptional\n) – Data is projected along this axis (\n\"x\"\n,\n\"y\"\n, or\n\"z\"\n).\nOverridden if\nparams[\"projection_axes\"]\nis provided\nlength_scale\n(\nstr\nor\nNone\n,\noptional\n) –\nOptionally add a horizontal bar denoting length scale in Mpc.\nOptions:\n\"top\nleft\"\n: add to top left panel\n\"top\nright\"\n: add to top right panel\n\"bottom\nleft\"\n: add to bottom left panel\n\"bottom\nright\"\n: add to bottom right panel\n\"all\ntop\"\n: add to all panels on top row\n\"all\nbottom\"\n: add to all panels on bottom row\n\"all\nleft\"\n: add to all panels on leftmost column\n\"all\nright\"\n: add to all panels on rightmost column\n\"all\"\n: add to all panels\nNone\n: no length scale on any panel\ntext_color\n(\nstr\n,\noptional\n) – Set the color of all text annotations. Default is “gray”\nwidth\n(\nfloat\n,\noptional\n) – Width of each projection panel in units of R200 for the halo.\nIf None, plots full subvolume around halo.\nReturns\n:\nA matplotlib Figure object.\nReturn type\n:\nmatplotlib.figure.Figure\nopencosmo.analysis.\nhalo_projection_array\n(\nhalo_ids\n,\ndata\n,\nfield\n=\n('dm',\n'particle_mass')\n,\nweight_field\n=\nNone\n,\nprojection_axis\n=\n'z'\n,\ncmap\n=\n'gray'\n,\nzlim\n=\nNone\n,\nparams\n=\nNone\n,\nlength_scale\n=\nNone\n,\ntext_color\n=\n'gray'\n,\nwidth\n=\nNone\n)\n\nCreates a multipanel figure of projections for different fields and/or halos.\nBy default, creates an arrangement of dark matter particle projections with the\nsame shape as\nhalo_ids\n. Each panel is an 800x800 pixel array.\nCustomizable — can change which fields are plotted for which halos, their order,\nweighting, etc., using\nparams\n.\nNOTE:\nDark matter particle masses often aren’t stored for gravity-only simulations\nbecause the particles all have the same mass by construction. The particles are also\nlabelled as “gravity” particles in this case instead of “dm” particles in the data.\nTo project dark matter particles in gravity only, one can use the\n(\"gravity\",\n\"particle_ones\")\nfield in place of\n(\"dm\",\n\"particle_mass\")\n. This will produce the same final image.\nParameters\n:\nhalo_ids\n(\nint\nor\n2D array\nof\nint\n) – Unique ID of the halo(s) to be visualized. The shape of\nhalo_ids\nsets the layout\nof the figure (e.g., if\nhalo_ids\nis a 2x3 array, the outputted figure will be a 2x3\narray of projections). If\nint\n, a single panel is output while preserving formatting.\ndata\n(\nopencosmo.StructureCollection\n) – OpenCosmo StructureCollection dataset containing both halo properties and particle data\n(e.g., output of\nopencosmo.open([haloproperties,\nsodbighaloparticles])\n).\nfield\n(\ntuple\nof\nstr\n,\noptional\n) – Field to plot for all panels. Follows yt naming conventions (e.g.,\n(\"dm\",\n\"particle_mass\")\n,\n(\"gas\",\n\"temperature\")\n). Overridden if\nparams[\"fields\"]\nis provided.\nweight_field\n(\ntuple\nof\nstr\n,\noptional\n) – Field to weight by during projection. Follows yt naming conventions.\nOverridden if\nparams[\"weight_fields\"]\nis provided.\nprojection_axis\n(\nstr\n,\noptional\n) – Data is projected along this axis (\n\"x\"\n,\n\"y\"\n, or\n\"z\"\n).\nOverridden if\nparams[\"projection_axes\"]\nis provided\ncmap\n(\nstr\n) – Matplotlib colormap to use for all panels. Overridden if\nparams[\"cmaps\"]\nis provided.\nSee\nhttps://matplotlib.org/stable/gallery/color/colormap_reference.html\nfor named colormaps.\nzlim\n(\ntuple\nof\nfloat\n,\noptional\n) – Colorbar limits for\nfield\n. Overridden if\nparams[\"zlims\"]\nis provided.\nlength_scale\n(\nstr\nor\nNone\n,\noptional\n) –\nOptionally add a horizontal bar denoting length scale in Mpc.\nOptions:\n\"top\nleft\"\n: add to top left panel\n\"top\nright\"\n: add to top right panel\n\"bottom\nleft\"\n: add to bottom left panel\n\"bottom\nright\"\n: add to bottom right panel\n\"all\ntop\"\n: add to all panels on top row\n\"all\nbottom\"\n: add to all panels on bottom row\n\"all\nleft\"\n: add to all panels on leftmost column\n\"all\nright\"\n: add to all panels on rightmost column\n\"all\"\n: add to all panels\nNone\n: no length scale shown\nparams\n(\ndict\n,\noptional\n) –\nDictionary of customization parameters for the projection panels. Overrides\ndefaults. All values must be 2D arrays with the same shape as\nhalo_ids\n.\nKeys may include:\n\"fields\"\n: 2D array of fields to plot (yt naming conventions)\n\"weight_fields\"\n: 2D array of projection weights (or None)\n\"projection_axes\"\n: 2D array of projection axes (“x”, “y”, or “z”)\n\"zlims\"\n: 2D array of colorbar limits (log-scaled)\n\"labels\"\n: 2D array of panel labels (or None)\n\"cmaps\"\n: 2D array of Matplotlib colormaps for each panel\n\"widths\"\n: 2D array of widths in units of R200\ntext_color\n(\nstr\n,\noptional\n) – Set the color of all text annotations. Default is “gray”\nwidth\n(\nfloat\n,\noptional\n) – Width of each projection panel in units of R200 for the halo.\nOverridden if\nparams[\"widths\"]\nis provided.\nIf None, plots full subvolume.\nReturns\n:\nA Matplotlib Figure object.\nReturn type\n:\nmatplotlib.figure.Figure\nopencosmo.analysis.\nParticleProjectionPlot\n(\n*\nargs\n,\n**\nkwargs\n)\n\nWrapper for\nyt.ParticleProjectionPlot\n.\nCreates a 2D projection plot of particle-based data along a specified axis.\nParameters\n:\n*args\n– Positional arguments passed directly to\nyt.ParticleProjectionPlot\n.\n**kwargs\n– Keyword arguments passed directly to\nyt.ParticleProjectionPlot\n.\nReturns\n:\nA ParticleProjectionPlot object containing the particle projection plot.\nReturn type\n:\nyt.visualization.plot_window.ParticleProjectionPlot\nopencosmo.analysis.\nProjectionPlot\n(\n*\nargs\n,\n**\nkwargs\n)\n\nWrapper for\nyt.ProjectionPlot\n.\nCreates a 2D projection plot of particle-based data along a specified axis.\nSmoothing is applied to SPH particle data over the smoothing length\nParameters\n:\n*args\n– Positional arguments passed directly to\nyt.ProjectionPlot\n.\n**kwargs\n– Keyword arguments passed directly to\nyt.ProjectionPlot\n.\nReturns\n:\nA ProjectionPlot object containing the smoothed particle projection plot.\nReturn type\n:\nyt.visualization.plot_window.ProjectionPlot\nopencosmo.analysis.\nSlicePlot\n(\n*\nargs\n,\n**\nkwargs\n)\n\nWrapper for\nyt.SlicePlot\n.\nCreates a 2D slice plot of particle-based data along a specified axis.\nSmoothing is applied to SPH particle data over the smoothing length\nParameters\n:\n*args\n– Positional arguments passed directly to\nyt.SlicePlot\n.\n**kwargs\n– Keyword arguments passed directly to\nyt.SlicePlot\n.\nReturns\n:\nA PlotWindow object containing the particle slice plot.\nReturn type\n:\nyt.visualization.plot_window.PlotWindow\nopencosmo.analysis.\nProfilePlot\n(\n*\nargs\n,\n**\nkwargs\n)\n\nWrapper for\nyt.ProfilePlot\n.\nCreates a bin-averaged profile of a dependent variable\nas a function of one or more independent variables.\nParameters\n:\n*args\n– Positional arguments passed directly to\nyt.ProfilePlot\n.\n**kwargs\n– Keyword arguments passed directly to\nyt.ProfilePlot\n.\nReturns\n:\nA PlotWindow object containing the profile plot.\nReturn type\n:\nyt.visualization.plot_window.PlotWindow\nopencosmo.analysis.\nPhasePlot\n(\n*\nargs\n,\n**\nkwargs\n)\n\nWrapper for\nyt.PhasePlot\n.\nCreates a 2D histogram (phase plot) showing how one quantity varies as a function\nof two others, useful for visualizing thermodynamic or structural relationships\n(e.g., temperature vs. density colored by mass).\nParameters\n:\n*args\n– Positional arguments passed directly to\nyt.PhasePlot\n.\n**kwargs\n– Keyword arguments passed directly to\nyt.PhasePlot\n.\nReturns\n:\nA PlotWindow object containing the phase plot.\nReturn type\n:\nyt.visualization.plot_window.PlotWindow"}
{"url": "https://opencosmo.readthedocs.io/en/latest/changelog.html", "title": "opencosmo 1.0.1 (2026-01-21) — OpenCosmo Documentation", "content": "# opencosmo 1.0.1 (2026-01-21)\nopencosmo 1.0.1 (2026-01-21)\n\n## Improvements\nImprovements\n\nDisable unneeded verification step for eagerly-evaluate columns in\nDataset.evaluate\n.\n# opencosmo 1.0.0 (2026-01-21)\nopencosmo 1.0.0 (2026-01-21)\n\n## Bugfixes\nBugfixes\n\nFix a bug that caused structure collections to not open correctly if the individual datasets were lightcone datasets. (#103)\nFix a bug that prevented adding together columns with logarithmic units (#142)\nAttempting to sort by a column that is not in the dataset now errors correctly.\nCorrectly update order across ranks when stacking lightcones.\nFix a bug that could cause\nStructureCollection.evaluate\nto fail if the\ndataset\nargument is set to halo_properties or galaxy_properties.\nFix a bug that could cause created columns to be evaluated twice instead of correctly being cached.\nFix a bug that could cause filtering to fail on very short datasets.\nFix a bug that could cause lightcone stacking to fail with large numbers of datasets\nFix a bug that could cause segfaults (in numba-accelerated code) when running take operations\nFix a bug that could cause unit conversions on structure collections to not propogate to a halo_properties or galaxy_properties dataset.\nFix a bug that could cause user-created columns to be ordered incorrectly if they were inserted into a sorted dataset or lighcone.\nFix a bug that could cause writes to fail for a large SimulationCollection\nFix a bug that could cause writing to segfault when working in MPI\nSubmitting a filter without actually passing filters will now return the original dataset rather than an empty dataset.\n## New Features\nNew Features\n\nwith_units\ncan now be used to provide unit conversions, in addition to changing conventions (#43)\nDescriptions of columns can now be accessed with\nDataset.descriptions\n(#122)\nwith_new_columns\nnow accepts a\ndescriptions\nargument for providing column descriptions\nStructureCollection.evaluate\nnow performs evaluation on individual structures when\ndataset\nargument is passed.\nopencosmo.write()\nnow works in MPI contexts even if parallel-hdf5 is not installed.\nAdd\nStructureCollection.evaluate_on_dataset\n, which performs evaluate on a single dataset in the structure collection without chunking.\nAdd a number of common column combinations that can be used to add columns to a dataset with\nwith_new_columns\n<opencosmo.Dataset.with_new_columns()\n. See the\ncolumn API reference\nfor details\nAdded additional supported outpt formats “pandas”, “polars” and “arrow”\nAdded support for working with maps stored with a Healpix decomposition with new the\nopencosmo.HealpixMap\nclass.\nColumn filters can now be combined with boolean operators & (and) and | (or).\nColumns created with\nopencosmo.col()\nnow support\n.log10()\n,\n.exp10()\nand\n.sqrt()\nColumns that are read from disk are now cached, ensuring that requesting data second time is always fast.\nWhen writing lightcones, datasets from adjacent redshift slices will now be stacked into a single dataset if their combined length is small enough.\n## Improvements\nImprovements\n\nStructureCollection.select\nand\nStructureCollection.drop\nnow follow the same semantics as\nStructureCollection.evaluate\nfor passing columns from multiple datasets in a single function call.\nStructureCollection.select\n,\nStructureCollection.drop\n, and\nStructureCollection.evaluate\nnow support specifying columns in nested collections.\nAdd the ability to create datasets entirely in memory, which is used at present to support downgrading healpix maps.\nEvaluations in individual dataset are now performed lazily unless\ninsert\n=\nFalse\nOpening multiple files will now fail with a clear error message if one or more of the files is not opencosmo-formatted.\nReads from hdf5 now use\nread_direct\n, which improves read performance especially with large datasets.\nRewrite DataIndex to be fully functional, and accelerate with Numba.\nThe logic behind\noc.write\nhas been completely rewritten to be more functional, reliable, and easy to extend. This change does not affect how the function is used, and does not make any changes to the data format.\nWriting in an MPI context will now fail with an error if no rank has data to write\nDataset.evaluate\nis now performed lazily when\ninsert\n=\nTrue\n.\n## Miscellaneous\nMiscellaneous\n\nColumn management has been reworked and centralized\nMove all annotation-only imports behind a\nTYPE_CHECKING\nblock and add\nfrom\n__future__\nimport\nannotations\nto most files. This significantly improves initial import time.\nUnit handling has been fully rewritten, making it much more flexible for backend work.\n## Deprecations and Removals\nDeprecations and Removals\n\nFunctions passed into\nDataset.evaluate\nmust now always explicitly list columns as arguments”\nopencosmo.read (deprecated since 0.7) has been removed.\nopencosmo.open_linked_files (deprecated sinced 0.8) has been removed\nopencosmo.Dataset.collect has been removed\n# opencosmo 0.9.6 (2025-10-20)\nopencosmo 0.9.6 (2025-10-20)\n\n## Bugfixes\nBugfixes\n\nMake diffstar pop optional in diffsky parameters\n# opencosmo 0.9.4 (2025-09-26)\nopencosmo 0.9.4 (2025-09-26)\n\n## Bugfixes\nBugfixes\n\nFix a bug that could cause opening halos and galaxies only to fail\n# opencosmo 0.9.3 (2025-09-25)\nopencosmo 0.9.3 (2025-09-25)\n\n## Bugfixes\nBugfixes\n\nFix an issue that could cause opens with multiple files to fail in MPI contexts\n# opencosmo 0.9.2 (2025-09-25)\nopencosmo 0.9.2 (2025-09-25)\n\n## Bugfixes\nBugfixes\n\nFix a but that could cause opening properties and profiles without particles to fail.\n# opencosmo 0.9.1 (2025-09-16)\nopencosmo 0.9.1 (2025-09-16)\n\n## Bugfixes\nBugfixes\n\nRe-add license file to package for conda-forge compatability\n# opencosmo 0.9.0 (2025-09-15)\nopencosmo 0.9.0 (2025-09-15)\n\n## Features\nFeatures\n\nUnitful values in headers now carry astropy units and transform under unit transformations (#70)\nwith_new_columns\ncan now take numpy arrays or astropy quantities (#77)\nAdd a new “evaluate” method to the standard API, which fills the role of “with_new_columns” for cases when the computation is more than just a simple algebraic combination of columns (#77)\nAdd “sort_by” to the standard API, which allows ordering by the value of a column. (#112)\nMutli-dimensional columns are now unpacked correctly when only reading a single row\nYou can now drop entire datasets from a StructureCollection using\nwith_datasets\nYou can now dump datasets and some data from collections to parquet with\nopencosmo.io.write_parquet()\nYou can now remove datasets from a StructureCollection with\nwith_datasets\n## Bugfixes\nBugfixes\n\nFixed a bug that could cause MPI writes to stall after queries that returned small numbers of rows. (#99)\nFixed a bug that could cause printing lightcone summary to fail when column originally contained “redshift” column that had been dropped. (#101)\nFix a bug for installing analysis tools\n## Deprecations and Removals\nDeprecations and Removals\n\nStructureCollections now always require a dataset be specified when calling\nselect\n## Misc\nMisc\n\n#106\nDependency management now handled by UV\nMinimum required version of mpi4py has been decreased to match version available in ALCF base python environments.\nParameters accessed through a dataset are now returned as dictionaries\nRemove significant amounts of dead code, reorganize for clarity\nReplace astropy.table.Table with astropy.table.QTable, which generally handles units much more cleanly\nUpdate evaluate to avoid evaluating first row twice\nUpdate evaluate to respect default parameters\n# opencosmo 0.8.0 (2025-07-22)\nopencosmo 0.8.0 (2025-07-22)\n\n## Features\nFeatures\n\nStructureCollections now filter out structures with no particles by default (#75)\nColumns can now be dropped with\nDataset.drop <opencosmo.Dataset.drop>\n(inverse of “select”) (#77)\nData can now be requested from datasets as Numpy arrays (#78)\nData retrieved with the\n.data\nattribute on datasets are now cached (#78)\nAdded a new “Lightcone” collection type.\nAnalysis modules can now be installed with new command line script\nopencosmo\ninstall\nData producers can now define user-provided flags that determine which datasets should be loaded in a multiple-dataset or group of files.\nThe opencosmo.analysis module now includes tools for creating visualizations of halos\n## Bugfixes\nBugfixes\n\nStructureCollection and Datasets no longer raises StopIteration error if empty (prints warning) (#84)\nFixed yt interface for gravity-only simulations (#90)\n## Improved Documentation\nImproved Documentation\n\nAdd towncrier for automated changelog management\n## Deprecations and Removals\nDeprecations and Removals\n\nopen_linked_files has been depcrecated and will be removed in the future. Use\nopencosmo.open()\ninstead.\n## Misc\nMisc\n\nAdd installation from files with multiple datasets\nData opening logic has been rewritten from scratch, singnificantly improving performance when opening many file.\nPartitioning in MPI now ignores regions that do not have data\nThe header reading logic has been generalized to allow more flexibility in defining new data types\nUnit handling now supports data stored in conventions other than scalefree"}
{"url": "https://opencosmo.readthedocs.io/en/latest/_sources/main_api.rst.txt", "title": "https://opencosmo.readthedocs.io/en/latest/_sources/main_api.rst.txt", "content": ""}
{"url": "https://opencosmo.readthedocs.io/en/latest/_sources/first_steps.rst.txt", "title": "https://opencosmo.readthedocs.io/en/latest/_sources/first_steps.rst.txt", "content": ""}
{"url": "https://opencosmo.readthedocs.io/en/latest/_sources/index.rst.txt", "title": "https://opencosmo.readthedocs.io/en/latest/_sources/index.rst.txt", "content": ""}
{"url": "https://opencosmo.readthedocs.io/en/latest/_images/opencosmo_light.png", "title": "https://opencosmo.readthedocs.io/en/latest/_images/opencosmo_light.png", "content": ""}
{"url": "https://opencosmo.readthedocs.io/en/latest/_sources/installation.rst.txt", "title": "https://opencosmo.readthedocs.io/en/latest/_sources/installation.rst.txt", "content": ""}
